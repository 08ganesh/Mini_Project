{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49c6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (287113, 3)\n",
      "Validation set shape: (13368, 3)\n",
      "Test set shape: (11490, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('validation.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Print shape of each dataset to confirm loading\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", valid_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f35add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After sampling 1%:\n",
      "Train: (2871, 3)\n",
      "Validation: (134, 3)\n",
      "Test: (115, 3)\n"
     ]
    }
   ],
   "source": [
    "    # Take only 1% of each dataset \n",
    "train_sample = train_df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "valid_sample = valid_df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "test_sample = test_df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print new shapes\n",
    "print(\"\\nAfter sampling 1%:\")\n",
    "print(\"Train:\", train_sample.shape)\n",
    "print(\"Validation:\", valid_sample.shape)\n",
    "print(\"Test:\", test_sample.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89f68d",
   "metadata": {},
   "source": [
    "for labelled dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to create labeled dataset from structured data\n",
    "def create_labeled_dataset(structure_data, split_name=\"\"):\n",
    "    labeled_data = []\n",
    "    for doc_id, structure in enumerate(structure_data):\n",
    "        first_paragraph = True\n",
    "        for segment in structure:\n",
    "            sentences = segment['sentences']\n",
    "            seg_type = segment['type']\n",
    "            para_id = segment['paragraph_id']\n",
    "\n",
    "            for sent_id, sentence in enumerate(sentences):\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence or len(sentence) < 5:\n",
    "                    continue\n",
    "\n",
    "                if seg_type == 'header':\n",
    "                    label = 'header'\n",
    "                else:\n",
    "                    if first_paragraph and sent_id == 0:\n",
    "                        label = 'subheader'\n",
    "                    else:\n",
    "                        label = 'paragraph'\n",
    "\n",
    "                labeled_data.append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'para_id': para_id,\n",
    "                    'sent_id': sent_id,\n",
    "                    'sentence': sentence,\n",
    "                    'label': label\n",
    "                })\n",
    "\n",
    "            if seg_type == 'paragraph':\n",
    "                first_paragraph = False\n",
    "    \n",
    "    df = pd.DataFrame(labeled_data)\n",
    "    print(f\" {split_name.capitalize()} labeled dataset created. Total sentences: {len(df)}\")\n",
    "    print(df['label'].value_counts(), \"\\n\")\n",
    "    return df\n",
    "\n",
    "# Apply to all splits\n",
    "train_labeled_df = create_labeled_dataset(train_sample['structure'], split_name='train')\n",
    "val_labeled_df   = create_labeled_dataset(valid_sample['structure'], split_name='val')\n",
    "test_labeled_df  = create_labeled_dataset(test_sample['structure'], split_name='test')\n",
    "\n",
    "# Save to CSVs\n",
    "train_labeled_df.to_csv(\"labeled_dataset_train.csv\", index=False)\n",
    "val_labeled_df.to_csv(\"labeled_dataset_val.csv\", index=False)\n",
    "test_labeled_df.to_csv(\"labeled_dataset_test.csv\", index=False)\n",
    "\n",
    "print(\" All labeled datasets saved: train, val, test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1254f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper: Load structure and embeddings\n",
    "def load_data_for_split(split_name):\n",
    "    df = pd.read_csv(f\"labeled_dataset_{split_name}.csv\")\n",
    "    embeddings = torch.load(f\"{split_name}_sentence_embeddings.pt\")\n",
    "    return df, embeddings\n",
    "\n",
    "# Helper: Extract sentences + mapping\n",
    "def extract_all_sentences(structured_articles):\n",
    "    all_sentences = []\n",
    "    mapping = []  # (doc_id, para_id, sent_id)\n",
    "    for doc_id, article_structure in enumerate(structured_articles):\n",
    "        for para in article_structure:\n",
    "            para_id = para['paragraph_id']\n",
    "            for sent_id, sentence in enumerate(para['sentences']):\n",
    "                all_sentences.append(sentence)\n",
    "                mapping.append((doc_id, para_id, sent_id))\n",
    "    return all_sentences, mapping\n",
    "\n",
    "# Processing function\n",
    "def process_labeled_split(split_name, sample_df):\n",
    "    print(f\"\\n Processing split: {split_name}\")\n",
    "\n",
    "    # Step 1: Load labeled sentences and embeddings\n",
    "    labeled_df, embeddings = load_data_for_split(split_name)\n",
    "    print(f\" Loaded labeled dataset: {len(labeled_df)} rows\")\n",
    "    print(f\" Loaded embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Step 2: Extract sentences and map from structure\n",
    "    structured_articles = sample_df['structure'].tolist()\n",
    "    sentences, sentence_map = extract_all_sentences(structured_articles)\n",
    "\n",
    "    if len(sentences) != embeddings.shape[0]:\n",
    "        print(\" Warning: Sentence count mismatch with embeddings. Truncating.\")\n",
    "        sentence_map = sentence_map[:embeddings.shape[0]]\n",
    "        sentences = sentences[:embeddings.shape[0]]\n",
    "\n",
    "    # Step 3: Create sentence map DataFrame\n",
    "    map_df = pd.DataFrame(sentence_map, columns=['doc_id', 'para_id', 'sent_id'])\n",
    "    map_df['embedding_idx'] = map_df.index\n",
    "\n",
    "    # Step 4: Merge\n",
    "    merged_df = labeled_df.merge(map_df, on=['doc_id', 'para_id', 'sent_id'], how='left')\n",
    "    missing = merged_df['embedding_idx'].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"  {missing} sentences missing embeddings. Dropping them.\")\n",
    "        merged_df = merged_df.dropna(subset=['embedding_idx'])\n",
    "\n",
    "    # Step 5: Assign embeddings\n",
    "    embeddings_list = []\n",
    "    for idx in tqdm(merged_df['embedding_idx'], desc=f\"{split_name} - Assigning embeddings\"):\n",
    "        idx = int(idx)\n",
    "        if idx < embeddings.shape[0]:\n",
    "            embedding = embeddings[idx].cpu().numpy()\n",
    "            embeddings_list.append(embedding)\n",
    "        else:\n",
    "            embeddings_list.append(None)\n",
    "    merged_df['embedding'] = embeddings_list\n",
    "\n",
    "    # Step 6: Save\n",
    "    merged_df.to_pickle(f\"labeled_dataset_with_embeddings_{split_name}.pkl\")\n",
    "    print(f\"  Saved: labeled_dataset_with_embeddings_{split_name}.pkl ({len(merged_df)} rows)\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'split': split_name,\n",
    "        'num_articles': len(sample_df),\n",
    "        'num_sentences': len(sentences),\n",
    "        'sentence_map': sentence_map[:10],\n",
    "        'sample_sentences': sentences[:10]\n",
    "    }\n",
    "    with open(f\"stage1_output_metadata_{split_name}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\" Metadata saved: stage1_output_metadata_{split_name}.json\")\n",
    "\n",
    "# Apply to all splits\n",
    "process_labeled_split(\"train\", train_sample)\n",
    "process_labeled_split(\"val\", valid_sample)\n",
    "process_labeled_split(\"test\", test_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb72217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Load updated 1% split datasets\n",
    "train_df = pd.read_pickle('labeled_dataset_with_embeddings_train.pkl')\n",
    "val_df   = pd.read_pickle('labeled_dataset_with_embeddings_val.pkl')\n",
    "test_df  = pd.read_pickle('labeled_dataset_with_embeddings_test.pkl')\n",
    "\n",
    "print(f\"Loaded: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['label_encoded']   = label_encoder.transform(val_df['label'])\n",
    "test_df['label_encoded']  = label_encoder.transform(test_df['label'])\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Convert to datasets\n",
    "train_dataset = TextDataset(list(train_df['embedding']), train_df['label_encoded'].values)\n",
    "val_dataset   = TextDataset(list(val_df['embedding']),   val_df['label_encoded'].values)\n",
    "test_dataset  = TextDataset(list(test_df['embedding']),  test_df['label_encoded'].values)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32)\n",
    "\n",
    "# Model definition\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cpu')  # or 'cuda' if using GPU\n",
    "model = Classifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Save final model and label encoder\n",
    "torch.save(model.state_dict(), 'classifier_model.pt')\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\" Step 3: Final model and label encoder saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e767966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People enjoyed temperatures of 17C at Brighton beach in West Sussex and Weymouth in Dorset .\n",
      "Asda claims it will sell a million sausages over long weekend despite night temperatures dropping to minus 1C .\n",
      "But the good weather has not been enjoyed by all as the north west and Scotland have seen heavy rain .\n"
     ]
    }
   ],
   "source": [
    "print(train_sample[\"highlights\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54339d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_Attn_Classifier(\n",
       "  (bilstm): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
       "  (attention_fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load trained components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BiLSTM + Attention Classifier\n",
    "class BiLSTM_Attn_Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTM_Attn_Classifier, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.attention_fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        attn_weights = F.softmax(self.attention_fc(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "        output = self.classifier(context_vector)\n",
    "        return output\n",
    "\n",
    "# Load model and label encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "input_dim = 768\n",
    "hidden_dim = 256\n",
    "output_dim = len(label_encoder.classes_)\n",
    "\n",
    "model = BiLSTM_Attn_Classifier(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('classifier_model.pt', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab0c20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'attention_fc.weight', 'attention_fc.bias', 'classifier.weight', 'classifier.bias'])\n",
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint = torch.load('classifier_model.pt', map_location='cpu')\n",
    "print(checkpoint.keys())  # View state_dict keys\n",
    "print(checkpoint['classifier.weight'].shape)  # Confirm output_dim=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8977347",
   "metadata": {},
   "source": [
    "preprocessing and structural parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e1a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eggon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Segmenting all articles into paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting Articles: 100%|██████████| 2871/2871 [00:00<00:00, 6299.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2: Embedding + Classifying 2871 paragraphs...\n",
      " Embedding 2871 paragraphs...\n",
      " Predicting labels using BiLSTM-Attn classifier...\n",
      " Step 3: Grouping results back into article structure...\n",
      " Step 4: Assigning structure and article_id back to DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Structure: 100%|██████████| 2871/2871 [00:00<00:00, 950497.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stage 1 complete.\n",
      " Step 1: Segmenting all articles into paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting Articles: 100%|██████████| 134/134 [00:00<00:00, 2543.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2: Embedding + Classifying 134 paragraphs...\n",
      " Embedding 134 paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicting labels using BiLSTM-Attn classifier...\n",
      " Step 3: Grouping results back into article structure...\n",
      " Step 4: Assigning structure and article_id back to DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Structure: 100%|██████████| 134/134 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stage 1 complete.\n",
      " Step 1: Segmenting all articles into paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting Articles: 100%|██████████| 115/115 [00:00<00:00, 1970.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2: Embedding + Classifying 115 paragraphs...\n",
      " Embedding 115 paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicting labels using BiLSTM-Attn classifier...\n",
      " Step 3: Grouping results back into article structure...\n",
      " Step 4: Assigning structure and article_id back to DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Structure: 100%|██████████| 115/115 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stage 1 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Ensure nltk is installed and download required data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the model once\n",
    "bert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Helper: Extract sentences + mapping\n",
    "def extract_all_sentences(structured_articles):\n",
    "    all_sentences = []\n",
    "    mapping = []  # (doc_id, para_id, sent_id)\n",
    "\n",
    "    for doc_id, article_structure in enumerate(structured_articles):\n",
    "        for para in article_structure:\n",
    "            para_id = para['paragraph_id']\n",
    "            for sent_id, sentence in enumerate(para['sentences']):\n",
    "                all_sentences.append(sentence)\n",
    "                mapping.append((doc_id, para_id, sent_id))\n",
    "    return all_sentences, mapping\n",
    "\n",
    "# Helper: Process the DataFrame to extract structure using trained model\n",
    "def process_article(article, article_idx):\n",
    "    if pd.isna(article) or not article.strip():\n",
    "        return []\n",
    "    paragraphs = re.split(r'\\n{2,}|\\n', article)  # Split based on double newlines or single newlines\n",
    "    return [{'id': f'{article_idx}_{i}', 'text': p.strip()} for i, p in enumerate(paragraphs) if p.strip()]\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return [s for s in nltk.sent_tokenize(text) if any(len(w) >= 3 for w in nltk.word_tokenize(s) if w.isalnum())]\n",
    "\n",
    "def embed_and_classify(paragraphs, model, label_encoder, batch_size=128):\n",
    "    texts = [p['text'] for p in paragraphs]\n",
    "    para_ids = [p['id'] for p in paragraphs]\n",
    "\n",
    "    print(f\" Embedding {len(texts)} paragraphs...\")\n",
    "    embeddings = bert_model.encode(texts, convert_to_tensor=True, batch_size=batch_size)\n",
    "\n",
    "    # Assuming you have your BiLSTM-Attn model here for classification\n",
    "    print(\" Predicting labels using BiLSTM-Attn classifier...\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(embeddings)  # Use your trained BiLSTM-Attn model here\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = label_encoder.inverse_transform(preds)  # Assuming you have a label encoder\n",
    "\n",
    "    results = []\n",
    "    for pid, label, text in zip(para_ids, labels, texts):\n",
    "        aid, para_idx = map(int, pid.split('_'))\n",
    "        sentences = tokenize_sentences(text)\n",
    "        if sentences:\n",
    "            results.append({\n",
    "                'article_id': aid,\n",
    "                'paragraph_id': para_idx,\n",
    "                'type': label,\n",
    "                'sentences': sentences\n",
    "            })\n",
    "    return results\n",
    "\n",
    "def process_dataframe(df, model, label_encoder):\n",
    "    print(\" Step 1: Segmenting all articles into paragraphs...\")\n",
    "    all_paragraphs = []\n",
    "    for idx, article in tqdm(enumerate(df['article']), total=len(df), desc=\"Segmenting Articles\"):\n",
    "        segs = process_article(article, idx)\n",
    "        all_paragraphs.extend(segs)\n",
    "\n",
    "    print(f\" Step 2: Embedding + Classifying {len(all_paragraphs)} paragraphs...\")\n",
    "    results = embed_and_classify(all_paragraphs, model, label_encoder)\n",
    "\n",
    "    print(\" Step 3: Grouping results back into article structure...\")\n",
    "    structured_by_article = {}\n",
    "\n",
    "    for r in results:\n",
    "        aid = r['article_id']\n",
    "        structured_by_article.setdefault(aid, []).append({\n",
    "            'paragraph_id': r['paragraph_id'],\n",
    "            'type': r['type'],\n",
    "            'sentences': r['sentences']\n",
    "        })  \n",
    "\n",
    "    print(\" Step 4: Assigning structure and article_id back to DataFrame...\")\n",
    "\n",
    "    # This ensures proper 1:1 mapping\n",
    "    df['structure'] = [structured_by_article.get(i, []) for i in tqdm(range(len(df)), desc=\"Finalizing Structure\")]\n",
    "    df['article_id'] = df.index  # Assigning article_id from DataFrame index (guaranteed match)\n",
    "\n",
    "    print(\" Stage 1 complete.\")\n",
    "    return df['structure']\n",
    "\n",
    "\n",
    "# Example assumes you've already run classification and have model/label_encoder loaded\n",
    "\n",
    "# Process and update the DataFrame with structure for train, valid, and test datasets\n",
    "train_sample['structure'] = process_dataframe(train_sample, model, label_encoder)\n",
    "valid_sample['structure'] = process_dataframe(valid_sample, model, label_encoder)\n",
    "test_sample['structure'] = process_dataframe(test_sample, model, label_encoder)\n",
    "\n",
    "# Save them with structure and article_id so you can reuse later\n",
    "train_sample.to_pickle(\"train_with_structure_and_article_id.pkl\")\n",
    "valid_sample.to_pickle(\"val_with_structure_and_article_id.pkl\")\n",
    "test_sample.to_pickle(\"test_with_structure_and_article_id.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4d9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b775c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Preview:                                             id  \\\n",
      "0     ed0fed726929c1eeabe6c390e47128dbb7d7a055   \n",
      "1     023cd84001b33aed4ff0f3f5ecb0fdd2151cf543   \n",
      "2     6a70a0d8d3ed365fe1df6d35f1587a8b9b298618   \n",
      "3     b37204c13ea38b511265e41ac69fb12acfb63f85   \n",
      "4     c24e5805afd5145bc48410e876db91d44a06be5e   \n",
      "...                                        ...   \n",
      "2866  9ddececa81dae92cb8a8f385f5fdbb6e138aa3f4   \n",
      "2867  c87fc65a0beeae74cc0dc411c2103a52968e1374   \n",
      "2868  dc9e61006c1e5689d6f8c99ef2703ac49659e308   \n",
      "2869  ba355bd2f4d3b4abf9381dd26b1e78e254c89027   \n",
      "2870  ccc2b5c8f53847c876df383f714b02097f70162c   \n",
      "\n",
      "                                                article  \\\n",
      "0     By . Mia De Graaf . Britons flocked to beaches...   \n",
      "1     A couple who weighed a combined 32st were sham...   \n",
      "2     Video footage shows the heart stopping moment ...   \n",
      "3     Istanbul, Turkey (CNN) -- About 250 people rac...   \n",
      "4     By . Daily Mail Reporter . PUBLISHED: . 12:53 ...   \n",
      "...                                                 ...   \n",
      "2866  (CNN) -- Casey Anthony, accused of killing her...   \n",
      "2867  American gymnast Aly Raisman has revealed the ...   \n",
      "2868  By . Matt Blake . PUBLISHED: . 06:59 EST, 29 N...   \n",
      "2869  NEW YORK (CNN) -- Ten members of an internatio...   \n",
      "2870  (CNN) -- An attorney representing the family o...   \n",
      "\n",
      "                                             highlights  \\\n",
      "0     People enjoyed temperatures of 17C at Brighton...   \n",
      "1     Couple started piling on pounds after the birt...   \n",
      "2     A 17-year-old boy suffering lacerations to his...   \n",
      "3     Syrians citizens hightail it to Turkey .\\nMost...   \n",
      "4     The Xue Long had provided the helicopter that ...   \n",
      "...                                                 ...   \n",
      "2866  NEW: Jurors see a videotape of a November 2008...   \n",
      "2867  Aly, from Massachusetts,  she said it made her...   \n",
      "2868  The cow got its head stuck in the seat in a fi...   \n",
      "2869  Officials say the original versions of the goo...   \n",
      "2870  Attorney for family of girl killed in police r...   \n",
      "\n",
      "                                              structure  article_id  \n",
      "0     [{'paragraph_id': 0, 'type': 'paragraph', 'sen...           0  \n",
      "1     [{'paragraph_id': 0, 'type': 'paragraph', 'sen...           1  \n",
      "2     [{'paragraph_id': 0, 'type': 'paragraph', 'sen...           2  \n",
      "3     [{'paragraph_id': 0, 'type': 'paragraph', 'sen...           3  \n",
      "4     [{'paragraph_id': 0, 'type': 'paragraph', 'sen...           4  \n",
      "...                                                 ...         ...  \n",
      "2866  [{'paragraph_id': 0, 'type': 'subheader', 'sen...        2866  \n",
      "2867  [{'paragraph_id': 0, 'type': 'paragraph', 'sen...        2867  \n",
      "2868  [{'paragraph_id': 0, 'type': 'paragraph', 'sen...        2868  \n",
      "2869  [{'paragraph_id': 0, 'type': 'paragraph', 'sen...        2869  \n",
      "2870  [{'paragraph_id': 0, 'type': 'paragraph', 'sen...        2870  \n",
      "\n",
      "[2871 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_with_structure_and_article_id.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Check and print 2 full-length articles\n",
    "if isinstance(data, list):\n",
    "    for i in range(min(2, len(data))):\n",
    "        print(f\"Article {i+1}:\\n{data[i]}\\n{'-'*80}\")\n",
    "elif isinstance(data, dict):\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        print(f\"Article {i+1} ID: {key}\\nContent:\\n{value}\\n{'-'*80}\")\n",
    "        if i == 1:\n",
    "            break\n",
    "else:\n",
    "    print(\"Unsupported data type:\", type(data))\n",
    "    print(\"Preview:\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'article', 'highlights', 'structure', 'article_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_sample.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e006a6",
   "metadata": {},
   "source": [
    "Embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating MPNet embeddings: 100%|██████████| 120565/120565 [1:56:10<00:00, 17.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sentence-level dataset with all-mpnet-base-v2 embeddings: final_sentence_level_dataset_with_all_mpnet.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data from train, validation, and test files\n",
    "train_df = pd.read_pickle(\"train_with_structure_and_article_id.pkl\")\n",
    "val_df = pd.read_pickle(\"val_with_structure_and_article_id.pkl\")\n",
    "test_df = pd.read_pickle(\"test_with_structure_and_article_id.pkl\")\n",
    "\n",
    "# Function to flatten structure into sentence-level records\n",
    "def flatten_structure(df):\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        highlights = row.get('highlights', '')\n",
    "        structure = row.get('structure', [])\n",
    "        \n",
    "        for para in structure:\n",
    "            para_id = para['paragraph_id']\n",
    "            for sentence in para['sentences']:\n",
    "                records.append({\n",
    "                    'article_id': article_id,\n",
    "                    'paragraph_id': para_id,\n",
    "                    'sentence': sentence,\n",
    "                    'highlights': highlights\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Flatten all splits (train, val, test)\n",
    "train_sentence_df = flatten_structure(train_df)\n",
    "val_sentence_df = flatten_structure(val_df)\n",
    "test_sentence_df = flatten_structure(test_df)\n",
    "\n",
    "# Concatenate all splits into a single dataframe for processing\n",
    "sentence_df = pd.concat([train_sentence_df, val_sentence_df, test_sentence_df], ignore_index=True)\n",
    "\n",
    "# Load pretrained MPNet model for sentence embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for sentence in tqdm(sentence_df['sentence'], desc=\"Generating MPNet embeddings\"):\n",
    "    emb = model.encode(sentence)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Add embeddings to the dataframe\n",
    "sentence_df['embedding'] = embeddings\n",
    "\n",
    "# Save the sentence-level dataset with embeddings\n",
    "sentence_df.to_pickle(\"final_sentence_level_dataset_with_all_mpnet.pkl\")\n",
    "print(\"Saved sentence-level dataset with all-mpnet-base-v2 embeddings: final_sentence_level_dataset_with_all_mpnet.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a34c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_id  paragraph_id  \\\n",
      "0           0             0   \n",
      "1           0             0   \n",
      "2           0             0   \n",
      "3           0             0   \n",
      "4           0             0   \n",
      "\n",
      "                                            sentence  \\\n",
      "0                                     Mia De Graaf .   \n",
      "1  Britons flocked to beaches across the southern...   \n",
      "2  Temperatures soared to 17C in Brighton and Dor...   \n",
      "3  Figures from Asda suggest the unexpected sunsh...   \n",
      "4  Sun's out: Brighton beach was packed with Brit...   \n",
      "\n",
      "                                          highlights  \\\n",
      "0  People enjoyed temperatures of 17C at Brighton...   \n",
      "1  People enjoyed temperatures of 17C at Brighton...   \n",
      "2  People enjoyed temperatures of 17C at Brighton...   \n",
      "3  People enjoyed temperatures of 17C at Brighton...   \n",
      "4  People enjoyed temperatures of 17C at Brighton...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [0.017809153, 0.09870978, 0.032285955, -0.0337...  \n",
      "1  [-0.026629565, -0.031049574, -0.003912661, -0....  \n",
      "2  [-0.08872982, -0.10456379, 0.019644251, 0.0163...  \n",
      "3  [0.013843446, 0.0053730276, -0.01078347, -0.02...  \n",
      "4  [-0.03601021, -0.090377316, 0.008455765, -0.00...  \n",
      "\n",
      "🧾 Columns in the dataset:\n",
      "['article_id', 'paragraph_id', 'sentence', 'highlights', 'embedding']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_pickle(\"final_sentence_level_dataset_with_all_mpnet.pkl\")\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check column names\n",
    "print(\"\\n🧾 Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55384",
   "metadata": {},
   "source": [
    "K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic clustering and segmentation completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  11%|█         | 1/9 [08:44<1:09:53, 524.19s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=2 → Silhouette Score: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  22%|██▏       | 2/9 [15:08<51:34, 442.06s/K]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=3 → Silhouette Score: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  33%|███▎      | 3/9 [21:33<41:34, 415.75s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=4 → Silhouette Score: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  44%|████▍     | 4/9 [27:55<33:31, 402.36s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=5 → Silhouette Score: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  56%|█████▌    | 5/9 [34:16<26:19, 394.82s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=6 → Silhouette Score: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  67%|██████▋   | 6/9 [40:41<19:34, 391.34s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=7 → Silhouette Score: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  78%|███████▊  | 7/9 [47:02<12:56, 388.12s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=8 → Silhouette Score: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K:  89%|████████▉ | 8/9 [53:23<06:25, 385.81s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=9 → Silhouette Score: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing best K: 100%|██████████| 9/9 [59:45<00:00, 398.36s/K]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=10 → Silhouette Score: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best K found: 10 with score 0.0192\n",
      "Saved sentence-level dataset with dynamic cluster_id as: sentence_level_dataset_with_clusters.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving clustered sections: 100%|██████████| 120565/120565 [00:03<00:00, 33635.21sentence/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic segmentation into 10 clusters complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset with MPNet embeddings\n",
    "df = pd.read_pickle(\"final_sentence_level_dataset_with_all_mpnet.pkl\")\n",
    "embeddings = np.vstack(df['embedding'].values)\n",
    "\n",
    "# Step 1: Automatically choose best K using silhouette score\n",
    "def choose_best_k(embeddings, min_k=2, max_k=10):\n",
    "    best_score = -1\n",
    "    best_k = min_k\n",
    "    scores = []\n",
    "    \n",
    "    for k in tqdm(range(min_k, max_k + 1), desc=\"Choosing best K\", unit=\"K\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)  # Removed n_jobs\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        scores.append(score)\n",
    "        print(f\"K={k} → Silhouette Score: {score:.4f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "\n",
    "    # Plot scores for reference\n",
    "    plt.plot(range(min_k, max_k + 1), scores, marker='o')\n",
    "    plt.title(\"Silhouette Scores vs K\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\" Best K found: {best_k} with score {best_score:.4f}\")\n",
    "    return best_k\n",
    "\n",
    "# Show progress after completion of step 1\n",
    "tqdm.write(\"Semantic clustering and segmentation completed successfully!\")\n",
    "\n",
    "# Find the best K dynamically\n",
    "best_k = choose_best_k(embeddings, min_k=2, max_k=10)\n",
    "\n",
    "# Step 2: Perform KMeans with best K\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)  # Removed n_jobs\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "df['cluster_id'] = cluster_labels\n",
    "\n",
    "# Save updated DataFrame\n",
    "df.to_pickle(\"sentence_level_dataset_with_clusters.pkl\")\n",
    "print(\"Saved sentence-level dataset with dynamic cluster_id as: sentence_level_dataset_with_clusters.pkl\")\n",
    "\n",
    "# Step 3: Save clustered sections\n",
    "sections_by_cluster = defaultdict(list)\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Saving clustered sections\", unit=\"sentence\"):\n",
    "    sections_by_cluster[row['cluster_id']].append(row['sentence'])\n",
    "\n",
    "with open(\"semantic_clusters.json\", \"w\") as f:\n",
    "    json.dump({f\"Cluster_{k}\": v for k, v in sections_by_cluster.items()}, f, indent=2)\n",
    "\n",
    "# Show final progress message\n",
    "tqdm.write(f\"Semantic segmentation into {best_k} clusters complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Sentences from Clusters ===\n",
      "\n",
      "\n",
      "--- Cluster 0 ---\n",
      "1. is expected to cast over northern and western areas of the country, with .\n",
      "2. Istanbul, Turkey (CNN) -- About 250 people raced across the Syrian border into Turkey, government officials said Saturday, a flight that reflects the fear and violence gripping the Arab nation.\n",
      "3. Turkish Foreign Ministry spokesman Selcuk Unal said the government is trying to determine more about the people and how and why they chose to leave Syria.\n",
      "4. \"They just came to the border post and want to go in without passports.\n",
      "5. Yaylidagi Mayor Mustafa Kemal Dagistanli and another local government official, who asked not to be named, said the people are Syrian citizens and Muslims.\n",
      "\n",
      "--- Cluster 1 ---\n",
      "1. A couple who weighed a combined 32st were shamed into slimming by their own family - during Christmas dinner.\n",
      "2. Our weight loss has helped the kids as well.\n",
      "3. 'Weight Watchers is easy to follow.\n",
      "4. We tried other diets before, but this is the one we have done well with.\n",
      "5. 'We used to eat fairly healthily, but on a weekend, we would go to McDonald's with the children and we would go to the local chip shop one night a week.\n",
      "\n",
      "--- Cluster 2 ---\n",
      "1. manager John Young said.\n",
      "2. A man with a tennis ball dangling from his shorts will have to do.\n",
      "3. Three circular goals are used at each end of the field, as per the books.\n",
      "4. Filmmaker Farzad Sangari, who raised almost $27,000 on Kickstarter to make the documentary, says: \"If anybody actually watched it or played it, they would realize the amount of athleticism and mental dexterity it takes to play this sport.\"\n",
      "5. Therein lies the dilemma for quidditch: how to take its next step and, like a sporting Pinocchio, become a real boy.\n",
      "\n",
      "--- Cluster 3 ---\n",
      "1. Britons flocked to beaches across the southern coast yesterday as millions look set to bask in glorious sunshine today.\n",
      "2. Temperatures soared to 17C in Brighton and Dorset, with people starting their long weekend in deck chairs by the sea.\n",
      "3. Figures from Asda suggest the unexpected sunshine has also inspired a wave of impromptu barbecues, with sales of sausages and equipment expected to triple those in April.\n",
      "4. Sun's out: Brighton beach was packed with Britons enjoying the unexpected sunshine to start the long weekend as temperatures hit 17C across the south coast .\n",
      "5. Although frost is set to hit the south tonight - with temperatures dropping to 1C - Britons stocking up for a barbecue will be in luck tomorrow, with forecasters predicting dry and sunny weather across southern England, southern Wales and the south Midlands.\n",
      "\n",
      "--- Cluster 4 ---\n",
      "1. A NSW police spokeswoman told Daily Mail Australia police were aware of the situation.\n",
      "2. You are a snitch.\n",
      "3. Don't get caught, or the game ends.\n",
      "4. \"You are going to lose on [unfair] snitch catches,\" he continues.\n",
      "5. (CNN) -- PepsiCo is ending its relationship with rapper Lil Wayne over what the company calls an \"offensive reference to a revered civil rights icon\" -- 14-year-old Emmett Till, who was killed nearly 60 years ago.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load cluster-assigned dataset\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_clusters.pkl\")\n",
    "\n",
    "# Load cluster-to-sentences mapping\n",
    "with open(\"semantic_clusters.json\", \"r\") as f:\n",
    "    sections_by_cluster = json.load(f)\n",
    "\n",
    "# Convert keys to integers\n",
    "sections_by_cluster = {int(k.replace(\"Cluster_\", \"\")): v for k, v in sections_by_cluster.items()}\n",
    "\n",
    "# Print sample sentences from some clusters\n",
    "best_k = df['cluster_id'].nunique()\n",
    "num_clusters_to_print = min(5, best_k)\n",
    "num_sentences_to_show = 5\n",
    "\n",
    "print(\"\\n=== Sample Sentences from Clusters ===\\n\")\n",
    "for cluster_id in range(num_clusters_to_print):\n",
    "    print(f\"\\n--- Cluster {cluster_id} ---\")\n",
    "    cluster_sentences = sections_by_cluster[cluster_id][:num_sentences_to_show]\n",
    "    for idx, sentence in enumerate(cluster_sentences, 1):\n",
    "        print(f\"{idx}. {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780d004",
   "metadata": {},
   "source": [
    "Semantic segmentation to cluster sentences into sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af93d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Semantic segmentation complete. Sections assigned.\n",
      "Saved to: sentence_level_dataset_with_sections.pkl\n",
      "   article_id  section_id  paragraph_id  \\\n",
      "0           0           0             0   \n",
      "1           0           1             0   \n",
      "2           0           1             0   \n",
      "\n",
      "                                            sentence  \\\n",
      "0                                     Mia De Graaf .   \n",
      "1  Britons flocked to beaches across the southern...   \n",
      "2  Temperatures soared to 17C in Brighton and Dor...   \n",
      "\n",
      "                                          highlights  \\\n",
      "0  People enjoyed temperatures of 17C at Brighton...   \n",
      "1  People enjoyed temperatures of 17C at Brighton...   \n",
      "2  People enjoyed temperatures of 17C at Brighton...   \n",
      "\n",
      "                                           embedding  cluster_id  \n",
      "0  [0.017809153, 0.09870978, 0.032285955, -0.0337...           8  \n",
      "1  [-0.026629565, -0.031049574, -0.003912661, -0....           3  \n",
      "2  [-0.08872982, -0.10456379, 0.019644251, 0.0163...           3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Saved to: sentence_level_dataset_with_sections.pkl\"\n",
    "# Step 1: Load dataset with clusters\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_clusters.pkl\")\n",
    "\n",
    "# Step 2: Assign section_id based on (article_id, cluster_id) grouping\n",
    "df['section_id'] = df.groupby(['article_id', 'cluster_id'], sort=False).ngroup()\n",
    "\n",
    "# Step 3: Reorder columns for clarity\n",
    "df = df[['article_id', 'section_id', 'paragraph_id', 'sentence', 'highlights', 'embedding', 'cluster_id']]\n",
    "\n",
    "# Step 4: Save the updated DataFrame for future use\n",
    "df.to_pickle(\"sentence_level_dataset_with_sections.pkl\")\n",
    "\n",
    "# Optional: View a few rows\n",
    "print(\" Semantic segmentation complete. Sections assigned.\")\n",
    "print(f\"Saved to: sentence_level_dataset_with_sections.pkl\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1829f",
   "metadata": {},
   "source": [
    " Semantic Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f323fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 120565 sentences.\n",
      "Available columns: ['article_id', 'section_id', 'paragraph_id', 'sentence', 'highlights', 'embedding', 'cluster_id']\n",
      "Created 'doc_id' from 'article_id'.\n",
      "Applied Label Encoding to 'section_id' using the pre-saved encoder and added 'label_encoded' column.\n",
      "Generating section embeddings grouped by (doc_id, section_id)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sections: 100%|██████████| 15697/15697 [00:01<00:00, 10622.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying contextual enhancement to each sentence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120565/120565 [00:01<00:00, 81796.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enhanced dataset with 'doc_id' and context-aware embeddings to: sentence_level_dataset_with_contextual_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Enable tqdm progress bars for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load dataset with sentence embeddings\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_sections.pkl\")\n",
    "print(f\"Loaded dataset with {len(df)} sentences.\")\n",
    "print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "# Step 1: Ensure 'section_id' is available\n",
    "if 'section_id' not in df.columns:\n",
    "    raise KeyError(\"'section_id' column is missing in the DataFrame.\")\n",
    "\n",
    "# Step 2: Ensure doc_id exists\n",
    "if 'doc_id' not in df.columns:\n",
    "    if 'article_id' in df.columns:\n",
    "        df['doc_id'] = df['article_id']\n",
    "        print(\"Created 'doc_id' from 'article_id'.\")\n",
    "    else:\n",
    "        print(\"'doc_id' and 'article_id' not found. Creating fallback 'doc_id' based on section breaks.\")\n",
    "        df['doc_id'] = (df['section_id'] != df['section_id'].shift()).cumsum()\n",
    "\n",
    "# Step 3: Convert sentence embeddings to NumPy arrays\n",
    "df['sentence_embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# Step 4: Load the pre-saved LabelEncoder (no import, using pre-trained one)\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Step 5: Convert all values in the 'section_id' column to string or integer consistently\n",
    "# Here, we convert them to strings to avoid mixing types\n",
    "df['section_id'] = df['section_id'].astype(str)\n",
    "\n",
    "# Step 6: Apply the pre-saved LabelEncoder to the section_id column\n",
    "df['label_encoded'] = label_encoder.transform(df['section_id'])\n",
    "print(\"Applied Label Encoding to 'section_id' using the pre-saved encoder and added 'label_encoded' column.\")\n",
    "\n",
    "# Set blending factor\n",
    "alpha = 0.6  # You can adjust this value\n",
    "\n",
    "# Step 7: Compute section-level embeddings (mean of sentence embeddings per section)\n",
    "print(\"Generating section embeddings grouped by (doc_id, section_id)...\")\n",
    "section_embeddings = {}\n",
    "for (doc_id, section_id), group in tqdm(df.groupby(['doc_id', 'section_id']), desc=\"Sections\"):\n",
    "    section_embeddings[(doc_id, section_id)] = np.mean(group['sentence_embedding'].tolist(), axis=0)\n",
    "\n",
    "# Step 8: Define enhancement function\n",
    "def enhance_embedding(row, alpha, section_embeddings):\n",
    "    v_i = row['sentence_embedding']\n",
    "    key = (row['doc_id'], row['section_id'])\n",
    "    u_j = section_embeddings.get(key, None)  # Fetch section embedding\n",
    "    if u_j is not None:\n",
    "        return alpha * v_i + (1 - alpha) * u_j  # Apply enhancement formula\n",
    "    else:\n",
    "        return v_i  # If no section embedding, return original sentence embedding\n",
    "\n",
    "# Step 9: Apply contextual enhancement to each sentence\n",
    "print(\"Applying contextual enhancement to each sentence...\")\n",
    "df['enhanced_embedding'] = df.progress_apply(\n",
    "    lambda row: enhance_embedding(row, alpha, section_embeddings),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 10: Save updated dataset with enhanced embeddings\n",
    "output_path = \"sentence_level_dataset_with_contextual_embeddings.pkl\"\n",
    "df.to_pickle(output_path)\n",
    "print(f\"Saved enhanced dataset with 'doc_id' and context-aware embeddings to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab898629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 120565 sentences.\n",
      "Available columns: ['article_id', 'section_id', 'paragraph_id', 'sentence', 'highlights', 'embedding', 'cluster_id']\n",
      "Created 'doc_id' from 'article_id'.\n",
      "Applied Label Encoding to 'section_id' using the pre-saved encoder and added 'label_encoded' column.\n",
      "Generating section embeddings grouped by (doc_id, section_id)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sections: 100%|██████████| 15697/15697 [00:01<00:00, 11849.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying contextual enhancement to each sentence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120565/120565 [00:01<00:00, 81321.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enhanced dataset with 'doc_id' and context-aware embeddings to: sentence_level_dataset_with_contextual_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Enable tqdm progress bars for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load dataset with sentence embeddings\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_sections.pkl\")\n",
    "print(f\"Loaded dataset with {len(df)} sentences.\")\n",
    "print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "# Step 1: Ensure 'section_id' is available\n",
    "if 'section_id' not in df.columns:\n",
    "    raise KeyError(\"'section_id' column is missing in the DataFrame.\")\n",
    "\n",
    "# Step 2: Ensure doc_id exists\n",
    "if 'doc_id' not in df.columns:\n",
    "    if 'article_id' in df.columns:\n",
    "        df['doc_id'] = df['article_id']\n",
    "        print(\"Created 'doc_id' from 'article_id'.\")\n",
    "    else:\n",
    "        print(\"'doc_id' and 'article_id' not found. Creating fallback 'doc_id' based on section breaks.\")\n",
    "        df['doc_id'] = (df['section_id'] != df['section_id'].shift()).cumsum()\n",
    "\n",
    "# Step 3: Convert sentence embeddings to NumPy arrays\n",
    "df['sentence_embedding'] = df['embedding'].apply(lambda x: np.array(x))\n",
    "\n",
    "# Step 4: Load the pre-saved LabelEncoder (no import, using pre-trained one)\n",
    "# This step assumes that 'label_encoder.pkl' contains the correct pre-trained encoder\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Step 5: Convert all values in the 'section_id' column to string to maintain consistency\n",
    "df['section_id'] = df['section_id'].astype(str)\n",
    "\n",
    "# Step 6: Apply the pre-saved LabelEncoder to the section_id column\n",
    "# Use the pre-trained label encoder to transform section IDs to numeric labels\n",
    "try:\n",
    "    df['label_encoded'] = label_encoder.transform(df['section_id'])\n",
    "    print(\"Applied Label Encoding to 'section_id' using the pre-saved encoder and added 'label_encoded' column.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error in Label Encoding: {e}\")\n",
    "    print(\"Ensure that 'section_id' values in the dataset are consistent with the pre-trained encoder.\")\n",
    "\n",
    "# Set blending factor\n",
    "alpha = 0.6  # You can adjust this value for enhancement\n",
    "\n",
    "# Step 7: Compute section-level embeddings (mean of sentence embeddings per section)\n",
    "# For each section in each document, compute the mean sentence embedding\n",
    "print(\"Generating section embeddings grouped by (doc_id, section_id)...\")\n",
    "section_embeddings = {}\n",
    "for (doc_id, section_id), group in tqdm(df.groupby(['doc_id', 'section_id']), desc=\"Sections\"):\n",
    "    section_embeddings[(doc_id, section_id)] = np.mean(group['sentence_embedding'].tolist(), axis=0)\n",
    "\n",
    "# Step 8: Define enhancement function\n",
    "# This function enhances sentence embeddings based on the section embedding\n",
    "def enhance_embedding(row, alpha, section_embeddings):\n",
    "    v_i = row['sentence_embedding']  # The sentence embedding\n",
    "    key = (row['doc_id'], row['section_id'])  # The (doc_id, section_id) key\n",
    "    u_j = section_embeddings.get(key, None)  # Fetch the section embedding\n",
    "    if u_j is not None:\n",
    "        return alpha * v_i + (1 - alpha) * u_j  # Apply the enhancement formula\n",
    "    else:\n",
    "        return v_i  # If no section embedding, return the original sentence embedding\n",
    "\n",
    "# Step 9: Apply contextual enhancement to each sentence\n",
    "# Enhance each sentence embedding based on the section embeddings\n",
    "print(\"Applying contextual enhancement to each sentence...\")\n",
    "df['enhanced_embedding'] = df.progress_apply(\n",
    "    lambda row: enhance_embedding(row, alpha, section_embeddings),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 10: Save updated dataset with enhanced embeddings\n",
    "# Save the enhanced embeddings into a new dataset\n",
    "output_path = \"sentence_level_dataset_with_contextual_embeddings.pkl\"\n",
    "df.to_pickle(output_path)\n",
    "print(f\"Saved enhanced dataset with 'doc_id' and context-aware embeddings to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69adaba",
   "metadata": {},
   "source": [
    "hierarchial construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99760a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Hierarchical Trees and Dynamic Edges (first 5 documents):\n",
      "\n",
      "Document 0 segments: [0, 1, 2, 3, 4, 6, 7, 8, 9]\n",
      "   No dynamic links above threshold.\n",
      "\n",
      "Document 1 segments: [0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "   No dynamic links above threshold.\n",
      "\n",
      "Document 2 segments: [1, 2, 3, 4, 6, 7, 9]\n",
      "   No dynamic links above threshold.\n",
      "\n",
      "Document 3 segments: [0, 1, 2, 3, 4, 6, 7, 8, 9]\n",
      "    Dynamic Links (max 8 shown):\n",
      "     Segment 9 ↔ Segment 1 | Similarity: 0.8288\n",
      "\n",
      "Document 4 segments: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    Dynamic Links (max 8 shown):\n",
      "     Segment 7 ↔ Segment 3 | Similarity: 0.7722\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "# Load enhanced sentence-level dataset\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_contextual_embeddings.pkl\")\n",
    "\n",
    "# Step 1: Check required columns\n",
    "assert 'enhanced_embedding' in df.columns, \"Missing 'enhanced_embedding' in dataset.\"\n",
    "\n",
    "# Step 2: Add hierarchical info (segment structure)\n",
    "df['sent_id'] = df.groupby('doc_id').cumcount()\n",
    "df['para_id'] = df['sent_id'] // 5\n",
    "\n",
    "# Step 3: Group sentences into segments using cluster_id\n",
    "sentence_map = list(zip(df['doc_id'], df['cluster_id'], df['sent_id']))\n",
    "embedding_array = np.vstack(df['enhanced_embedding'].to_numpy())\n",
    "\n",
    "doc_tree = defaultdict(lambda: defaultdict(list))\n",
    "for i, (doc_id, cluster_id, sent_id) in enumerate(sentence_map):\n",
    "    doc_tree[doc_id][cluster_id].append(i)\n",
    "\n",
    "# Step 4: Compute segment-level embeddings\n",
    "segment_embeddings = {}\n",
    "for doc_id, segments in doc_tree.items():\n",
    "    for cluster_id, sentence_ids in segments.items():\n",
    "        seg_embed = np.mean(embedding_array[sentence_ids], axis=0)\n",
    "        segment_embeddings[(doc_id, cluster_id)] = seg_embed\n",
    "\n",
    "# Step 5: Construct dynamic edges between segments (unsupervised)\n",
    "similarity_threshold = 0.75\n",
    "dynamic_edges = defaultdict(list)\n",
    "\n",
    "for doc_id, segments in doc_tree.items():\n",
    "    cluster_ids = list(segments.keys())\n",
    "    for i in range(len(cluster_ids)):\n",
    "        for j in range(i + 1, len(cluster_ids)):\n",
    "            cid1, cid2 = cluster_ids[i], cluster_ids[j]\n",
    "            vec1 = segment_embeddings[(doc_id, cid1)]\n",
    "            vec2 = segment_embeddings[(doc_id, cid2)]\n",
    "            sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            if sim >= similarity_threshold:\n",
    "                dynamic_edges[doc_id].append((cid1, cid2, round(float(sim), 4)))\n",
    "\n",
    "# Step 6: Save outputs\n",
    "with open(\"stage3_dynamic_edges.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        str(doc_id): [(int(a), int(b), float(s)) for a, b, s in edges]\n",
    "        for doc_id, edges in dynamic_edges.items()\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Step 7: Preview output\n",
    "print(\"\\nSample Hierarchical Trees and Dynamic Edges (first 5 documents):\")\n",
    "for doc_id in list(doc_tree.keys())[:5]:\n",
    "    print(f\"\\nDocument {doc_id} segments: {sorted(doc_tree[doc_id].keys())}\")\n",
    "    edges = dynamic_edges.get(doc_id, [])\n",
    "    if not edges:\n",
    "        print(\"   No dynamic links above threshold.\")\n",
    "    else:\n",
    "        print(\"    Dynamic Links (max 8 shown):\")\n",
    "        for a, b, s in edges[:8]:\n",
    "            print(f\"     Segment {a} ↔ Segment {b} | Similarity: {s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11360d",
   "metadata": {},
   "source": [
    "Triplet position encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c462b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120565 sentence entries from contextual embedding dataset.\n",
      "Computing mean embeddings per document...\n",
      "Computing triplets and contextual scores (Relevance only)...\n",
      "Saved triplet positions to 'stage4_triplet_positions.pt'\n",
      "Saved contextual scores to 'stage4_contextual_scores.pt'\n",
      "\n",
      "Sample Triplet Position Encoding and Contextual Scores:\n",
      "Sentence 1:\n",
      "  Triplet: (doc 0, para 0, cluster 8)\n",
      "  Contextual Score: 0.1085\n",
      "Sentence 2:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4544\n",
      "Sentence 3:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.39\n",
      "Sentence 4:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.5529\n",
      "Sentence 5:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4753\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Parameters ===\n",
    "ALPHA = 1.0  # Using full weight for relevance score only\n",
    "\n",
    "# === Load sentence-level data from Stage 1 (with embeddings, paragraph, cluster info) ===\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_contextual_embeddings.pkl\")\n",
    "print(f\"Loaded {len(df)} sentence entries from contextual embedding dataset.\")\n",
    "\n",
    "# === Compute document-level mean embeddings V_doc ===\n",
    "print(\"Computing mean embeddings per document...\")\n",
    "doc_embeddings = df.groupby(\"article_id\")[\"embedding\"].apply(\n",
    "    lambda x: np.mean(np.stack(x), axis=0)\n",
    ").to_dict()\n",
    "\n",
    "# === Apply Triplet Encoding and Contextual Score Calculation ===\n",
    "triplet_positions = []\n",
    "contextual_scores = []\n",
    "\n",
    "print(\"Computing triplets and contextual scores (Relevance only)...\")\n",
    "for idx, row in df.iterrows():\n",
    "    doc_id = row[\"article_id\"]\n",
    "    para_id = row[\"paragraph_id\"]\n",
    "    cluster_id = row[\"cluster_id\"]\n",
    "    vi = np.array(row[\"embedding\"], dtype=np.float32)\n",
    "\n",
    "    # Document-level mean embedding\n",
    "    vdoc = np.array(doc_embeddings.get(doc_id, np.zeros_like(vi)), dtype=np.float32)\n",
    "\n",
    "    # Safety check for shape\n",
    "    if vi.shape != vdoc.shape:\n",
    "        print(f\"[WARNING] Skipping row {idx} due to shape mismatch: \"\n",
    "              f\"vi shape {vi.shape}, vdoc shape {vdoc.shape}\")\n",
    "        continue\n",
    "\n",
    "    # Compute relevance score only\n",
    "    rel_score = cosine_similarity([vi], [vdoc])[0][0]\n",
    "    ci = ALPHA * rel_score  # Only relevance\n",
    "\n",
    "    triplet_positions.append((doc_id, para_id, cluster_id))\n",
    "    contextual_scores.append(float(ci))\n",
    "\n",
    "# === Save Outputs ===\n",
    "torch.save(triplet_positions, \"stage4_triplet_positions.pt\")\n",
    "torch.save(contextual_scores, \"stage4_contextual_scores.pt\")\n",
    "\n",
    "print(f\"Saved triplet positions to 'stage4_triplet_positions.pt'\")\n",
    "print(f\"Saved contextual scores to 'stage4_contextual_scores.pt'\")\n",
    "\n",
    "# === Sample Output Preview ===\n",
    "print(\"\\nSample Triplet Position Encoding and Contextual Scores:\")\n",
    "for i in range(min(5, len(triplet_positions))):\n",
    "    doc, para, clus = triplet_positions[i]\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(f\"  Triplet: (doc {doc}, para {para}, cluster {clus})\")\n",
    "    print(f\"  Contextual Score: {round(contextual_scores[i], 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd6596c",
   "metadata": {},
   "source": [
    "printing sample of triplet position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96efadcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Triplet Position Encoding and Contextual Scores:\n",
      "Sentence 1:\n",
      "  Triplet: (doc 0, para 0, cluster 8)\n",
      "  Contextual Score: 0.1085\n",
      "Sentence 2:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4544\n",
      "Sentence 3:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.39\n",
      "Sentence 4:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.5529\n",
      "Sentence 5:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4753\n",
      "Sentence 6:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.5311\n",
      "Sentence 7:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4259\n",
      "Sentence 8:\n",
      "  Triplet: (doc 0, para 0, cluster 9)\n",
      "  Contextual Score: 0.4058\n",
      "Sentence 9:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4836\n",
      "Sentence 10:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4289\n",
      "Sentence 11:\n",
      "  Triplet: (doc 0, para 0, cluster 9)\n",
      "  Contextual Score: 0.2863\n",
      "Sentence 12:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4973\n",
      "Sentence 13:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.5382\n",
      "Sentence 14:\n",
      "  Triplet: (doc 0, para 0, cluster 3)\n",
      "  Contextual Score: 0.4771\n",
      "Sentence 15:\n",
      "  Triplet: (doc 0, para 0, cluster 6)\n",
      "  Contextual Score: 0.4859\n"
     ]
    }
   ],
   "source": [
    "# === Sample Output Preview ===\n",
    "print(\"\\nSample Triplet Position Encoding and Contextual Scores:\")\n",
    "for i in range(min(15, len(triplet_positions))):\n",
    "    doc, para, clus = triplet_positions[i]\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(f\"  Triplet: (doc {doc}, para {para}, cluster {clus})\")\n",
    "    print(f\"  Contextual Score: {round(contextual_scores[i], 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd1f4d",
   "metadata": {},
   "source": [
    "Contextual Relevance Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f563c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Adjusted Scores (Stage 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120565/120565 [07:20<00:00, 273.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved scored sentences to 'stage5_scored_sentences.pkl'\n",
      "\n",
      " Sample Adjusted Scores (first 5 rows):\n",
      "   doc_id  paragraph_id  cluster_id  adjusted_score\n",
      "0       0             0           8        0.416276\n",
      "1       0             0           3        0.594504\n",
      "2       0             0           3        0.358956\n",
      "3       0             0           3        0.356710\n",
      "4       0             0           3        0.312188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load Correct Inputs from Stage 4 ===\n",
    "df = pd.read_pickle(\"sentence_level_dataset_with_contextual_embeddings.pkl\")\n",
    "triplet_positions = torch.load(\"stage4_triplet_positions.pt\")\n",
    "contextual_scores = torch.load(\"stage4_contextual_scores.pt\")\n",
    "\n",
    "# === Add triplet and contextual score to DataFrame ===\n",
    "df['triplet'] = triplet_positions\n",
    "df['contextual_score'] = contextual_scores\n",
    "\n",
    "# === Compute document-level embedding V_doc ===\n",
    "doc_embeddings = defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    doc_embeddings[row['doc_id']].append(row['enhanced_embedding'])\n",
    "\n",
    "V_doc = {doc_id: np.mean(embeds, axis=0) for doc_id, embeds in doc_embeddings.items()}\n",
    "\n",
    "# === Precompute all embeddings per document for faster contextual similarity ===\n",
    "grouped_embeddings = {\n",
    "    doc_id: np.vstack(embeds) for doc_id, embeds in doc_embeddings.items()\n",
    "}\n",
    "\n",
    "# === Function: compute contextual neighborhood coherence ===\n",
    "def compute_contextual_score(idx, emb, local_embeds, local_idx, k=3):\n",
    "    sims = cosine_similarity([emb], local_embeds)[0]\n",
    "    sims[local_idx] = 0  # ignore self\n",
    "    top_k = np.argsort(sims)[-k:]\n",
    "    return float(np.mean(sims[top_k]))\n",
    "\n",
    "# === Novelty & Redundancy — Setup ===\n",
    "seen_embeddings = defaultdict(list)\n",
    "\n",
    "# === Score parameters ===\n",
    "OMEGA = 0.3  # coherence weight\n",
    "LAMBDA = 0.3  # relevance weight\n",
    "ETA = 0.2     # novelty weight\n",
    "THETA = 0.2   # redundancy penalty\n",
    "\n",
    "# === Compute final AdjustedScore per sentence ===\n",
    "adjusted_scores = []\n",
    "\n",
    "print(\"Computing Adjusted Scores (Stage 5)...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    doc_id = row['doc_id']\n",
    "    vi = row['enhanced_embedding']\n",
    "    vdoc = V_doc[doc_id]\n",
    "\n",
    "    # Relevance\n",
    "    rel = cosine_similarity([vi], [vdoc])[0][0]\n",
    "\n",
    "    # Context\n",
    "    local_embeds = grouped_embeddings[doc_id]\n",
    "    local_idx = list(df[df['doc_id'] == doc_id].index).index(idx)\n",
    "    ctx = compute_contextual_score(idx, vi, local_embeds, local_idx)\n",
    "\n",
    "    # Novelty & Redundancy\n",
    "    if seen_embeddings[doc_id]:\n",
    "        sim_to_seen = cosine_similarity([vi], seen_embeddings[doc_id])[0]\n",
    "        novelty = 1 - np.max(sim_to_seen)\n",
    "        redundancy = np.max(sim_to_seen)\n",
    "    else:\n",
    "        novelty = 1.0\n",
    "        redundancy = 0.0\n",
    "\n",
    "    # Final Adjusted Score\n",
    "    score = OMEGA * ctx + LAMBDA * rel + ETA * novelty - THETA * redundancy\n",
    "    adjusted_scores.append(float(score))\n",
    "\n",
    "    # Update seen_embeddings for next sentence\n",
    "    seen_embeddings[doc_id].append(vi)\n",
    "\n",
    "# Add scores to DataFrame\n",
    "df['adjusted_score'] = adjusted_scores\n",
    "\n",
    "# === Save for Stage 6 ===\n",
    "df.to_pickle(\"stage5_scored_sentences.pkl\")\n",
    "print(\" Saved scored sentences to 'stage5_scored_sentences.pkl'\")\n",
    "\n",
    "# === Sample output ===\n",
    "print(\"\\n Sample Adjusted Scores (first 5 rows):\")\n",
    "print(df[['doc_id', 'paragraph_id', 'cluster_id', 'adjusted_score']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4299a",
   "metadata": {},
   "source": [
    "Sentence selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89cfb0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample Selected Sentences (First 3 Documents):\n",
      "\n",
      "Document 0:\n",
      "  Rank 1: Sentence Index 1 | (Doc 0, Para 0, Cluster 3) | Score: 0.6277999877929688\n",
      "  Rank 2: Sentence Index 117002 | (Doc 0, Para 0, Cluster 2) | Score: 0.5634999871253967\n",
      "  Rank 3: Sentence Index 112591 | (Doc 0, Para 0, Cluster 7) | Score: 0.5403000116348267\n",
      "\n",
      "Document 1:\n",
      "  Rank 1: Sentence Index 39 | (Doc 1, Para 0, Cluster 1) | Score: 0.6437000036239624\n",
      "  Rank 2: Sentence Index 40 | (Doc 1, Para 0, Cluster 8) | Score: 0.5774999856948853\n",
      "  Rank 3: Sentence Index 41 | (Doc 1, Para 0, Cluster 7) | Score: 0.552299976348877\n",
      "\n",
      "Document 2:\n",
      "  Rank 1: Sentence Index 76 | (Doc 2, Para 0, Cluster 7) | Score: 0.7297000288963318\n",
      "  Rank 2: Sentence Index 112659 | (Doc 2, Para 0, Cluster 2) | Score: 0.6118000149726868\n",
      "  Rank 3: Sentence Index 98 | (Doc 2, Para 0, Cluster 1) | Score: 0.5891000032424927\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# === Load input ===\n",
    "df = pd.read_pickle(\"stage5_scored_sentences.pkl\")  # Ensure this has: doc_id, paragraph_id, cluster_id, adjusted_score, enhanced_embedding\n",
    "\n",
    "# === Convert embedding list to numpy array ===\n",
    "embedding_array = np.vstack(df['enhanced_embedding'].values)\n",
    "\n",
    "# === Group sentence indices by document and cluster ===\n",
    "doc_clusters = defaultdict(lambda: defaultdict(list))\n",
    "for idx, row in df.iterrows():\n",
    "    doc_clusters[row['doc_id']][row['cluster_id']].append(idx)\n",
    "\n",
    "# === Compute cluster centroids ===\n",
    "cluster_centroids = {}\n",
    "for doc_id, clusters in doc_clusters.items():\n",
    "    for cid, indices in clusters.items():\n",
    "        cluster_embeddings = embedding_array[indices]\n",
    "        centroid = np.mean(cluster_embeddings, axis=0)\n",
    "        cluster_centroids[(doc_id, cid)] = centroid\n",
    "\n",
    "# === Scoring weights ===\n",
    "alpha = 0.6  # weight for adjusted relevance\n",
    "beta = 0.4   # weight for cluster centrality\n",
    "\n",
    "# === Compute selection scores ===\n",
    "selection_scores = []\n",
    "for idx, row in df.iterrows():\n",
    "    embedding = embedding_array[idx]\n",
    "    adjusted = row['adjusted_score']\n",
    "    cluster_center = cluster_centroids[(row['doc_id'], row['cluster_id'])]\n",
    "    centrality = cosine_similarity([embedding], [cluster_center])[0][0]\n",
    "    score = alpha * adjusted + beta * centrality\n",
    "    selection_scores.append(score)\n",
    "\n",
    "df['selection_score'] = selection_scores\n",
    "\n",
    "# === Select top-k sentences per document ===\n",
    "k = 3\n",
    "top_sentences_per_doc = {}\n",
    "\n",
    "for doc_id, group in df.groupby('doc_id'):\n",
    "    top_k = group.sort_values(by='selection_score', ascending=False).head(k)\n",
    "    top_sentences_per_doc[doc_id] = list(top_k.index)\n",
    "\n",
    "# === Save outputs ===\n",
    "torch.save(selection_scores, \"stage6_selection_scores.pt\")\n",
    "torch.save(top_sentences_per_doc, \"stage6_top_sentences.pt\")\n",
    "df.to_pickle(\"stage6_sentences_with_selection_scores.pkl\")\n",
    "\n",
    "# === Print preview ===\n",
    "print(\"\\n Sample Selected Sentences (First 3 Documents):\")\n",
    "for doc_id in range(3):\n",
    "    print(f\"\\nDocument {doc_id}:\")\n",
    "    for rank, idx in enumerate(top_sentences_per_doc.get(doc_id, []), start=1):\n",
    "        row = df.loc[idx]\n",
    "        print(f\"  Rank {rank}: Sentence Index {idx} | (Doc {row['doc_id']}, Para {row['paragraph_id']}, Cluster {row['cluster_id']}) | Score: {round(row['selection_score'], 4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c962733",
   "metadata": {},
   "source": [
    "sample sentences selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d709cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample Selected Sentences:\n",
      "\n",
      "Document 0:\n",
      "  Rank 1: Britons flocked to beaches across the southern coast yesterday as millions look set to bask in glorious sunshine today.\n",
      "  Rank 2: How marvellous is that?\n",
      "  Rank 3: 'It was unbelievable,' he recalled.\n",
      "\n",
      "Document 1:\n",
      "  Rank 1: A couple who weighed a combined 32st were shamed into slimming by their own family - during Christmas dinner.\n",
      "  Rank 2: Margaret Gibson, 37, and her husband, James, 41, from Biddulph, Staffs, started piling on the pounds after the birth of their two children just over a decade ago.\n",
      "  Rank 3: But after taunts during the festive feast - and a warning from James's doctor that he couldn't undergo a procedure because he would 'die on the operating table' - the pair took action and have lost more than 7st between them.\n",
      "\n",
      "Document 2:\n",
      "  Rank 1: Video footage shows the heart stopping moment a 17 year old boy was bitten on the hand by a shark.\n",
      "  Rank 2: Everton boss Roberto Martinez feels the poor condition of the pitch at Dynamo Kiev's Olympic Stadium is 'a concern' - but he is sure it will be just as problematic for one side as the other in Thursday's Europa League last-16 second-leg encounter.\n",
      "  Rank 3: But the warning did not deter many people who proceeded to continue swimming in the sea despite being told by lifeguards .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load top sentence indices\n",
    "top_sentences_per_doc = torch.load(\"stage6_top_sentences.pt\")\n",
    "\n",
    "# Load the sentence dataset (ensure this matches with the indices)\n",
    "sentence_df = pd.read_pickle(\"sentence_level_dataset_with_clusters.pkl\")\n",
    "sentences = sentence_df[\"sentence\"].tolist()\n",
    "\n",
    "# Print sentences for a few documents\n",
    "print(\"\\n Sample Selected Sentences:\")\n",
    "\n",
    "for doc_id in range(3):  # Adjust range as needed\n",
    "    print(f\"\\nDocument {doc_id}:\")\n",
    "    for rank, idx in enumerate(top_sentences_per_doc.get(doc_id, []), start=1):\n",
    "        print(f\"  Rank {rank}: {sentences[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eaefc0",
   "metadata": {},
   "source": [
    "Post Processing and Refinement\n",
    "#redundacy Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58699978",
   "metadata": {},
   "source": [
    "final summary generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae18f29",
   "metadata": {},
   "source": [
    "post processing and refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaffac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load sentence-level data ===\n",
    "sentence_df = pd.read_pickle(\"sentence_level_dataset_with_clusters.pkl\")\n",
    "embedding_df = pd.read_pickle(\"sentence_level_dataset_with_contextual_embeddings.pkl\")\n",
    "\n",
    "sentences = sentence_df[\"sentence\"].tolist()\n",
    "\n",
    "# Extract embeddings\n",
    "if \"embedding\" in embedding_df.columns:\n",
    "    embeddings = np.vstack(embedding_df[\"embedding\"].values)\n",
    "else:\n",
    "    raise ValueError(\"Embedding column not found!\")\n",
    "\n",
    "# Load top sentences per document\n",
    "top_sentences_per_doc = torch.load(\"stage6_top_sentences.pt\")\n",
    "\n",
    "# === Redundancy Filtering ===\n",
    "tau = 0.85\n",
    "filtered_summaries = {}\n",
    "\n",
    "for doc_id, sent_indices in top_sentences_per_doc.items():\n",
    "    filtered = []\n",
    "\n",
    "    for i in sent_indices:\n",
    "        vi = embeddings[i]\n",
    "        is_redundant = False\n",
    "\n",
    "        for j in filtered:\n",
    "            vj = embeddings[j]\n",
    "            sim = cosine_similarity([vi], [vj])[0][0]\n",
    "            if sim > tau:\n",
    "                is_redundant = True\n",
    "                break\n",
    "\n",
    "        if not is_redundant:\n",
    "            filtered.append(i)\n",
    "\n",
    "    filtered_summaries[doc_id] = filtered\n",
    "\n",
    "# === Post-Processing ===\n",
    "gamma = 0.6\n",
    "delta = 0.4\n",
    "cohesion_threshold = 0.25\n",
    "\n",
    "final_summaries = {}\n",
    "\n",
    "def compute_order_score(i, j):\n",
    "    pos_adj = 1.0 / (1 + abs(i - j))\n",
    "    thematic_sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "    return gamma * pos_adj + delta * thematic_sim\n",
    "\n",
    "def lm_score(embedding1, embedding2):\n",
    "    emb1 = torch.tensor(embedding1, dtype=torch.float32).unsqueeze(0)\n",
    "    emb2 = torch.tensor(embedding2, dtype=torch.float32).unsqueeze(0)\n",
    "    return F.cosine_similarity(emb1, emb2, dim=-1).item()\n",
    "\n",
    "for doc_id, sentence_indices in tqdm(filtered_summaries.items(), desc=\"Post-processing\"):\n",
    "    if not sentence_indices:\n",
    "        final_summaries[doc_id] = \"\"\n",
    "        continue\n",
    "\n",
    "    # Logical ordering\n",
    "    ordered = [sentence_indices[0]]\n",
    "    remaining = sentence_indices[1:]\n",
    "\n",
    "    while remaining:\n",
    "        last_idx = ordered[-1]\n",
    "        next_idx = max(remaining, key=lambda j: compute_order_score(last_idx, j))\n",
    "        ordered.append(next_idx)\n",
    "        remaining.remove(next_idx)\n",
    "\n",
    "    # Cohesion filtering\n",
    "    refined = [sentences[ordered[0]]]\n",
    "    prev_embedding = embeddings[ordered[0]]\n",
    "\n",
    "    for idx in ordered[1:]:\n",
    "        curr_embedding = embeddings[idx]\n",
    "        score = lm_score(prev_embedding, curr_embedding)\n",
    "        if score >= cohesion_threshold:\n",
    "            refined.append(sentences[idx])\n",
    "            prev_embedding = curr_embedding\n",
    "\n",
    "    # Style cleanup\n",
    "    cleaned = []\n",
    "    for s in refined:\n",
    "        if isinstance(s, str):\n",
    "            text = s.strip()\n",
    "            if len(text.split()) > 2 and text.lower() != \"the .\":\n",
    "                cleaned.append(text)\n",
    "\n",
    "    final_summaries[doc_id] = \" \".join(cleaned) if cleaned else \"\"\n",
    "\n",
    "# === Save output ===\n",
    "torch.save(filtered_summaries, \"stage7_filtered_sentences.pt\")\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"doc_id\": doc_id, \"refined_summary\": summary}\n",
    "    for doc_id, summary in final_summaries.items()\n",
    "])\n",
    "summary_df.to_csv(\"deep_extract_stage7_final_summaries.csv\", index=False)\n",
    "\n",
    "print(\"\\n Redundancy filtering + post-processing complete!\")\n",
    "print(\" Final summaries saved to 'deep_extract_stage7_final_summaries.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84932acb",
   "metadata": {},
   "source": [
    "reference extractive summaries creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64163ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Ensure nltk is installed and download required data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the model once\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Load your dataset\n",
    "# train_sample = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Prepare to store extractive summaries\n",
    "extractive_summaries = []\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for doc_id, row in tqdm(train_sample.iterrows(), total=len(train_sample), desc=\"Generating Extractive Summaries\"):\n",
    "    article_text = row['article']\n",
    "    highlight = row['highlights']\n",
    "\n",
    "    # Split article into sentences (basic sentence segmentation)\n",
    "    article_sentences = [s.strip() for s in article_text.split('. ') if s.strip()]\n",
    "    \n",
    "    # Skip if article or highlight is empty\n",
    "    if not article_sentences or not highlight.strip():\n",
    "        continue\n",
    "\n",
    "    # Compute embeddings\n",
    "    sent_embeds = model.encode(article_sentences, convert_to_numpy=True)\n",
    "    highlight_embed = model.encode([highlight], convert_to_numpy=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sims = cosine_similarity(highlight_embed, sent_embeds)[0]\n",
    "\n",
    "    # Get top 3 most similar sentences\n",
    "    top_indices = np.argsort(sims)[-3:][::-1]\n",
    "    selected_sents = [article_sentences[i] for i in top_indices]\n",
    "\n",
    "    # Combine selected sentences into extractive summary\n",
    "    extractive_summary = ' '.join(selected_sents)\n",
    "\n",
    "    # Store results\n",
    "    extractive_summaries.append({\n",
    "        'doc_id': doc_id,\n",
    "        'extractive_summary': extractive_summary,\n",
    "        'original_highlight': highlight\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "extractive_df = pd.DataFrame(extractive_summaries)\n",
    "#extractive_df.to_csv(\"pseudo_extractive_summaries.csv\", index=False)\n",
    "\n",
    "print(\" Extractive summaries saved to pseudo_extractive_summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5c3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractive_df.to_csv(\"new_extractive_summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dad41e",
   "metadata": {},
   "source": [
    "Rouge-Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac67a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ROUGE-1 F1: 0.4612\n",
      " ROUGE-2 F1: 0.3392\n",
      " ROUGE-L F1: 0.3671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Load both generated and reference summaries\n",
    "gen = pd.read_csv(\"deep_extract_stage7_final_summaries.csv\")  # Contains 'refined_summary'\n",
    "ref = pd.read_csv(\"new_extractive_summaries.csv\")             # Contains 'extractive_summary'\n",
    "\n",
    "# Merge using proper IDs (ensure they match by content, typically doc_id or id)\n",
    "#merged = pd.merge(gen, ref, left_on=\"doc_id\", right_on=\"id\")\n",
    "merged = pd.merge(gen, ref, on=\"doc_id\")\n",
    "\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Ensure there are no NaNs and all values are strings\n",
    "merged['extractive_summary'] = merged['extractive_summary'].fillna(\"\").astype(str)\n",
    "merged['refined_summary'] = merged['refined_summary'].fillna(\"\").astype(str)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "r1, r2, rl = [], [], []\n",
    "for _, row in merged.iterrows():\n",
    "    scores = scorer.score(row['extractive_summary'], row['refined_summary'])\n",
    "    r1.append(scores['rouge1'].fmeasure)\n",
    "    r2.append(scores['rouge2'].fmeasure)\n",
    "    rl.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Print average ROUGE scores\n",
    "print(f\" ROUGE-1 F1: {sum(r1)/len(r1):.4f}\")\n",
    "print(f\" ROUGE-2 F1: {sum(r2)/len(r2):.4f}\")\n",
    "print(f\" ROUGE-L F1: {sum(rl)/len(rl):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f0f68",
   "metadata": {},
   "source": [
    "Baseline summary comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5deb68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation for 2871 rows...\n",
      "Row 1/2871 evaluated.\n",
      "Row 2/2871 evaluated.\n",
      "Row 3/2871 evaluated.\n",
      "Row 4/2871 evaluated.\n",
      "Row 5/2871 evaluated.\n",
      "Row 6/2871 evaluated.\n",
      "Row 7/2871 evaluated.\n",
      "Row 8/2871 evaluated.\n",
      "Row 9/2871 evaluated.\n",
      "Row 10/2871 evaluated.\n",
      "Row 11/2871 evaluated.\n",
      "Row 12/2871 evaluated.\n",
      "Row 13/2871 evaluated.\n",
      "Row 14/2871 evaluated.\n",
      "Row 15/2871 evaluated.\n",
      "Row 16/2871 evaluated.\n",
      "Row 17/2871 evaluated.\n",
      "Row 18/2871 evaluated.\n",
      "Row 19/2871 evaluated.\n",
      "Row 20/2871 evaluated.\n",
      "Row 21/2871 evaluated.\n",
      "Row 22/2871 evaluated.\n",
      "Row 23/2871 evaluated.\n",
      "Row 24/2871 evaluated.\n",
      "Row 25/2871 evaluated.\n",
      "Row 26/2871 evaluated.\n",
      "Row 27/2871 evaluated.\n",
      "Row 28/2871 evaluated.\n",
      "Row 29/2871 evaluated.\n",
      "Row 30/2871 evaluated.\n",
      "Row 31/2871 evaluated.\n",
      "Row 32/2871 evaluated.\n",
      "Row 33/2871 evaluated.\n",
      "Row 34/2871 evaluated.\n",
      "Row 35/2871 evaluated.\n",
      "Row 36/2871 evaluated.\n",
      "Row 37/2871 evaluated.\n",
      "Row 38/2871 evaluated.\n",
      "Row 39/2871 evaluated.\n",
      "Row 40/2871 evaluated.\n",
      "Row 41/2871 evaluated.\n",
      "Row 42/2871 evaluated.\n",
      "Row 43/2871 evaluated.\n",
      "Row 44/2871 evaluated.\n",
      "Row 45/2871 evaluated.\n",
      "Row 46/2871 evaluated.\n",
      "Row 47/2871 evaluated.\n",
      "Row 48/2871 evaluated.\n",
      "Row 49/2871 evaluated.\n",
      "Row 50/2871 evaluated.\n",
      "Row 51/2871 evaluated.\n",
      "Row 52/2871 evaluated.\n",
      "Row 53/2871 evaluated.\n",
      "Row 54/2871 evaluated.\n",
      "Row 55/2871 evaluated.\n",
      "Row 56/2871 evaluated.\n",
      "Row 57/2871 evaluated.\n",
      "Row 58/2871 evaluated.\n",
      "Row 59/2871 evaluated.\n",
      "Row 60/2871 evaluated.\n",
      "Row 61/2871 evaluated.\n",
      "Row 62/2871 evaluated.\n",
      "Row 63/2871 evaluated.\n",
      "Row 64/2871 evaluated.\n",
      "Row 65/2871 evaluated.\n",
      "Row 66/2871 evaluated.\n",
      "Row 67/2871 evaluated.\n",
      "Row 68/2871 evaluated.\n",
      "Row 69/2871 evaluated.\n",
      "Row 70/2871 evaluated.\n",
      "Row 71/2871 evaluated.\n",
      "Row 72/2871 evaluated.\n",
      "Row 73/2871 evaluated.\n",
      "Row 74/2871 evaluated.\n",
      "Row 75/2871 evaluated.\n",
      "Row 76/2871 evaluated.\n",
      "Row 77/2871 evaluated.\n",
      "Row 78/2871 evaluated.\n",
      "Row 79/2871 evaluated.\n",
      "Row 80/2871 evaluated.\n",
      "Row 81/2871 evaluated.\n",
      "Row 82/2871 evaluated.\n",
      "Row 83/2871 evaluated.\n",
      "Row 84/2871 evaluated.\n",
      "Row 85/2871 evaluated.\n",
      "Row 86/2871 evaluated.\n",
      "Row 87/2871 evaluated.\n",
      "Row 88/2871 evaluated.\n",
      "Row 89/2871 evaluated.\n",
      "Row 90/2871 evaluated.\n",
      "Row 91/2871 evaluated.\n",
      "Row 92/2871 evaluated.\n",
      "Row 93/2871 evaluated.\n",
      "Row 94/2871 evaluated.\n",
      "Row 95/2871 evaluated.\n",
      "Row 96/2871 evaluated.\n",
      "Row 97/2871 evaluated.\n",
      "Row 98/2871 evaluated.\n",
      "Row 99/2871 evaluated.\n",
      "Row 100/2871 evaluated.\n",
      "Row 101/2871 evaluated.\n",
      "Row 102/2871 evaluated.\n",
      "Row 103/2871 evaluated.\n",
      "Row 104/2871 evaluated.\n",
      "Row 105/2871 evaluated.\n",
      "Row 106/2871 evaluated.\n",
      "Row 107/2871 evaluated.\n",
      "Row 108/2871 evaluated.\n",
      "Row 109/2871 evaluated.\n",
      "Row 110/2871 evaluated.\n",
      "Row 111/2871 evaluated.\n",
      "Row 112/2871 evaluated.\n",
      "Row 113/2871 evaluated.\n",
      "Row 114/2871 evaluated.\n",
      "Row 115/2871 evaluated.\n",
      "Row 116/2871 evaluated.\n",
      "Row 117/2871 evaluated.\n",
      "Row 118/2871 evaluated.\n",
      "Row 119/2871 evaluated.\n",
      "Row 120/2871 evaluated.\n",
      "Row 121/2871 evaluated.\n",
      "Row 122/2871 evaluated.\n",
      "Row 123/2871 evaluated.\n",
      "Row 124/2871 evaluated.\n",
      "Row 125/2871 evaluated.\n",
      "Row 126/2871 evaluated.\n",
      "Row 127/2871 evaluated.\n",
      "Row 128/2871 evaluated.\n",
      "Row 129/2871 evaluated.\n",
      "Row 130/2871 evaluated.\n",
      "Row 131/2871 evaluated.\n",
      "Row 132/2871 evaluated.\n",
      "Row 133/2871 evaluated.\n",
      "Row 134/2871 evaluated.\n",
      "Row 135/2871 evaluated.\n",
      "Row 136/2871 evaluated.\n",
      "Row 137/2871 evaluated.\n",
      "Row 138/2871 evaluated.\n",
      "Row 139/2871 evaluated.\n",
      "Row 140/2871 evaluated.\n",
      "Row 141/2871 evaluated.\n",
      "Row 142/2871 evaluated.\n",
      "Row 143/2871 evaluated.\n",
      "Row 144/2871 evaluated.\n",
      "Row 145/2871 evaluated.\n",
      "Row 146/2871 evaluated.\n",
      "Row 147/2871 evaluated.\n",
      "Row 148/2871 evaluated.\n",
      "Row 149/2871 evaluated.\n",
      "Row 150/2871 evaluated.\n",
      "Row 151/2871 evaluated.\n",
      "Row 152/2871 evaluated.\n",
      "Row 153/2871 evaluated.\n",
      "Row 154/2871 evaluated.\n",
      "Row 155/2871 evaluated.\n",
      "Row 156/2871 evaluated.\n",
      "Row 157/2871 evaluated.\n",
      "Row 158/2871 evaluated.\n",
      "Row 159/2871 evaluated.\n",
      "Row 160/2871 evaluated.\n",
      "Row 161/2871 evaluated.\n",
      "Row 162/2871 evaluated.\n",
      "Row 163/2871 evaluated.\n",
      "Row 164/2871 evaluated.\n",
      "Row 165/2871 evaluated.\n",
      "Row 166/2871 evaluated.\n",
      "Row 167/2871 evaluated.\n",
      "Row 168/2871 evaluated.\n",
      "Row 169/2871 evaluated.\n",
      "Row 170/2871 evaluated.\n",
      "Row 171/2871 evaluated.\n",
      "Row 172/2871 evaluated.\n",
      "Row 173/2871 evaluated.\n",
      "Row 174/2871 evaluated.\n",
      "Row 175/2871 evaluated.\n",
      "Row 176/2871 evaluated.\n",
      "Row 177/2871 evaluated.\n",
      "Row 178/2871 evaluated.\n",
      "Row 179/2871 evaluated.\n",
      "Row 180/2871 evaluated.\n",
      "Row 181/2871 evaluated.\n",
      "Row 182/2871 evaluated.\n",
      "Row 183/2871 evaluated.\n",
      "Row 184/2871 evaluated.\n",
      "Row 185/2871 evaluated.\n",
      "Row 186/2871 evaluated.\n",
      "Row 187/2871 evaluated.\n",
      "Row 188/2871 evaluated.\n",
      "Row 189/2871 evaluated.\n",
      "Row 190/2871 evaluated.\n",
      "Row 191/2871 evaluated.\n",
      "Row 192/2871 evaluated.\n",
      "Row 193/2871 evaluated.\n",
      "Row 194/2871 evaluated.\n",
      "Row 195/2871 evaluated.\n",
      "Row 196/2871 evaluated.\n",
      "Row 197/2871 evaluated.\n",
      "Row 198/2871 evaluated.\n",
      "Row 199/2871 evaluated.\n",
      "Row 200/2871 evaluated.\n",
      "Row 201/2871 evaluated.\n",
      "Row 202/2871 evaluated.\n",
      "Row 203/2871 evaluated.\n",
      "Row 204/2871 evaluated.\n",
      "Row 205/2871 evaluated.\n",
      "Row 206/2871 evaluated.\n",
      "Row 207/2871 evaluated.\n",
      "Row 208/2871 evaluated.\n",
      "Row 209/2871 evaluated.\n",
      "Row 210/2871 evaluated.\n",
      "Row 211/2871 evaluated.\n",
      "Row 212/2871 evaluated.\n",
      "Row 213/2871 evaluated.\n",
      "Row 214/2871 evaluated.\n",
      "Row 215/2871 evaluated.\n",
      "Row 216/2871 evaluated.\n",
      "Row 217/2871 evaluated.\n",
      "Row 218/2871 evaluated.\n",
      "Row 219/2871 evaluated.\n",
      "Row 220/2871 evaluated.\n",
      "Row 221/2871 evaluated.\n",
      "Row 222/2871 evaluated.\n",
      "Row 223/2871 evaluated.\n",
      "Row 224/2871 evaluated.\n",
      "Row 225/2871 evaluated.\n",
      "Row 226/2871 evaluated.\n",
      "Row 227/2871 evaluated.\n",
      "Row 228/2871 evaluated.\n",
      "Row 229/2871 evaluated.\n",
      "Row 230/2871 evaluated.\n",
      "Row 231/2871 evaluated.\n",
      "Row 232/2871 evaluated.\n",
      "Row 233/2871 evaluated.\n",
      "Row 234/2871 evaluated.\n",
      "Row 235/2871 evaluated.\n",
      "Row 236/2871 evaluated.\n",
      "Row 237/2871 evaluated.\n",
      "Row 238/2871 evaluated.\n",
      "Row 239/2871 evaluated.\n",
      "Row 240/2871 evaluated.\n",
      "Row 241/2871 evaluated.\n",
      "Row 242/2871 evaluated.\n",
      "Row 243/2871 evaluated.\n",
      "Row 244/2871 evaluated.\n",
      "Row 245/2871 evaluated.\n",
      "Row 246/2871 evaluated.\n",
      "Row 247/2871 evaluated.\n",
      "Row 248/2871 evaluated.\n",
      "Row 249/2871 evaluated.\n",
      "Row 250/2871 evaluated.\n",
      "Row 251/2871 evaluated.\n",
      "Row 252/2871 evaluated.\n",
      "Row 253/2871 evaluated.\n",
      "Row 254/2871 evaluated.\n",
      "Row 255/2871 evaluated.\n",
      "Row 256/2871 evaluated.\n",
      "Row 257/2871 evaluated.\n",
      "Row 258/2871 evaluated.\n",
      "Row 259/2871 evaluated.\n",
      "Row 260/2871 evaluated.\n",
      "Row 261/2871 evaluated.\n",
      "Row 262/2871 evaluated.\n",
      "Row 263/2871 evaluated.\n",
      "Row 264/2871 evaluated.\n",
      "Row 265/2871 evaluated.\n",
      "Row 266/2871 evaluated.\n",
      "Row 267/2871 evaluated.\n",
      "Row 268/2871 evaluated.\n",
      "Row 269/2871 evaluated.\n",
      "Row 270/2871 evaluated.\n",
      "Row 271/2871 evaluated.\n",
      "Row 272/2871 evaluated.\n",
      "Row 273/2871 evaluated.\n",
      "Row 274/2871 evaluated.\n",
      "Row 275/2871 evaluated.\n",
      "Row 276/2871 evaluated.\n",
      "Row 277/2871 evaluated.\n",
      "Row 278/2871 evaluated.\n",
      "Row 279/2871 evaluated.\n",
      "Row 280/2871 evaluated.\n",
      "Row 281/2871 evaluated.\n",
      "Row 282/2871 evaluated.\n",
      "Row 283/2871 evaluated.\n",
      "Row 284/2871 evaluated.\n",
      "Row 285/2871 evaluated.\n",
      "Row 286/2871 evaluated.\n",
      "Row 287/2871 evaluated.\n",
      "Row 288/2871 evaluated.\n",
      "Row 289/2871 evaluated.\n",
      "Row 290/2871 evaluated.\n",
      "Row 291/2871 evaluated.\n",
      "Row 292/2871 evaluated.\n",
      "Row 293/2871 evaluated.\n",
      "Row 294/2871 evaluated.\n",
      "Row 295/2871 evaluated.\n",
      "Row 296/2871 evaluated.\n",
      "Row 297/2871 evaluated.\n",
      "Row 298/2871 evaluated.\n",
      "Row 299/2871 evaluated.\n",
      "Row 300/2871 evaluated.\n",
      "Row 301/2871 evaluated.\n",
      "Row 302/2871 evaluated.\n",
      "Row 303/2871 evaluated.\n",
      "Row 304/2871 evaluated.\n",
      "Row 305/2871 evaluated.\n",
      "Row 306/2871 evaluated.\n",
      "Row 307/2871 evaluated.\n",
      "Row 308/2871 evaluated.\n",
      "Row 309/2871 evaluated.\n",
      "Row 310/2871 evaluated.\n",
      "Row 311/2871 evaluated.\n",
      "Row 312/2871 evaluated.\n",
      "Row 313/2871 evaluated.\n",
      "Row 314/2871 evaluated.\n",
      "Row 315/2871 evaluated.\n",
      "Row 316/2871 evaluated.\n",
      "Row 317/2871 evaluated.\n",
      "Row 318/2871 evaluated.\n",
      "Row 319/2871 evaluated.\n",
      "Row 320/2871 evaluated.\n",
      "Row 321/2871 evaluated.\n",
      "Row 322/2871 evaluated.\n",
      "Row 323/2871 evaluated.\n",
      "Row 324/2871 evaluated.\n",
      "Row 325/2871 evaluated.\n",
      "Row 326/2871 evaluated.\n",
      "Row 327/2871 evaluated.\n",
      "Row 328/2871 evaluated.\n",
      "Row 329/2871 evaluated.\n",
      "Row 330/2871 evaluated.\n",
      "Row 331/2871 evaluated.\n",
      "Row 332/2871 evaluated.\n",
      "Row 333/2871 evaluated.\n",
      "Row 334/2871 evaluated.\n",
      "Row 335/2871 evaluated.\n",
      "Row 336/2871 evaluated.\n",
      "Row 337/2871 evaluated.\n",
      "Row 338/2871 evaluated.\n",
      "Row 339/2871 evaluated.\n",
      "Row 340/2871 evaluated.\n",
      "Row 341/2871 evaluated.\n",
      "Row 342/2871 evaluated.\n",
      "Row 343/2871 evaluated.\n",
      "Row 344/2871 evaluated.\n",
      "Row 345/2871 evaluated.\n",
      "Row 346/2871 evaluated.\n",
      "Row 347/2871 evaluated.\n",
      "Row 348/2871 evaluated.\n",
      "Row 349/2871 evaluated.\n",
      "Row 350/2871 evaluated.\n",
      "Row 351/2871 evaluated.\n",
      "Row 352/2871 evaluated.\n",
      "Row 353/2871 evaluated.\n",
      "Row 354/2871 evaluated.\n",
      "Row 355/2871 evaluated.\n",
      "Row 356/2871 evaluated.\n",
      "Row 357/2871 evaluated.\n",
      "Row 358/2871 evaluated.\n",
      "Row 359/2871 evaluated.\n",
      "Row 360/2871 evaluated.\n",
      "Row 361/2871 evaluated.\n",
      "Row 362/2871 evaluated.\n",
      "Row 363/2871 evaluated.\n",
      "Row 364/2871 evaluated.\n",
      "Row 365/2871 evaluated.\n",
      "Row 366/2871 evaluated.\n",
      "Row 367/2871 evaluated.\n",
      "Row 368/2871 evaluated.\n",
      "Row 369/2871 evaluated.\n",
      "Row 370/2871 evaluated.\n",
      "Row 371/2871 evaluated.\n",
      "Row 372/2871 evaluated.\n",
      "Row 373/2871 evaluated.\n",
      "Row 374/2871 evaluated.\n",
      "Row 375/2871 evaluated.\n",
      "Row 376/2871 evaluated.\n",
      "Row 377/2871 evaluated.\n",
      "Row 378/2871 evaluated.\n",
      "Row 379/2871 evaluated.\n",
      "Row 380/2871 evaluated.\n",
      "Row 381/2871 evaluated.\n",
      "Row 382/2871 evaluated.\n",
      "Row 383/2871 evaluated.\n",
      "Row 384/2871 evaluated.\n",
      "Row 385/2871 evaluated.\n",
      "Row 386/2871 evaluated.\n",
      "Row 387/2871 evaluated.\n",
      "Row 388/2871 evaluated.\n",
      "Row 389/2871 evaluated.\n",
      "Row 390/2871 evaluated.\n",
      "Row 391/2871 evaluated.\n",
      "Row 392/2871 evaluated.\n",
      "Row 393/2871 evaluated.\n",
      "Row 394/2871 evaluated.\n",
      "Row 395/2871 evaluated.\n",
      "Row 396/2871 evaluated.\n",
      "Row 397/2871 evaluated.\n",
      "Row 398/2871 evaluated.\n",
      "Row 399/2871 evaluated.\n",
      "Row 400/2871 evaluated.\n",
      "Row 401/2871 evaluated.\n",
      "Row 402/2871 evaluated.\n",
      "Row 403/2871 evaluated.\n",
      "Row 404/2871 evaluated.\n",
      "Row 405/2871 evaluated.\n",
      "Row 406/2871 evaluated.\n",
      "Row 407/2871 evaluated.\n",
      "Row 408/2871 evaluated.\n",
      "Row 409/2871 evaluated.\n",
      "Row 410/2871 evaluated.\n",
      "Row 411/2871 evaluated.\n",
      "Row 412/2871 evaluated.\n",
      "Row 413/2871 evaluated.\n",
      "Row 414/2871 evaluated.\n",
      "Row 415/2871 evaluated.\n",
      "Row 416/2871 evaluated.\n",
      "Row 417/2871 evaluated.\n",
      "Row 418/2871 evaluated.\n",
      "Row 419/2871 evaluated.\n",
      "Row 420/2871 evaluated.\n",
      "Row 421/2871 evaluated.\n",
      "Row 422/2871 evaluated.\n",
      "Row 423/2871 evaluated.\n",
      "Row 424/2871 evaluated.\n",
      "Row 425/2871 evaluated.\n",
      "Row 426/2871 evaluated.\n",
      "Row 427/2871 evaluated.\n",
      "Row 428/2871 evaluated.\n",
      "Row 429/2871 evaluated.\n",
      "Row 430/2871 evaluated.\n",
      "Row 431/2871 evaluated.\n",
      "Row 432/2871 evaluated.\n",
      "Row 433/2871 evaluated.\n",
      "Row 434/2871 evaluated.\n",
      "Row 435/2871 evaluated.\n",
      "Row 436/2871 evaluated.\n",
      "Row 437/2871 evaluated.\n",
      "Row 438/2871 evaluated.\n",
      "Row 439/2871 evaluated.\n",
      "Row 440/2871 evaluated.\n",
      "Row 441/2871 evaluated.\n",
      "Row 442/2871 evaluated.\n",
      "Row 443/2871 evaluated.\n",
      "Row 444/2871 evaluated.\n",
      "Row 445/2871 evaluated.\n",
      "Row 446/2871 evaluated.\n",
      "Row 447/2871 evaluated.\n",
      "Row 448/2871 evaluated.\n",
      "Row 449/2871 evaluated.\n",
      "Row 450/2871 evaluated.\n",
      "Row 451/2871 evaluated.\n",
      "Row 452/2871 evaluated.\n",
      "Row 453/2871 evaluated.\n",
      "Row 454/2871 evaluated.\n",
      "Row 455/2871 evaluated.\n",
      "Row 456/2871 evaluated.\n",
      "Row 457/2871 evaluated.\n",
      "Row 458/2871 evaluated.\n",
      "Row 459/2871 evaluated.\n",
      "Row 460/2871 evaluated.\n",
      "Row 461/2871 evaluated.\n",
      "Row 462/2871 evaluated.\n",
      "Row 463/2871 evaluated.\n",
      "Row 464/2871 evaluated.\n",
      "Row 465/2871 evaluated.\n",
      "Row 466/2871 evaluated.\n",
      "Row 467/2871 evaluated.\n",
      "Row 468/2871 evaluated.\n",
      "Row 469/2871 evaluated.\n",
      "Row 470/2871 evaluated.\n",
      "Row 471/2871 evaluated.\n",
      "Row 472/2871 evaluated.\n",
      "Row 473/2871 evaluated.\n",
      "Row 474/2871 evaluated.\n",
      "Row 475/2871 evaluated.\n",
      "Row 476/2871 evaluated.\n",
      "Row 477/2871 evaluated.\n",
      "Row 478/2871 evaluated.\n",
      "Row 479/2871 evaluated.\n",
      "Row 480/2871 evaluated.\n",
      "Row 481/2871 evaluated.\n",
      "Row 482/2871 evaluated.\n",
      "Row 483/2871 evaluated.\n",
      "Row 484/2871 evaluated.\n",
      "Row 485/2871 evaluated.\n",
      "Row 486/2871 evaluated.\n",
      "Row 487/2871 evaluated.\n",
      "Row 488/2871 evaluated.\n",
      "Row 489/2871 evaluated.\n",
      "Row 490/2871 evaluated.\n",
      "Row 491/2871 evaluated.\n",
      "Row 492/2871 evaluated.\n",
      "Row 493/2871 evaluated.\n",
      "Row 494/2871 evaluated.\n",
      "Row 495/2871 evaluated.\n",
      "Row 496/2871 evaluated.\n",
      "Row 497/2871 evaluated.\n",
      "Row 498/2871 evaluated.\n",
      "Row 499/2871 evaluated.\n",
      "Row 500/2871 evaluated.\n",
      "Row 501/2871 evaluated.\n",
      "Row 502/2871 evaluated.\n",
      "Row 503/2871 evaluated.\n",
      "Row 504/2871 evaluated.\n",
      "Row 505/2871 evaluated.\n",
      "Row 506/2871 evaluated.\n",
      "Row 507/2871 evaluated.\n",
      "Row 508/2871 evaluated.\n",
      "Row 509/2871 evaluated.\n",
      "Row 510/2871 evaluated.\n",
      "Row 511/2871 evaluated.\n",
      "Row 512/2871 evaluated.\n",
      "Row 513/2871 evaluated.\n",
      "Row 514/2871 evaluated.\n",
      "Row 515/2871 evaluated.\n",
      "Row 516/2871 evaluated.\n",
      "Row 517/2871 evaluated.\n",
      "Row 518/2871 evaluated.\n",
      "Row 519/2871 evaluated.\n",
      "Row 520/2871 evaluated.\n",
      "Row 521/2871 evaluated.\n",
      "Row 522/2871 evaluated.\n",
      "Row 523/2871 evaluated.\n",
      "Row 524/2871 evaluated.\n",
      "Row 525/2871 evaluated.\n",
      "Row 526/2871 evaluated.\n",
      "Row 527/2871 evaluated.\n",
      "Row 528/2871 evaluated.\n",
      "Row 529/2871 evaluated.\n",
      "Row 530/2871 evaluated.\n",
      "Row 531/2871 evaluated.\n",
      "Row 532/2871 evaluated.\n",
      "Row 533/2871 evaluated.\n",
      "Row 534/2871 evaluated.\n",
      "Row 535/2871 evaluated.\n",
      "Row 536/2871 evaluated.\n",
      "Row 537/2871 evaluated.\n",
      "Row 538/2871 evaluated.\n",
      "Row 539/2871 evaluated.\n",
      "Row 540/2871 evaluated.\n",
      "Row 541/2871 evaluated.\n",
      "Row 542/2871 evaluated.\n",
      "Row 543/2871 evaluated.\n",
      "Row 544/2871 evaluated.\n",
      "Row 545/2871 evaluated.\n",
      "Row 546/2871 evaluated.\n",
      "Row 547/2871 evaluated.\n",
      "Row 548/2871 evaluated.\n",
      "Row 549/2871 evaluated.\n",
      "Row 550/2871 evaluated.\n",
      "Row 551/2871 evaluated.\n",
      "Row 552/2871 evaluated.\n",
      "Row 553/2871 evaluated.\n",
      "Row 554/2871 evaluated.\n",
      "Row 555/2871 evaluated.\n",
      "Row 556/2871 evaluated.\n",
      "Row 557/2871 evaluated.\n",
      "Row 558/2871 evaluated.\n",
      "Row 559/2871 evaluated.\n",
      "Row 560/2871 evaluated.\n",
      "Row 561/2871 evaluated.\n",
      "Row 562/2871 evaluated.\n",
      "Row 563/2871 evaluated.\n",
      "Row 564/2871 evaluated.\n",
      "Row 565/2871 evaluated.\n",
      "Row 566/2871 evaluated.\n",
      "Row 567/2871 evaluated.\n",
      "Row 568/2871 evaluated.\n",
      "Row 569/2871 evaluated.\n",
      "Row 570/2871 evaluated.\n",
      "Row 571/2871 evaluated.\n",
      "Row 572/2871 evaluated.\n",
      "Row 573/2871 evaluated.\n",
      "Row 574/2871 evaluated.\n",
      "Row 575/2871 evaluated.\n",
      "Row 576/2871 evaluated.\n",
      "Row 577/2871 evaluated.\n",
      "Row 578/2871 evaluated.\n",
      "Row 579/2871 evaluated.\n",
      "Row 580/2871 evaluated.\n",
      "Row 581/2871 evaluated.\n",
      "Row 582/2871 evaluated.\n",
      "Row 583/2871 evaluated.\n",
      "Row 584/2871 evaluated.\n",
      "Row 585/2871 evaluated.\n",
      "Row 586/2871 evaluated.\n",
      "Row 587/2871 evaluated.\n",
      "Row 588/2871 evaluated.\n",
      "Row 589/2871 evaluated.\n",
      "Row 590/2871 evaluated.\n",
      "Row 591/2871 evaluated.\n",
      "Row 592/2871 evaluated.\n",
      "Row 593/2871 evaluated.\n",
      "Row 594/2871 evaluated.\n",
      "Row 595/2871 evaluated.\n",
      "Row 596/2871 evaluated.\n",
      "Row 597/2871 evaluated.\n",
      "Row 598/2871 evaluated.\n",
      "Row 599/2871 evaluated.\n",
      "Row 600/2871 evaluated.\n",
      "Row 601/2871 evaluated.\n",
      "Row 602/2871 evaluated.\n",
      "Row 603/2871 evaluated.\n",
      "Row 604/2871 evaluated.\n",
      "Row 605/2871 evaluated.\n",
      "Row 606/2871 evaluated.\n",
      "Row 607/2871 evaluated.\n",
      "Row 608/2871 evaluated.\n",
      "Row 609/2871 evaluated.\n",
      "Row 610/2871 evaluated.\n",
      "Row 611/2871 evaluated.\n",
      "Row 612/2871 evaluated.\n",
      "Row 613/2871 evaluated.\n",
      "Row 614/2871 evaluated.\n",
      "Row 615/2871 evaluated.\n",
      "Row 616/2871 evaluated.\n",
      "Row 617/2871 evaluated.\n",
      "Row 618/2871 evaluated.\n",
      "Row 619/2871 evaluated.\n",
      "Row 620/2871 evaluated.\n",
      "Row 621/2871 evaluated.\n",
      "Row 622/2871 evaluated.\n",
      "Row 623/2871 evaluated.\n",
      "Row 624/2871 evaluated.\n",
      "Row 625/2871 evaluated.\n",
      "Row 626/2871 evaluated.\n",
      "Row 627/2871 evaluated.\n",
      "Row 628/2871 evaluated.\n",
      "Row 629/2871 evaluated.\n",
      "Row 630/2871 evaluated.\n",
      "Row 631/2871 evaluated.\n",
      "Row 632/2871 evaluated.\n",
      "Skipping row 633: Empty or invalid text.\n",
      "Row 634/2871 evaluated.\n",
      "Row 635/2871 evaluated.\n",
      "Row 636/2871 evaluated.\n",
      "Row 637/2871 evaluated.\n",
      "Row 638/2871 evaluated.\n",
      "Row 639/2871 evaluated.\n",
      "Row 640/2871 evaluated.\n",
      "Row 641/2871 evaluated.\n",
      "Row 642/2871 evaluated.\n",
      "Row 643/2871 evaluated.\n",
      "Row 644/2871 evaluated.\n",
      "Row 645/2871 evaluated.\n",
      "Row 646/2871 evaluated.\n",
      "Row 647/2871 evaluated.\n",
      "Row 648/2871 evaluated.\n",
      "Row 649/2871 evaluated.\n",
      "Row 650/2871 evaluated.\n",
      "Row 651/2871 evaluated.\n",
      "Row 652/2871 evaluated.\n",
      "Row 653/2871 evaluated.\n",
      "Row 654/2871 evaluated.\n",
      "Row 655/2871 evaluated.\n",
      "Row 656/2871 evaluated.\n",
      "Row 657/2871 evaluated.\n",
      "Row 658/2871 evaluated.\n",
      "Row 659/2871 evaluated.\n",
      "Row 660/2871 evaluated.\n",
      "Row 661/2871 evaluated.\n",
      "Row 662/2871 evaluated.\n",
      "Row 663/2871 evaluated.\n",
      "Row 664/2871 evaluated.\n",
      "Row 665/2871 evaluated.\n",
      "Row 666/2871 evaluated.\n",
      "Row 667/2871 evaluated.\n",
      "Row 668/2871 evaluated.\n",
      "Row 669/2871 evaluated.\n",
      "Row 670/2871 evaluated.\n",
      "Row 671/2871 evaluated.\n",
      "Row 672/2871 evaluated.\n",
      "Row 673/2871 evaluated.\n",
      "Row 674/2871 evaluated.\n",
      "Row 675/2871 evaluated.\n",
      "Row 676/2871 evaluated.\n",
      "Row 677/2871 evaluated.\n",
      "Row 678/2871 evaluated.\n",
      "Row 679/2871 evaluated.\n",
      "Row 680/2871 evaluated.\n",
      "Row 681/2871 evaluated.\n",
      "Row 682/2871 evaluated.\n",
      "Row 683/2871 evaluated.\n",
      "Row 684/2871 evaluated.\n",
      "Row 685/2871 evaluated.\n",
      "Row 686/2871 evaluated.\n",
      "Row 687/2871 evaluated.\n",
      "Row 688/2871 evaluated.\n",
      "Row 689/2871 evaluated.\n",
      "Row 690/2871 evaluated.\n",
      "Row 691/2871 evaluated.\n",
      "Row 692/2871 evaluated.\n",
      "Row 693/2871 evaluated.\n",
      "Row 694/2871 evaluated.\n",
      "Row 695/2871 evaluated.\n",
      "Row 696/2871 evaluated.\n",
      "Row 697/2871 evaluated.\n",
      "Row 698/2871 evaluated.\n",
      "Row 699/2871 evaluated.\n",
      "Row 700/2871 evaluated.\n",
      "Row 701/2871 evaluated.\n",
      "Row 702/2871 evaluated.\n",
      "Row 703/2871 evaluated.\n",
      "Row 704/2871 evaluated.\n",
      "Row 705/2871 evaluated.\n",
      "Row 706/2871 evaluated.\n",
      "Row 707/2871 evaluated.\n",
      "Row 708/2871 evaluated.\n",
      "Row 709/2871 evaluated.\n",
      "Row 710/2871 evaluated.\n",
      "Row 711/2871 evaluated.\n",
      "Row 712/2871 evaluated.\n",
      "Row 713/2871 evaluated.\n",
      "Row 714/2871 evaluated.\n",
      "Row 715/2871 evaluated.\n",
      "Row 716/2871 evaluated.\n",
      "Row 717/2871 evaluated.\n",
      "Row 718/2871 evaluated.\n",
      "Row 719/2871 evaluated.\n",
      "Row 720/2871 evaluated.\n",
      "Row 721/2871 evaluated.\n",
      "Row 722/2871 evaluated.\n",
      "Row 723/2871 evaluated.\n",
      "Row 724/2871 evaluated.\n",
      "Row 725/2871 evaluated.\n",
      "Row 726/2871 evaluated.\n",
      "Row 727/2871 evaluated.\n",
      "Row 728/2871 evaluated.\n",
      "Row 729/2871 evaluated.\n",
      "Row 730/2871 evaluated.\n",
      "Row 731/2871 evaluated.\n",
      "Row 732/2871 evaluated.\n",
      "Row 733/2871 evaluated.\n",
      "Row 734/2871 evaluated.\n",
      "Row 735/2871 evaluated.\n",
      "Row 736/2871 evaluated.\n",
      "Row 737/2871 evaluated.\n",
      "Row 738/2871 evaluated.\n",
      "Row 739/2871 evaluated.\n",
      "Row 740/2871 evaluated.\n",
      "Row 741/2871 evaluated.\n",
      "Row 742/2871 evaluated.\n",
      "Row 743/2871 evaluated.\n",
      "Row 744/2871 evaluated.\n",
      "Row 745/2871 evaluated.\n",
      "Row 746/2871 evaluated.\n",
      "Row 747/2871 evaluated.\n",
      "Row 748/2871 evaluated.\n",
      "Row 749/2871 evaluated.\n",
      "Row 750/2871 evaluated.\n",
      "Row 751/2871 evaluated.\n",
      "Row 752/2871 evaluated.\n",
      "Row 753/2871 evaluated.\n",
      "Row 754/2871 evaluated.\n",
      "Row 755/2871 evaluated.\n",
      "Row 756/2871 evaluated.\n",
      "Row 757/2871 evaluated.\n",
      "Row 758/2871 evaluated.\n",
      "Row 759/2871 evaluated.\n",
      "Row 760/2871 evaluated.\n",
      "Row 761/2871 evaluated.\n",
      "Row 762/2871 evaluated.\n",
      "Row 763/2871 evaluated.\n",
      "Row 764/2871 evaluated.\n",
      "Row 765/2871 evaluated.\n",
      "Row 766/2871 evaluated.\n",
      "Row 767/2871 evaluated.\n",
      "Row 768/2871 evaluated.\n",
      "Row 769/2871 evaluated.\n",
      "Row 770/2871 evaluated.\n",
      "Row 771/2871 evaluated.\n",
      "Row 772/2871 evaluated.\n",
      "Row 773/2871 evaluated.\n",
      "Row 774/2871 evaluated.\n",
      "Row 775/2871 evaluated.\n",
      "Row 776/2871 evaluated.\n",
      "Row 777/2871 evaluated.\n",
      "Row 778/2871 evaluated.\n",
      "Row 779/2871 evaluated.\n",
      "Row 780/2871 evaluated.\n",
      "Row 781/2871 evaluated.\n",
      "Row 782/2871 evaluated.\n",
      "Row 783/2871 evaluated.\n",
      "Row 784/2871 evaluated.\n",
      "Row 785/2871 evaluated.\n",
      "Row 786/2871 evaluated.\n",
      "Row 787/2871 evaluated.\n",
      "Row 788/2871 evaluated.\n",
      "Row 789/2871 evaluated.\n",
      "Row 790/2871 evaluated.\n",
      "Row 791/2871 evaluated.\n",
      "Row 792/2871 evaluated.\n",
      "Row 793/2871 evaluated.\n",
      "Row 794/2871 evaluated.\n",
      "Row 795/2871 evaluated.\n",
      "Row 796/2871 evaluated.\n",
      "Row 797/2871 evaluated.\n",
      "Row 798/2871 evaluated.\n",
      "Row 799/2871 evaluated.\n",
      "Row 800/2871 evaluated.\n",
      "Row 801/2871 evaluated.\n",
      "Row 802/2871 evaluated.\n",
      "Row 803/2871 evaluated.\n",
      "Row 804/2871 evaluated.\n",
      "Row 805/2871 evaluated.\n",
      "Row 806/2871 evaluated.\n",
      "Row 807/2871 evaluated.\n",
      "Row 808/2871 evaluated.\n",
      "Row 809/2871 evaluated.\n",
      "Row 810/2871 evaluated.\n",
      "Row 811/2871 evaluated.\n",
      "Row 812/2871 evaluated.\n",
      "Row 813/2871 evaluated.\n",
      "Row 814/2871 evaluated.\n",
      "Row 815/2871 evaluated.\n",
      "Row 816/2871 evaluated.\n",
      "Row 817/2871 evaluated.\n",
      "Row 818/2871 evaluated.\n",
      "Row 819/2871 evaluated.\n",
      "Row 820/2871 evaluated.\n",
      "Row 821/2871 evaluated.\n",
      "Row 822/2871 evaluated.\n",
      "Row 823/2871 evaluated.\n",
      "Row 824/2871 evaluated.\n",
      "Row 825/2871 evaluated.\n",
      "Row 826/2871 evaluated.\n",
      "Row 827/2871 evaluated.\n",
      "Row 828/2871 evaluated.\n",
      "Row 829/2871 evaluated.\n",
      "Row 830/2871 evaluated.\n",
      "Row 831/2871 evaluated.\n",
      "Row 832/2871 evaluated.\n",
      "Row 833/2871 evaluated.\n",
      "Row 834/2871 evaluated.\n",
      "Row 835/2871 evaluated.\n",
      "Row 836/2871 evaluated.\n",
      "Row 837/2871 evaluated.\n",
      "Row 838/2871 evaluated.\n",
      "Row 839/2871 evaluated.\n",
      "Row 840/2871 evaluated.\n",
      "Row 841/2871 evaluated.\n",
      "Row 842/2871 evaluated.\n",
      "Row 843/2871 evaluated.\n",
      "Row 844/2871 evaluated.\n",
      "Row 845/2871 evaluated.\n",
      "Row 846/2871 evaluated.\n",
      "Row 847/2871 evaluated.\n",
      "Row 848/2871 evaluated.\n",
      "Row 849/2871 evaluated.\n",
      "Row 850/2871 evaluated.\n",
      "Row 851/2871 evaluated.\n",
      "Row 852/2871 evaluated.\n",
      "Row 853/2871 evaluated.\n",
      "Row 854/2871 evaluated.\n",
      "Row 855/2871 evaluated.\n",
      "Row 856/2871 evaluated.\n",
      "Row 857/2871 evaluated.\n",
      "Row 858/2871 evaluated.\n",
      "Row 859/2871 evaluated.\n",
      "Row 860/2871 evaluated.\n",
      "Row 861/2871 evaluated.\n",
      "Row 862/2871 evaluated.\n",
      "Row 863/2871 evaluated.\n",
      "Row 864/2871 evaluated.\n",
      "Row 865/2871 evaluated.\n",
      "Row 866/2871 evaluated.\n",
      "Row 867/2871 evaluated.\n",
      "Row 868/2871 evaluated.\n",
      "Row 869/2871 evaluated.\n",
      "Row 870/2871 evaluated.\n",
      "Row 871/2871 evaluated.\n",
      "Row 872/2871 evaluated.\n",
      "Row 873/2871 evaluated.\n",
      "Row 874/2871 evaluated.\n",
      "Row 875/2871 evaluated.\n",
      "Row 876/2871 evaluated.\n",
      "Row 877/2871 evaluated.\n",
      "Row 878/2871 evaluated.\n",
      "Row 879/2871 evaluated.\n",
      "Row 880/2871 evaluated.\n",
      "Row 881/2871 evaluated.\n",
      "Row 882/2871 evaluated.\n",
      "Row 883/2871 evaluated.\n",
      "Row 884/2871 evaluated.\n",
      "Row 885/2871 evaluated.\n",
      "Row 886/2871 evaluated.\n",
      "Row 887/2871 evaluated.\n",
      "Row 888/2871 evaluated.\n",
      "Row 889/2871 evaluated.\n",
      "Row 890/2871 evaluated.\n",
      "Row 891/2871 evaluated.\n",
      "Row 892/2871 evaluated.\n",
      "Row 893/2871 evaluated.\n",
      "Row 894/2871 evaluated.\n",
      "Row 895/2871 evaluated.\n",
      "Row 896/2871 evaluated.\n",
      "Row 897/2871 evaluated.\n",
      "Row 898/2871 evaluated.\n",
      "Row 899/2871 evaluated.\n",
      "Row 900/2871 evaluated.\n",
      "Row 901/2871 evaluated.\n",
      "Row 902/2871 evaluated.\n",
      "Row 903/2871 evaluated.\n",
      "Row 904/2871 evaluated.\n",
      "Row 905/2871 evaluated.\n",
      "Row 906/2871 evaluated.\n",
      "Row 907/2871 evaluated.\n",
      "Row 908/2871 evaluated.\n",
      "Row 909/2871 evaluated.\n",
      "Row 910/2871 evaluated.\n",
      "Row 911/2871 evaluated.\n",
      "Row 912/2871 evaluated.\n",
      "Row 913/2871 evaluated.\n",
      "Row 914/2871 evaluated.\n",
      "Row 915/2871 evaluated.\n",
      "Row 916/2871 evaluated.\n",
      "Row 917/2871 evaluated.\n",
      "Row 918/2871 evaluated.\n",
      "Row 919/2871 evaluated.\n",
      "Row 920/2871 evaluated.\n",
      "Row 921/2871 evaluated.\n",
      "Row 922/2871 evaluated.\n",
      "Row 923/2871 evaluated.\n",
      "Row 924/2871 evaluated.\n",
      "Row 925/2871 evaluated.\n",
      "Row 926/2871 evaluated.\n",
      "Row 927/2871 evaluated.\n",
      "Row 928/2871 evaluated.\n",
      "Row 929/2871 evaluated.\n",
      "Row 930/2871 evaluated.\n",
      "Row 931/2871 evaluated.\n",
      "Row 932/2871 evaluated.\n",
      "Row 933/2871 evaluated.\n",
      "Row 934/2871 evaluated.\n",
      "Row 935/2871 evaluated.\n",
      "Row 936/2871 evaluated.\n",
      "Row 937/2871 evaluated.\n",
      "Row 938/2871 evaluated.\n",
      "Row 939/2871 evaluated.\n",
      "Row 940/2871 evaluated.\n",
      "Row 941/2871 evaluated.\n",
      "Row 942/2871 evaluated.\n",
      "Row 943/2871 evaluated.\n",
      "Row 944/2871 evaluated.\n",
      "Row 945/2871 evaluated.\n",
      "Row 946/2871 evaluated.\n",
      "Row 947/2871 evaluated.\n",
      "Row 948/2871 evaluated.\n",
      "Row 949/2871 evaluated.\n",
      "Row 950/2871 evaluated.\n",
      "Row 951/2871 evaluated.\n",
      "Row 952/2871 evaluated.\n",
      "Row 953/2871 evaluated.\n",
      "Row 954/2871 evaluated.\n",
      "Row 955/2871 evaluated.\n",
      "Row 956/2871 evaluated.\n",
      "Row 957/2871 evaluated.\n",
      "Row 958/2871 evaluated.\n",
      "Row 959/2871 evaluated.\n",
      "Row 960/2871 evaluated.\n",
      "Row 961/2871 evaluated.\n",
      "Row 962/2871 evaluated.\n",
      "Row 963/2871 evaluated.\n",
      "Row 964/2871 evaluated.\n",
      "Row 965/2871 evaluated.\n",
      "Row 966/2871 evaluated.\n",
      "Row 967/2871 evaluated.\n",
      "Row 968/2871 evaluated.\n",
      "Row 969/2871 evaluated.\n",
      "Row 970/2871 evaluated.\n",
      "Row 971/2871 evaluated.\n",
      "Row 972/2871 evaluated.\n",
      "Row 973/2871 evaluated.\n",
      "Row 974/2871 evaluated.\n",
      "Row 975/2871 evaluated.\n",
      "Row 976/2871 evaluated.\n",
      "Row 977/2871 evaluated.\n",
      "Row 978/2871 evaluated.\n",
      "Row 979/2871 evaluated.\n",
      "Row 980/2871 evaluated.\n",
      "Row 981/2871 evaluated.\n",
      "Row 982/2871 evaluated.\n",
      "Row 983/2871 evaluated.\n",
      "Row 984/2871 evaluated.\n",
      "Row 985/2871 evaluated.\n",
      "Row 986/2871 evaluated.\n",
      "Row 987/2871 evaluated.\n",
      "Row 988/2871 evaluated.\n",
      "Row 989/2871 evaluated.\n",
      "Row 990/2871 evaluated.\n",
      "Row 991/2871 evaluated.\n",
      "Row 992/2871 evaluated.\n",
      "Row 993/2871 evaluated.\n",
      "Row 994/2871 evaluated.\n",
      "Row 995/2871 evaluated.\n",
      "Row 996/2871 evaluated.\n",
      "Row 997/2871 evaluated.\n",
      "Row 998/2871 evaluated.\n",
      "Row 999/2871 evaluated.\n",
      "Row 1000/2871 evaluated.\n",
      "Row 1001/2871 evaluated.\n",
      "Row 1002/2871 evaluated.\n",
      "Row 1003/2871 evaluated.\n",
      "Row 1004/2871 evaluated.\n",
      "Row 1005/2871 evaluated.\n",
      "Row 1006/2871 evaluated.\n",
      "Row 1007/2871 evaluated.\n",
      "Row 1008/2871 evaluated.\n",
      "Row 1009/2871 evaluated.\n",
      "Row 1010/2871 evaluated.\n",
      "Row 1011/2871 evaluated.\n",
      "Row 1012/2871 evaluated.\n",
      "Row 1013/2871 evaluated.\n",
      "Row 1014/2871 evaluated.\n",
      "Row 1015/2871 evaluated.\n",
      "Row 1016/2871 evaluated.\n",
      "Row 1017/2871 evaluated.\n",
      "Row 1018/2871 evaluated.\n",
      "Row 1019/2871 evaluated.\n",
      "Row 1020/2871 evaluated.\n",
      "Row 1021/2871 evaluated.\n",
      "Row 1022/2871 evaluated.\n",
      "Row 1023/2871 evaluated.\n",
      "Row 1024/2871 evaluated.\n",
      "Row 1025/2871 evaluated.\n",
      "Row 1026/2871 evaluated.\n",
      "Row 1027/2871 evaluated.\n",
      "Row 1028/2871 evaluated.\n",
      "Row 1029/2871 evaluated.\n",
      "Row 1030/2871 evaluated.\n",
      "Row 1031/2871 evaluated.\n",
      "Row 1032/2871 evaluated.\n",
      "Row 1033/2871 evaluated.\n",
      "Row 1034/2871 evaluated.\n",
      "Row 1035/2871 evaluated.\n",
      "Row 1036/2871 evaluated.\n",
      "Row 1037/2871 evaluated.\n",
      "Row 1038/2871 evaluated.\n",
      "Row 1039/2871 evaluated.\n",
      "Row 1040/2871 evaluated.\n",
      "Row 1041/2871 evaluated.\n",
      "Row 1042/2871 evaluated.\n",
      "Row 1043/2871 evaluated.\n",
      "Row 1044/2871 evaluated.\n",
      "Row 1045/2871 evaluated.\n",
      "Row 1046/2871 evaluated.\n",
      "Row 1047/2871 evaluated.\n",
      "Row 1048/2871 evaluated.\n",
      "Row 1049/2871 evaluated.\n",
      "Row 1050/2871 evaluated.\n",
      "Row 1051/2871 evaluated.\n",
      "Row 1052/2871 evaluated.\n",
      "Row 1053/2871 evaluated.\n",
      "Row 1054/2871 evaluated.\n",
      "Row 1055/2871 evaluated.\n",
      "Row 1056/2871 evaluated.\n",
      "Row 1057/2871 evaluated.\n",
      "Row 1058/2871 evaluated.\n",
      "Row 1059/2871 evaluated.\n",
      "Row 1060/2871 evaluated.\n",
      "Row 1061/2871 evaluated.\n",
      "Row 1062/2871 evaluated.\n",
      "Row 1063/2871 evaluated.\n",
      "Row 1064/2871 evaluated.\n",
      "Row 1065/2871 evaluated.\n",
      "Row 1066/2871 evaluated.\n",
      "Row 1067/2871 evaluated.\n",
      "Row 1068/2871 evaluated.\n",
      "Row 1069/2871 evaluated.\n",
      "Row 1070/2871 evaluated.\n",
      "Row 1071/2871 evaluated.\n",
      "Row 1072/2871 evaluated.\n",
      "Row 1073/2871 evaluated.\n",
      "Row 1074/2871 evaluated.\n",
      "Row 1075/2871 evaluated.\n",
      "Row 1076/2871 evaluated.\n",
      "Row 1077/2871 evaluated.\n",
      "Row 1078/2871 evaluated.\n",
      "Row 1079/2871 evaluated.\n",
      "Row 1080/2871 evaluated.\n",
      "Row 1081/2871 evaluated.\n",
      "Row 1082/2871 evaluated.\n",
      "Row 1083/2871 evaluated.\n",
      "Row 1084/2871 evaluated.\n",
      "Row 1085/2871 evaluated.\n",
      "Row 1086/2871 evaluated.\n",
      "Row 1087/2871 evaluated.\n",
      "Row 1088/2871 evaluated.\n",
      "Row 1089/2871 evaluated.\n",
      "Row 1090/2871 evaluated.\n",
      "Row 1091/2871 evaluated.\n",
      "Row 1092/2871 evaluated.\n",
      "Row 1093/2871 evaluated.\n",
      "Row 1094/2871 evaluated.\n",
      "Row 1095/2871 evaluated.\n",
      "Row 1096/2871 evaluated.\n",
      "Row 1097/2871 evaluated.\n",
      "Row 1098/2871 evaluated.\n",
      "Row 1099/2871 evaluated.\n",
      "Row 1100/2871 evaluated.\n",
      "Row 1101/2871 evaluated.\n",
      "Row 1102/2871 evaluated.\n",
      "Row 1103/2871 evaluated.\n",
      "Row 1104/2871 evaluated.\n",
      "Row 1105/2871 evaluated.\n",
      "Row 1106/2871 evaluated.\n",
      "Row 1107/2871 evaluated.\n",
      "Row 1108/2871 evaluated.\n",
      "Row 1109/2871 evaluated.\n",
      "Row 1110/2871 evaluated.\n",
      "Row 1111/2871 evaluated.\n",
      "Row 1112/2871 evaluated.\n",
      "Row 1113/2871 evaluated.\n",
      "Row 1114/2871 evaluated.\n",
      "Row 1115/2871 evaluated.\n",
      "Row 1116/2871 evaluated.\n",
      "Row 1117/2871 evaluated.\n",
      "Row 1118/2871 evaluated.\n",
      "Row 1119/2871 evaluated.\n",
      "Row 1120/2871 evaluated.\n",
      "Row 1121/2871 evaluated.\n",
      "Row 1122/2871 evaluated.\n",
      "Row 1123/2871 evaluated.\n",
      "Row 1124/2871 evaluated.\n",
      "Row 1125/2871 evaluated.\n",
      "Row 1126/2871 evaluated.\n",
      "Row 1127/2871 evaluated.\n",
      "Row 1128/2871 evaluated.\n",
      "Row 1129/2871 evaluated.\n",
      "Row 1130/2871 evaluated.\n",
      "Row 1131/2871 evaluated.\n",
      "Row 1132/2871 evaluated.\n",
      "Row 1133/2871 evaluated.\n",
      "Row 1134/2871 evaluated.\n",
      "Row 1135/2871 evaluated.\n",
      "Row 1136/2871 evaluated.\n",
      "Row 1137/2871 evaluated.\n",
      "Row 1138/2871 evaluated.\n",
      "Row 1139/2871 evaluated.\n",
      "Row 1140/2871 evaluated.\n",
      "Row 1141/2871 evaluated.\n",
      "Row 1142/2871 evaluated.\n",
      "Row 1143/2871 evaluated.\n",
      "Row 1144/2871 evaluated.\n",
      "Row 1145/2871 evaluated.\n",
      "Row 1146/2871 evaluated.\n",
      "Row 1147/2871 evaluated.\n",
      "Row 1148/2871 evaluated.\n",
      "Row 1149/2871 evaluated.\n",
      "Row 1150/2871 evaluated.\n",
      "Row 1151/2871 evaluated.\n",
      "Row 1152/2871 evaluated.\n",
      "Row 1153/2871 evaluated.\n",
      "Row 1154/2871 evaluated.\n",
      "Row 1155/2871 evaluated.\n",
      "Row 1156/2871 evaluated.\n",
      "Row 1157/2871 evaluated.\n",
      "Row 1158/2871 evaluated.\n",
      "Row 1159/2871 evaluated.\n",
      "Row 1160/2871 evaluated.\n",
      "Row 1161/2871 evaluated.\n",
      "Row 1162/2871 evaluated.\n",
      "Row 1163/2871 evaluated.\n",
      "Row 1164/2871 evaluated.\n",
      "Row 1165/2871 evaluated.\n",
      "Row 1166/2871 evaluated.\n",
      "Row 1167/2871 evaluated.\n",
      "Row 1168/2871 evaluated.\n",
      "Row 1169/2871 evaluated.\n",
      "Row 1170/2871 evaluated.\n",
      "Row 1171/2871 evaluated.\n",
      "Row 1172/2871 evaluated.\n",
      "Row 1173/2871 evaluated.\n",
      "Row 1174/2871 evaluated.\n",
      "Row 1175/2871 evaluated.\n",
      "Row 1176/2871 evaluated.\n",
      "Row 1177/2871 evaluated.\n",
      "Row 1178/2871 evaluated.\n",
      "Row 1179/2871 evaluated.\n",
      "Row 1180/2871 evaluated.\n",
      "Row 1181/2871 evaluated.\n",
      "Row 1182/2871 evaluated.\n",
      "Row 1183/2871 evaluated.\n",
      "Row 1184/2871 evaluated.\n",
      "Row 1185/2871 evaluated.\n",
      "Row 1186/2871 evaluated.\n",
      "Row 1187/2871 evaluated.\n",
      "Row 1188/2871 evaluated.\n",
      "Row 1189/2871 evaluated.\n",
      "Row 1190/2871 evaluated.\n",
      "Row 1191/2871 evaluated.\n",
      "Row 1192/2871 evaluated.\n",
      "Row 1193/2871 evaluated.\n",
      "Row 1194/2871 evaluated.\n",
      "Row 1195/2871 evaluated.\n",
      "Row 1196/2871 evaluated.\n",
      "Row 1197/2871 evaluated.\n",
      "Row 1198/2871 evaluated.\n",
      "Row 1199/2871 evaluated.\n",
      "Row 1200/2871 evaluated.\n",
      "Row 1201/2871 evaluated.\n",
      "Row 1202/2871 evaluated.\n",
      "Row 1203/2871 evaluated.\n",
      "Row 1204/2871 evaluated.\n",
      "Row 1205/2871 evaluated.\n",
      "Row 1206/2871 evaluated.\n",
      "Row 1207/2871 evaluated.\n",
      "Row 1208/2871 evaluated.\n",
      "Row 1209/2871 evaluated.\n",
      "Row 1210/2871 evaluated.\n",
      "Row 1211/2871 evaluated.\n",
      "Row 1212/2871 evaluated.\n",
      "Row 1213/2871 evaluated.\n",
      "Row 1214/2871 evaluated.\n",
      "Row 1215/2871 evaluated.\n",
      "Row 1216/2871 evaluated.\n",
      "Row 1217/2871 evaluated.\n",
      "Row 1218/2871 evaluated.\n",
      "Row 1219/2871 evaluated.\n",
      "Row 1220/2871 evaluated.\n",
      "Row 1221/2871 evaluated.\n",
      "Row 1222/2871 evaluated.\n",
      "Row 1223/2871 evaluated.\n",
      "Row 1224/2871 evaluated.\n",
      "Row 1225/2871 evaluated.\n",
      "Row 1226/2871 evaluated.\n",
      "Row 1227/2871 evaluated.\n",
      "Row 1228/2871 evaluated.\n",
      "Row 1229/2871 evaluated.\n",
      "Row 1230/2871 evaluated.\n",
      "Row 1231/2871 evaluated.\n",
      "Row 1232/2871 evaluated.\n",
      "Row 1233/2871 evaluated.\n",
      "Row 1234/2871 evaluated.\n",
      "Row 1235/2871 evaluated.\n",
      "Row 1236/2871 evaluated.\n",
      "Row 1237/2871 evaluated.\n",
      "Row 1238/2871 evaluated.\n",
      "Row 1239/2871 evaluated.\n",
      "Row 1240/2871 evaluated.\n",
      "Row 1241/2871 evaluated.\n",
      "Row 1242/2871 evaluated.\n",
      "Row 1243/2871 evaluated.\n",
      "Row 1244/2871 evaluated.\n",
      "Row 1245/2871 evaluated.\n",
      "Row 1246/2871 evaluated.\n",
      "Row 1247/2871 evaluated.\n",
      "Row 1248/2871 evaluated.\n",
      "Row 1249/2871 evaluated.\n",
      "Row 1250/2871 evaluated.\n",
      "Row 1251/2871 evaluated.\n",
      "Row 1252/2871 evaluated.\n",
      "Row 1253/2871 evaluated.\n",
      "Row 1254/2871 evaluated.\n",
      "Row 1255/2871 evaluated.\n",
      "Row 1256/2871 evaluated.\n",
      "Row 1257/2871 evaluated.\n",
      "Row 1258/2871 evaluated.\n",
      "Row 1259/2871 evaluated.\n",
      "Row 1260/2871 evaluated.\n",
      "Row 1261/2871 evaluated.\n",
      "Row 1262/2871 evaluated.\n",
      "Row 1263/2871 evaluated.\n",
      "Row 1264/2871 evaluated.\n",
      "Row 1265/2871 evaluated.\n",
      "Row 1266/2871 evaluated.\n",
      "Row 1267/2871 evaluated.\n",
      "Row 1268/2871 evaluated.\n",
      "Row 1269/2871 evaluated.\n",
      "Row 1270/2871 evaluated.\n",
      "Row 1271/2871 evaluated.\n",
      "Row 1272/2871 evaluated.\n",
      "Row 1273/2871 evaluated.\n",
      "Row 1274/2871 evaluated.\n",
      "Row 1275/2871 evaluated.\n",
      "Row 1276/2871 evaluated.\n",
      "Row 1277/2871 evaluated.\n",
      "Row 1278/2871 evaluated.\n",
      "Row 1279/2871 evaluated.\n",
      "Row 1280/2871 evaluated.\n",
      "Row 1281/2871 evaluated.\n",
      "Row 1282/2871 evaluated.\n",
      "Row 1283/2871 evaluated.\n",
      "Row 1284/2871 evaluated.\n",
      "Row 1285/2871 evaluated.\n",
      "Row 1286/2871 evaluated.\n",
      "Row 1287/2871 evaluated.\n",
      "Row 1288/2871 evaluated.\n",
      "Row 1289/2871 evaluated.\n",
      "Row 1290/2871 evaluated.\n",
      "Row 1291/2871 evaluated.\n",
      "Row 1292/2871 evaluated.\n",
      "Row 1293/2871 evaluated.\n",
      "Row 1294/2871 evaluated.\n",
      "Row 1295/2871 evaluated.\n",
      "Row 1296/2871 evaluated.\n",
      "Row 1297/2871 evaluated.\n",
      "Row 1298/2871 evaluated.\n",
      "Row 1299/2871 evaluated.\n",
      "Row 1300/2871 evaluated.\n",
      "Row 1301/2871 evaluated.\n",
      "Row 1302/2871 evaluated.\n",
      "Row 1303/2871 evaluated.\n",
      "Row 1304/2871 evaluated.\n",
      "Row 1305/2871 evaluated.\n",
      "Row 1306/2871 evaluated.\n",
      "Row 1307/2871 evaluated.\n",
      "Row 1308/2871 evaluated.\n",
      "Row 1309/2871 evaluated.\n",
      "Row 1310/2871 evaluated.\n",
      "Row 1311/2871 evaluated.\n",
      "Row 1312/2871 evaluated.\n",
      "Row 1313/2871 evaluated.\n",
      "Row 1314/2871 evaluated.\n",
      "Row 1315/2871 evaluated.\n",
      "Row 1316/2871 evaluated.\n",
      "Row 1317/2871 evaluated.\n",
      "Row 1318/2871 evaluated.\n",
      "Row 1319/2871 evaluated.\n",
      "Row 1320/2871 evaluated.\n",
      "Row 1321/2871 evaluated.\n",
      "Row 1322/2871 evaluated.\n",
      "Row 1323/2871 evaluated.\n",
      "Row 1324/2871 evaluated.\n",
      "Row 1325/2871 evaluated.\n",
      "Row 1326/2871 evaluated.\n",
      "Row 1327/2871 evaluated.\n",
      "Row 1328/2871 evaluated.\n",
      "Row 1329/2871 evaluated.\n",
      "Row 1330/2871 evaluated.\n",
      "Row 1331/2871 evaluated.\n",
      "Row 1332/2871 evaluated.\n",
      "Row 1333/2871 evaluated.\n",
      "Row 1334/2871 evaluated.\n",
      "Row 1335/2871 evaluated.\n",
      "Row 1336/2871 evaluated.\n",
      "Row 1337/2871 evaluated.\n",
      "Row 1338/2871 evaluated.\n",
      "Row 1339/2871 evaluated.\n",
      "Row 1340/2871 evaluated.\n",
      "Row 1341/2871 evaluated.\n",
      "Row 1342/2871 evaluated.\n",
      "Row 1343/2871 evaluated.\n",
      "Row 1344/2871 evaluated.\n",
      "Row 1345/2871 evaluated.\n",
      "Row 1346/2871 evaluated.\n",
      "Row 1347/2871 evaluated.\n",
      "Row 1348/2871 evaluated.\n",
      "Row 1349/2871 evaluated.\n",
      "Row 1350/2871 evaluated.\n",
      "Row 1351/2871 evaluated.\n",
      "Row 1352/2871 evaluated.\n",
      "Row 1353/2871 evaluated.\n",
      "Row 1354/2871 evaluated.\n",
      "Row 1355/2871 evaluated.\n",
      "Row 1356/2871 evaluated.\n",
      "Row 1357/2871 evaluated.\n",
      "Row 1358/2871 evaluated.\n",
      "Row 1359/2871 evaluated.\n",
      "Row 1360/2871 evaluated.\n",
      "Row 1361/2871 evaluated.\n",
      "Row 1362/2871 evaluated.\n",
      "Row 1363/2871 evaluated.\n",
      "Row 1364/2871 evaluated.\n",
      "Row 1365/2871 evaluated.\n",
      "Row 1366/2871 evaluated.\n",
      "Row 1367/2871 evaluated.\n",
      "Row 1368/2871 evaluated.\n",
      "Row 1369/2871 evaluated.\n",
      "Row 1370/2871 evaluated.\n",
      "Row 1371/2871 evaluated.\n",
      "Row 1372/2871 evaluated.\n",
      "Row 1373/2871 evaluated.\n",
      "Row 1374/2871 evaluated.\n",
      "Row 1375/2871 evaluated.\n",
      "Row 1376/2871 evaluated.\n",
      "Row 1377/2871 evaluated.\n",
      "Row 1378/2871 evaluated.\n",
      "Row 1379/2871 evaluated.\n",
      "Row 1380/2871 evaluated.\n",
      "Row 1381/2871 evaluated.\n",
      "Row 1382/2871 evaluated.\n",
      "Row 1383/2871 evaluated.\n",
      "Row 1384/2871 evaluated.\n",
      "Row 1385/2871 evaluated.\n",
      "Row 1386/2871 evaluated.\n",
      "Row 1387/2871 evaluated.\n",
      "Skipping row 1388: Empty or invalid text.\n",
      "Row 1389/2871 evaluated.\n",
      "Row 1390/2871 evaluated.\n",
      "Row 1391/2871 evaluated.\n",
      "Row 1392/2871 evaluated.\n",
      "Row 1393/2871 evaluated.\n",
      "Row 1394/2871 evaluated.\n",
      "Row 1395/2871 evaluated.\n",
      "Row 1396/2871 evaluated.\n",
      "Row 1397/2871 evaluated.\n",
      "Row 1398/2871 evaluated.\n",
      "Row 1399/2871 evaluated.\n",
      "Row 1400/2871 evaluated.\n",
      "Row 1401/2871 evaluated.\n",
      "Row 1402/2871 evaluated.\n",
      "Row 1403/2871 evaluated.\n",
      "Row 1404/2871 evaluated.\n",
      "Row 1405/2871 evaluated.\n",
      "Row 1406/2871 evaluated.\n",
      "Row 1407/2871 evaluated.\n",
      "Row 1408/2871 evaluated.\n",
      "Row 1409/2871 evaluated.\n",
      "Row 1410/2871 evaluated.\n",
      "Row 1411/2871 evaluated.\n",
      "Row 1412/2871 evaluated.\n",
      "Row 1413/2871 evaluated.\n",
      "Row 1414/2871 evaluated.\n",
      "Row 1415/2871 evaluated.\n",
      "Row 1416/2871 evaluated.\n",
      "Row 1417/2871 evaluated.\n",
      "Row 1418/2871 evaluated.\n",
      "Row 1419/2871 evaluated.\n",
      "Row 1420/2871 evaluated.\n",
      "Row 1421/2871 evaluated.\n",
      "Row 1422/2871 evaluated.\n",
      "Row 1423/2871 evaluated.\n",
      "Row 1424/2871 evaluated.\n",
      "Row 1425/2871 evaluated.\n",
      "Row 1426/2871 evaluated.\n",
      "Row 1427/2871 evaluated.\n",
      "Row 1428/2871 evaluated.\n",
      "Row 1429/2871 evaluated.\n",
      "Row 1430/2871 evaluated.\n",
      "Row 1431/2871 evaluated.\n",
      "Row 1432/2871 evaluated.\n",
      "Row 1433/2871 evaluated.\n",
      "Row 1434/2871 evaluated.\n",
      "Row 1435/2871 evaluated.\n",
      "Row 1436/2871 evaluated.\n",
      "Row 1437/2871 evaluated.\n",
      "Row 1438/2871 evaluated.\n",
      "Row 1439/2871 evaluated.\n",
      "Row 1440/2871 evaluated.\n",
      "Row 1441/2871 evaluated.\n",
      "Row 1442/2871 evaluated.\n",
      "Row 1443/2871 evaluated.\n",
      "Row 1444/2871 evaluated.\n",
      "Row 1445/2871 evaluated.\n",
      "Row 1446/2871 evaluated.\n",
      "Row 1447/2871 evaluated.\n",
      "Row 1448/2871 evaluated.\n",
      "Row 1449/2871 evaluated.\n",
      "Row 1450/2871 evaluated.\n",
      "Row 1451/2871 evaluated.\n",
      "Row 1452/2871 evaluated.\n",
      "Row 1453/2871 evaluated.\n",
      "Row 1454/2871 evaluated.\n",
      "Row 1455/2871 evaluated.\n",
      "Row 1456/2871 evaluated.\n",
      "Row 1457/2871 evaluated.\n",
      "Row 1458/2871 evaluated.\n",
      "Row 1459/2871 evaluated.\n",
      "Row 1460/2871 evaluated.\n",
      "Row 1461/2871 evaluated.\n",
      "Row 1462/2871 evaluated.\n",
      "Row 1463/2871 evaluated.\n",
      "Row 1464/2871 evaluated.\n",
      "Row 1465/2871 evaluated.\n",
      "Row 1466/2871 evaluated.\n",
      "Row 1467/2871 evaluated.\n",
      "Row 1468/2871 evaluated.\n",
      "Row 1469/2871 evaluated.\n",
      "Row 1470/2871 evaluated.\n",
      "Row 1471/2871 evaluated.\n",
      "Row 1472/2871 evaluated.\n",
      "Row 1473/2871 evaluated.\n",
      "Row 1474/2871 evaluated.\n",
      "Row 1475/2871 evaluated.\n",
      "Row 1476/2871 evaluated.\n",
      "Row 1477/2871 evaluated.\n",
      "Row 1478/2871 evaluated.\n",
      "Row 1479/2871 evaluated.\n",
      "Row 1480/2871 evaluated.\n",
      "Row 1481/2871 evaluated.\n",
      "Row 1482/2871 evaluated.\n",
      "Row 1483/2871 evaluated.\n",
      "Row 1484/2871 evaluated.\n",
      "Row 1485/2871 evaluated.\n",
      "Row 1486/2871 evaluated.\n",
      "Row 1487/2871 evaluated.\n",
      "Row 1488/2871 evaluated.\n",
      "Row 1489/2871 evaluated.\n",
      "Row 1490/2871 evaluated.\n",
      "Row 1491/2871 evaluated.\n",
      "Row 1492/2871 evaluated.\n",
      "Row 1493/2871 evaluated.\n",
      "Row 1494/2871 evaluated.\n",
      "Row 1495/2871 evaluated.\n",
      "Row 1496/2871 evaluated.\n",
      "Row 1497/2871 evaluated.\n",
      "Row 1498/2871 evaluated.\n",
      "Row 1499/2871 evaluated.\n",
      "Row 1500/2871 evaluated.\n",
      "Row 1501/2871 evaluated.\n",
      "Row 1502/2871 evaluated.\n",
      "Row 1503/2871 evaluated.\n",
      "Row 1504/2871 evaluated.\n",
      "Row 1505/2871 evaluated.\n",
      "Row 1506/2871 evaluated.\n",
      "Row 1507/2871 evaluated.\n",
      "Row 1508/2871 evaluated.\n",
      "Row 1509/2871 evaluated.\n",
      "Row 1510/2871 evaluated.\n",
      "Row 1511/2871 evaluated.\n",
      "Row 1512/2871 evaluated.\n",
      "Row 1513/2871 evaluated.\n",
      "Row 1514/2871 evaluated.\n",
      "Row 1515/2871 evaluated.\n",
      "Row 1516/2871 evaluated.\n",
      "Row 1517/2871 evaluated.\n",
      "Row 1518/2871 evaluated.\n",
      "Row 1519/2871 evaluated.\n",
      "Row 1520/2871 evaluated.\n",
      "Row 1521/2871 evaluated.\n",
      "Row 1522/2871 evaluated.\n",
      "Row 1523/2871 evaluated.\n",
      "Row 1524/2871 evaluated.\n",
      "Row 1525/2871 evaluated.\n",
      "Row 1526/2871 evaluated.\n",
      "Row 1527/2871 evaluated.\n",
      "Row 1528/2871 evaluated.\n",
      "Row 1529/2871 evaluated.\n",
      "Row 1530/2871 evaluated.\n",
      "Row 1531/2871 evaluated.\n",
      "Row 1532/2871 evaluated.\n",
      "Row 1533/2871 evaluated.\n",
      "Row 1534/2871 evaluated.\n",
      "Row 1535/2871 evaluated.\n",
      "Row 1536/2871 evaluated.\n",
      "Row 1537/2871 evaluated.\n",
      "Row 1538/2871 evaluated.\n",
      "Row 1539/2871 evaluated.\n",
      "Row 1540/2871 evaluated.\n",
      "Row 1541/2871 evaluated.\n",
      "Row 1542/2871 evaluated.\n",
      "Row 1543/2871 evaluated.\n",
      "Row 1544/2871 evaluated.\n",
      "Row 1545/2871 evaluated.\n",
      "Row 1546/2871 evaluated.\n",
      "Row 1547/2871 evaluated.\n",
      "Row 1548/2871 evaluated.\n",
      "Row 1549/2871 evaluated.\n",
      "Row 1550/2871 evaluated.\n",
      "Row 1551/2871 evaluated.\n",
      "Row 1552/2871 evaluated.\n",
      "Row 1553/2871 evaluated.\n",
      "Row 1554/2871 evaluated.\n",
      "Row 1555/2871 evaluated.\n",
      "Row 1556/2871 evaluated.\n",
      "Row 1557/2871 evaluated.\n",
      "Row 1558/2871 evaluated.\n",
      "Row 1559/2871 evaluated.\n",
      "Row 1560/2871 evaluated.\n",
      "Row 1561/2871 evaluated.\n",
      "Row 1562/2871 evaluated.\n",
      "Row 1563/2871 evaluated.\n",
      "Row 1564/2871 evaluated.\n",
      "Row 1565/2871 evaluated.\n",
      "Row 1566/2871 evaluated.\n",
      "Row 1567/2871 evaluated.\n",
      "Row 1568/2871 evaluated.\n",
      "Row 1569/2871 evaluated.\n",
      "Row 1570/2871 evaluated.\n",
      "Row 1571/2871 evaluated.\n",
      "Row 1572/2871 evaluated.\n",
      "Row 1573/2871 evaluated.\n",
      "Row 1574/2871 evaluated.\n",
      "Row 1575/2871 evaluated.\n",
      "Row 1576/2871 evaluated.\n",
      "Row 1577/2871 evaluated.\n",
      "Row 1578/2871 evaluated.\n",
      "Row 1579/2871 evaluated.\n",
      "Row 1580/2871 evaluated.\n",
      "Row 1581/2871 evaluated.\n",
      "Row 1582/2871 evaluated.\n",
      "Row 1583/2871 evaluated.\n",
      "Row 1584/2871 evaluated.\n",
      "Row 1585/2871 evaluated.\n",
      "Row 1586/2871 evaluated.\n",
      "Row 1587/2871 evaluated.\n",
      "Row 1588/2871 evaluated.\n",
      "Row 1589/2871 evaluated.\n",
      "Row 1590/2871 evaluated.\n",
      "Row 1591/2871 evaluated.\n",
      "Row 1592/2871 evaluated.\n",
      "Row 1593/2871 evaluated.\n",
      "Row 1594/2871 evaluated.\n",
      "Row 1595/2871 evaluated.\n",
      "Row 1596/2871 evaluated.\n",
      "Row 1597/2871 evaluated.\n",
      "Row 1598/2871 evaluated.\n",
      "Row 1599/2871 evaluated.\n",
      "Row 1600/2871 evaluated.\n",
      "Row 1601/2871 evaluated.\n",
      "Row 1602/2871 evaluated.\n",
      "Row 1603/2871 evaluated.\n",
      "Row 1604/2871 evaluated.\n",
      "Row 1605/2871 evaluated.\n",
      "Row 1606/2871 evaluated.\n",
      "Row 1607/2871 evaluated.\n",
      "Row 1608/2871 evaluated.\n",
      "Row 1609/2871 evaluated.\n",
      "Row 1610/2871 evaluated.\n",
      "Row 1611/2871 evaluated.\n",
      "Row 1612/2871 evaluated.\n",
      "Row 1613/2871 evaluated.\n",
      "Row 1614/2871 evaluated.\n",
      "Row 1615/2871 evaluated.\n",
      "Row 1616/2871 evaluated.\n",
      "Row 1617/2871 evaluated.\n",
      "Row 1618/2871 evaluated.\n",
      "Row 1619/2871 evaluated.\n",
      "Row 1620/2871 evaluated.\n",
      "Row 1621/2871 evaluated.\n",
      "Row 1622/2871 evaluated.\n",
      "Row 1623/2871 evaluated.\n",
      "Row 1624/2871 evaluated.\n",
      "Row 1625/2871 evaluated.\n",
      "Row 1626/2871 evaluated.\n",
      "Row 1627/2871 evaluated.\n",
      "Row 1628/2871 evaluated.\n",
      "Row 1629/2871 evaluated.\n",
      "Row 1630/2871 evaluated.\n",
      "Row 1631/2871 evaluated.\n",
      "Row 1632/2871 evaluated.\n",
      "Row 1633/2871 evaluated.\n",
      "Row 1634/2871 evaluated.\n",
      "Row 1635/2871 evaluated.\n",
      "Row 1636/2871 evaluated.\n",
      "Row 1637/2871 evaluated.\n",
      "Row 1638/2871 evaluated.\n",
      "Row 1639/2871 evaluated.\n",
      "Row 1640/2871 evaluated.\n",
      "Row 1641/2871 evaluated.\n",
      "Row 1642/2871 evaluated.\n",
      "Row 1643/2871 evaluated.\n",
      "Row 1644/2871 evaluated.\n",
      "Row 1645/2871 evaluated.\n",
      "Row 1646/2871 evaluated.\n",
      "Row 1647/2871 evaluated.\n",
      "Row 1648/2871 evaluated.\n",
      "Row 1649/2871 evaluated.\n",
      "Row 1650/2871 evaluated.\n",
      "Row 1651/2871 evaluated.\n",
      "Row 1652/2871 evaluated.\n",
      "Row 1653/2871 evaluated.\n",
      "Row 1654/2871 evaluated.\n",
      "Row 1655/2871 evaluated.\n",
      "Row 1656/2871 evaluated.\n",
      "Row 1657/2871 evaluated.\n",
      "Row 1658/2871 evaluated.\n",
      "Row 1659/2871 evaluated.\n",
      "Row 1660/2871 evaluated.\n",
      "Row 1661/2871 evaluated.\n",
      "Row 1662/2871 evaluated.\n",
      "Row 1663/2871 evaluated.\n",
      "Row 1664/2871 evaluated.\n",
      "Row 1665/2871 evaluated.\n",
      "Row 1666/2871 evaluated.\n",
      "Row 1667/2871 evaluated.\n",
      "Row 1668/2871 evaluated.\n",
      "Row 1669/2871 evaluated.\n",
      "Row 1670/2871 evaluated.\n",
      "Row 1671/2871 evaluated.\n",
      "Row 1672/2871 evaluated.\n",
      "Row 1673/2871 evaluated.\n",
      "Row 1674/2871 evaluated.\n",
      "Row 1675/2871 evaluated.\n",
      "Row 1676/2871 evaluated.\n",
      "Row 1677/2871 evaluated.\n",
      "Row 1678/2871 evaluated.\n",
      "Row 1679/2871 evaluated.\n",
      "Row 1680/2871 evaluated.\n",
      "Row 1681/2871 evaluated.\n",
      "Row 1682/2871 evaluated.\n",
      "Row 1683/2871 evaluated.\n",
      "Row 1684/2871 evaluated.\n",
      "Row 1685/2871 evaluated.\n",
      "Row 1686/2871 evaluated.\n",
      "Row 1687/2871 evaluated.\n",
      "Row 1688/2871 evaluated.\n",
      "Row 1689/2871 evaluated.\n",
      "Row 1690/2871 evaluated.\n",
      "Row 1691/2871 evaluated.\n",
      "Row 1692/2871 evaluated.\n",
      "Row 1693/2871 evaluated.\n",
      "Row 1694/2871 evaluated.\n",
      "Row 1695/2871 evaluated.\n",
      "Row 1696/2871 evaluated.\n",
      "Row 1697/2871 evaluated.\n",
      "Row 1698/2871 evaluated.\n",
      "Row 1699/2871 evaluated.\n",
      "Row 1700/2871 evaluated.\n",
      "Row 1701/2871 evaluated.\n",
      "Row 1702/2871 evaluated.\n",
      "Row 1703/2871 evaluated.\n",
      "Row 1704/2871 evaluated.\n",
      "Row 1705/2871 evaluated.\n",
      "Row 1706/2871 evaluated.\n",
      "Row 1707/2871 evaluated.\n",
      "Row 1708/2871 evaluated.\n",
      "Row 1709/2871 evaluated.\n",
      "Row 1710/2871 evaluated.\n",
      "Row 1711/2871 evaluated.\n",
      "Row 1712/2871 evaluated.\n",
      "Row 1713/2871 evaluated.\n",
      "Row 1714/2871 evaluated.\n",
      "Row 1715/2871 evaluated.\n",
      "Row 1716/2871 evaluated.\n",
      "Row 1717/2871 evaluated.\n",
      "Row 1718/2871 evaluated.\n",
      "Row 1719/2871 evaluated.\n",
      "Row 1720/2871 evaluated.\n",
      "Row 1721/2871 evaluated.\n",
      "Row 1722/2871 evaluated.\n",
      "Row 1723/2871 evaluated.\n",
      "Row 1724/2871 evaluated.\n",
      "Row 1725/2871 evaluated.\n",
      "Row 1726/2871 evaluated.\n",
      "Row 1727/2871 evaluated.\n",
      "Row 1728/2871 evaluated.\n",
      "Row 1729/2871 evaluated.\n",
      "Row 1730/2871 evaluated.\n",
      "Row 1731/2871 evaluated.\n",
      "Row 1732/2871 evaluated.\n",
      "Row 1733/2871 evaluated.\n",
      "Row 1734/2871 evaluated.\n",
      "Row 1735/2871 evaluated.\n",
      "Row 1736/2871 evaluated.\n",
      "Row 1737/2871 evaluated.\n",
      "Row 1738/2871 evaluated.\n",
      "Row 1739/2871 evaluated.\n",
      "Row 1740/2871 evaluated.\n",
      "Row 1741/2871 evaluated.\n",
      "Row 1742/2871 evaluated.\n",
      "Row 1743/2871 evaluated.\n",
      "Row 1744/2871 evaluated.\n",
      "Row 1745/2871 evaluated.\n",
      "Row 1746/2871 evaluated.\n",
      "Row 1747/2871 evaluated.\n",
      "Row 1748/2871 evaluated.\n",
      "Row 1749/2871 evaluated.\n",
      "Row 1750/2871 evaluated.\n",
      "Row 1751/2871 evaluated.\n",
      "Row 1752/2871 evaluated.\n",
      "Row 1753/2871 evaluated.\n",
      "Row 1754/2871 evaluated.\n",
      "Row 1755/2871 evaluated.\n",
      "Row 1756/2871 evaluated.\n",
      "Row 1757/2871 evaluated.\n",
      "Row 1758/2871 evaluated.\n",
      "Row 1759/2871 evaluated.\n",
      "Row 1760/2871 evaluated.\n",
      "Row 1761/2871 evaluated.\n",
      "Row 1762/2871 evaluated.\n",
      "Row 1763/2871 evaluated.\n",
      "Row 1764/2871 evaluated.\n",
      "Row 1765/2871 evaluated.\n",
      "Row 1766/2871 evaluated.\n",
      "Row 1767/2871 evaluated.\n",
      "Row 1768/2871 evaluated.\n",
      "Row 1769/2871 evaluated.\n",
      "Row 1770/2871 evaluated.\n",
      "Row 1771/2871 evaluated.\n",
      "Row 1772/2871 evaluated.\n",
      "Row 1773/2871 evaluated.\n",
      "Row 1774/2871 evaluated.\n",
      "Row 1775/2871 evaluated.\n",
      "Row 1776/2871 evaluated.\n",
      "Row 1777/2871 evaluated.\n",
      "Row 1778/2871 evaluated.\n",
      "Row 1779/2871 evaluated.\n",
      "Row 1780/2871 evaluated.\n",
      "Row 1781/2871 evaluated.\n",
      "Row 1782/2871 evaluated.\n",
      "Row 1783/2871 evaluated.\n",
      "Row 1784/2871 evaluated.\n",
      "Row 1785/2871 evaluated.\n",
      "Row 1786/2871 evaluated.\n",
      "Row 1787/2871 evaluated.\n",
      "Row 1788/2871 evaluated.\n",
      "Row 1789/2871 evaluated.\n",
      "Row 1790/2871 evaluated.\n",
      "Row 1791/2871 evaluated.\n",
      "Row 1792/2871 evaluated.\n",
      "Row 1793/2871 evaluated.\n",
      "Row 1794/2871 evaluated.\n",
      "Row 1795/2871 evaluated.\n",
      "Row 1796/2871 evaluated.\n",
      "Row 1797/2871 evaluated.\n",
      "Row 1798/2871 evaluated.\n",
      "Row 1799/2871 evaluated.\n",
      "Row 1800/2871 evaluated.\n",
      "Row 1801/2871 evaluated.\n",
      "Row 1802/2871 evaluated.\n",
      "Row 1803/2871 evaluated.\n",
      "Row 1804/2871 evaluated.\n",
      "Row 1805/2871 evaluated.\n",
      "Row 1806/2871 evaluated.\n",
      "Row 1807/2871 evaluated.\n",
      "Row 1808/2871 evaluated.\n",
      "Row 1809/2871 evaluated.\n",
      "Row 1810/2871 evaluated.\n",
      "Row 1811/2871 evaluated.\n",
      "Row 1812/2871 evaluated.\n",
      "Row 1813/2871 evaluated.\n",
      "Row 1814/2871 evaluated.\n",
      "Row 1815/2871 evaluated.\n",
      "Row 1816/2871 evaluated.\n",
      "Row 1817/2871 evaluated.\n",
      "Row 1818/2871 evaluated.\n",
      "Row 1819/2871 evaluated.\n",
      "Row 1820/2871 evaluated.\n",
      "Row 1821/2871 evaluated.\n",
      "Row 1822/2871 evaluated.\n",
      "Row 1823/2871 evaluated.\n",
      "Row 1824/2871 evaluated.\n",
      "Row 1825/2871 evaluated.\n",
      "Row 1826/2871 evaluated.\n",
      "Row 1827/2871 evaluated.\n",
      "Row 1828/2871 evaluated.\n",
      "Row 1829/2871 evaluated.\n",
      "Row 1830/2871 evaluated.\n",
      "Row 1831/2871 evaluated.\n",
      "Row 1832/2871 evaluated.\n",
      "Row 1833/2871 evaluated.\n",
      "Row 1834/2871 evaluated.\n",
      "Row 1835/2871 evaluated.\n",
      "Row 1836/2871 evaluated.\n",
      "Row 1837/2871 evaluated.\n",
      "Skipping row 1838: Empty or invalid text.\n",
      "Row 1839/2871 evaluated.\n",
      "Row 1840/2871 evaluated.\n",
      "Row 1841/2871 evaluated.\n",
      "Row 1842/2871 evaluated.\n",
      "Row 1843/2871 evaluated.\n",
      "Row 1844/2871 evaluated.\n",
      "Row 1845/2871 evaluated.\n",
      "Row 1846/2871 evaluated.\n",
      "Row 1847/2871 evaluated.\n",
      "Row 1848/2871 evaluated.\n",
      "Row 1849/2871 evaluated.\n",
      "Row 1850/2871 evaluated.\n",
      "Row 1851/2871 evaluated.\n",
      "Row 1852/2871 evaluated.\n",
      "Row 1853/2871 evaluated.\n",
      "Row 1854/2871 evaluated.\n",
      "Row 1855/2871 evaluated.\n",
      "Row 1856/2871 evaluated.\n",
      "Row 1857/2871 evaluated.\n",
      "Row 1858/2871 evaluated.\n",
      "Row 1859/2871 evaluated.\n",
      "Row 1860/2871 evaluated.\n",
      "Row 1861/2871 evaluated.\n",
      "Row 1862/2871 evaluated.\n",
      "Row 1863/2871 evaluated.\n",
      "Row 1864/2871 evaluated.\n",
      "Row 1865/2871 evaluated.\n",
      "Row 1866/2871 evaluated.\n",
      "Row 1867/2871 evaluated.\n",
      "Row 1868/2871 evaluated.\n",
      "Row 1869/2871 evaluated.\n",
      "Row 1870/2871 evaluated.\n",
      "Row 1871/2871 evaluated.\n",
      "Row 1872/2871 evaluated.\n",
      "Row 1873/2871 evaluated.\n",
      "Row 1874/2871 evaluated.\n",
      "Row 1875/2871 evaluated.\n",
      "Row 1876/2871 evaluated.\n",
      "Row 1877/2871 evaluated.\n",
      "Row 1878/2871 evaluated.\n",
      "Row 1879/2871 evaluated.\n",
      "Row 1880/2871 evaluated.\n",
      "Row 1881/2871 evaluated.\n",
      "Row 1882/2871 evaluated.\n",
      "Row 1883/2871 evaluated.\n",
      "Row 1884/2871 evaluated.\n",
      "Row 1885/2871 evaluated.\n",
      "Row 1886/2871 evaluated.\n",
      "Row 1887/2871 evaluated.\n",
      "Row 1888/2871 evaluated.\n",
      "Row 1889/2871 evaluated.\n",
      "Row 1890/2871 evaluated.\n",
      "Row 1891/2871 evaluated.\n",
      "Row 1892/2871 evaluated.\n",
      "Row 1893/2871 evaluated.\n",
      "Row 1894/2871 evaluated.\n",
      "Row 1895/2871 evaluated.\n",
      "Row 1896/2871 evaluated.\n",
      "Row 1897/2871 evaluated.\n",
      "Row 1898/2871 evaluated.\n",
      "Row 1899/2871 evaluated.\n",
      "Row 1900/2871 evaluated.\n",
      "Row 1901/2871 evaluated.\n",
      "Row 1902/2871 evaluated.\n",
      "Row 1903/2871 evaluated.\n",
      "Row 1904/2871 evaluated.\n",
      "Row 1905/2871 evaluated.\n",
      "Row 1906/2871 evaluated.\n",
      "Row 1907/2871 evaluated.\n",
      "Row 1908/2871 evaluated.\n",
      "Row 1909/2871 evaluated.\n",
      "Row 1910/2871 evaluated.\n",
      "Row 1911/2871 evaluated.\n",
      "Row 1912/2871 evaluated.\n",
      "Row 1913/2871 evaluated.\n",
      "Row 1914/2871 evaluated.\n",
      "Row 1915/2871 evaluated.\n",
      "Row 1916/2871 evaluated.\n",
      "Row 1917/2871 evaluated.\n",
      "Row 1918/2871 evaluated.\n",
      "Row 1919/2871 evaluated.\n",
      "Row 1920/2871 evaluated.\n",
      "Row 1921/2871 evaluated.\n",
      "Row 1922/2871 evaluated.\n",
      "Row 1923/2871 evaluated.\n",
      "Row 1924/2871 evaluated.\n",
      "Row 1925/2871 evaluated.\n",
      "Row 1926/2871 evaluated.\n",
      "Row 1927/2871 evaluated.\n",
      "Row 1928/2871 evaluated.\n",
      "Row 1929/2871 evaluated.\n",
      "Row 1930/2871 evaluated.\n",
      "Row 1931/2871 evaluated.\n",
      "Row 1932/2871 evaluated.\n",
      "Row 1933/2871 evaluated.\n",
      "Row 1934/2871 evaluated.\n",
      "Row 1935/2871 evaluated.\n",
      "Row 1936/2871 evaluated.\n",
      "Row 1937/2871 evaluated.\n",
      "Row 1938/2871 evaluated.\n",
      "Row 1939/2871 evaluated.\n",
      "Row 1940/2871 evaluated.\n",
      "Row 1941/2871 evaluated.\n",
      "Row 1942/2871 evaluated.\n",
      "Row 1943/2871 evaluated.\n",
      "Row 1944/2871 evaluated.\n",
      "Row 1945/2871 evaluated.\n",
      "Row 1946/2871 evaluated.\n",
      "Row 1947/2871 evaluated.\n",
      "Row 1948/2871 evaluated.\n",
      "Row 1949/2871 evaluated.\n",
      "Row 1950/2871 evaluated.\n",
      "Row 1951/2871 evaluated.\n",
      "Row 1952/2871 evaluated.\n",
      "Row 1953/2871 evaluated.\n",
      "Row 1954/2871 evaluated.\n",
      "Row 1955/2871 evaluated.\n",
      "Row 1956/2871 evaluated.\n",
      "Row 1957/2871 evaluated.\n",
      "Row 1958/2871 evaluated.\n",
      "Row 1959/2871 evaluated.\n",
      "Row 1960/2871 evaluated.\n",
      "Row 1961/2871 evaluated.\n",
      "Row 1962/2871 evaluated.\n",
      "Row 1963/2871 evaluated.\n",
      "Row 1964/2871 evaluated.\n",
      "Row 1965/2871 evaluated.\n",
      "Row 1966/2871 evaluated.\n",
      "Row 1967/2871 evaluated.\n",
      "Row 1968/2871 evaluated.\n",
      "Row 1969/2871 evaluated.\n",
      "Row 1970/2871 evaluated.\n",
      "Row 1971/2871 evaluated.\n",
      "Row 1972/2871 evaluated.\n",
      "Row 1973/2871 evaluated.\n",
      "Row 1974/2871 evaluated.\n",
      "Row 1975/2871 evaluated.\n",
      "Row 1976/2871 evaluated.\n",
      "Row 1977/2871 evaluated.\n",
      "Row 1978/2871 evaluated.\n",
      "Row 1979/2871 evaluated.\n",
      "Row 1980/2871 evaluated.\n",
      "Row 1981/2871 evaluated.\n",
      "Row 1982/2871 evaluated.\n",
      "Row 1983/2871 evaluated.\n",
      "Row 1984/2871 evaluated.\n",
      "Row 1985/2871 evaluated.\n",
      "Row 1986/2871 evaluated.\n",
      "Row 1987/2871 evaluated.\n",
      "Row 1988/2871 evaluated.\n",
      "Row 1989/2871 evaluated.\n",
      "Row 1990/2871 evaluated.\n",
      "Row 1991/2871 evaluated.\n",
      "Row 1992/2871 evaluated.\n",
      "Row 1993/2871 evaluated.\n",
      "Row 1994/2871 evaluated.\n",
      "Row 1995/2871 evaluated.\n",
      "Row 1996/2871 evaluated.\n",
      "Row 1997/2871 evaluated.\n",
      "Row 1998/2871 evaluated.\n",
      "Row 1999/2871 evaluated.\n",
      "Row 2000/2871 evaluated.\n",
      "Row 2001/2871 evaluated.\n",
      "Row 2002/2871 evaluated.\n",
      "Row 2003/2871 evaluated.\n",
      "Row 2004/2871 evaluated.\n",
      "Row 2005/2871 evaluated.\n",
      "Row 2006/2871 evaluated.\n",
      "Row 2007/2871 evaluated.\n",
      "Row 2008/2871 evaluated.\n",
      "Row 2009/2871 evaluated.\n",
      "Row 2010/2871 evaluated.\n",
      "Row 2011/2871 evaluated.\n",
      "Row 2012/2871 evaluated.\n",
      "Row 2013/2871 evaluated.\n",
      "Row 2014/2871 evaluated.\n",
      "Row 2015/2871 evaluated.\n",
      "Row 2016/2871 evaluated.\n",
      "Row 2017/2871 evaluated.\n",
      "Row 2018/2871 evaluated.\n",
      "Row 2019/2871 evaluated.\n",
      "Row 2020/2871 evaluated.\n",
      "Row 2021/2871 evaluated.\n",
      "Row 2022/2871 evaluated.\n",
      "Row 2023/2871 evaluated.\n",
      "Row 2024/2871 evaluated.\n",
      "Row 2025/2871 evaluated.\n",
      "Row 2026/2871 evaluated.\n",
      "Row 2027/2871 evaluated.\n",
      "Row 2028/2871 evaluated.\n",
      "Row 2029/2871 evaluated.\n",
      "Row 2030/2871 evaluated.\n",
      "Row 2031/2871 evaluated.\n",
      "Row 2032/2871 evaluated.\n",
      "Row 2033/2871 evaluated.\n",
      "Row 2034/2871 evaluated.\n",
      "Row 2035/2871 evaluated.\n",
      "Row 2036/2871 evaluated.\n",
      "Row 2037/2871 evaluated.\n",
      "Row 2038/2871 evaluated.\n",
      "Row 2039/2871 evaluated.\n",
      "Row 2040/2871 evaluated.\n",
      "Row 2041/2871 evaluated.\n",
      "Row 2042/2871 evaluated.\n",
      "Row 2043/2871 evaluated.\n",
      "Row 2044/2871 evaluated.\n",
      "Row 2045/2871 evaluated.\n",
      "Row 2046/2871 evaluated.\n",
      "Row 2047/2871 evaluated.\n",
      "Row 2048/2871 evaluated.\n",
      "Row 2049/2871 evaluated.\n",
      "Row 2050/2871 evaluated.\n",
      "Row 2051/2871 evaluated.\n",
      "Row 2052/2871 evaluated.\n",
      "Row 2053/2871 evaluated.\n",
      "Row 2054/2871 evaluated.\n",
      "Row 2055/2871 evaluated.\n",
      "Row 2056/2871 evaluated.\n",
      "Row 2057/2871 evaluated.\n",
      "Row 2058/2871 evaluated.\n",
      "Row 2059/2871 evaluated.\n",
      "Row 2060/2871 evaluated.\n",
      "Row 2061/2871 evaluated.\n",
      "Row 2062/2871 evaluated.\n",
      "Row 2063/2871 evaluated.\n",
      "Row 2064/2871 evaluated.\n",
      "Row 2065/2871 evaluated.\n",
      "Row 2066/2871 evaluated.\n",
      "Row 2067/2871 evaluated.\n",
      "Row 2068/2871 evaluated.\n",
      "Row 2069/2871 evaluated.\n",
      "Row 2070/2871 evaluated.\n",
      "Row 2071/2871 evaluated.\n",
      "Row 2072/2871 evaluated.\n",
      "Row 2073/2871 evaluated.\n",
      "Row 2074/2871 evaluated.\n",
      "Row 2075/2871 evaluated.\n",
      "Row 2076/2871 evaluated.\n",
      "Row 2077/2871 evaluated.\n",
      "Row 2078/2871 evaluated.\n",
      "Row 2079/2871 evaluated.\n",
      "Row 2080/2871 evaluated.\n",
      "Row 2081/2871 evaluated.\n",
      "Row 2082/2871 evaluated.\n",
      "Row 2083/2871 evaluated.\n",
      "Row 2084/2871 evaluated.\n",
      "Row 2085/2871 evaluated.\n",
      "Row 2086/2871 evaluated.\n",
      "Row 2087/2871 evaluated.\n",
      "Row 2088/2871 evaluated.\n",
      "Row 2089/2871 evaluated.\n",
      "Row 2090/2871 evaluated.\n",
      "Row 2091/2871 evaluated.\n",
      "Row 2092/2871 evaluated.\n",
      "Row 2093/2871 evaluated.\n",
      "Row 2094/2871 evaluated.\n",
      "Row 2095/2871 evaluated.\n",
      "Row 2096/2871 evaluated.\n",
      "Row 2097/2871 evaluated.\n",
      "Skipping row 2098: Empty or invalid text.\n",
      "Row 2099/2871 evaluated.\n",
      "Row 2100/2871 evaluated.\n",
      "Row 2101/2871 evaluated.\n",
      "Row 2102/2871 evaluated.\n",
      "Row 2103/2871 evaluated.\n",
      "Row 2104/2871 evaluated.\n",
      "Row 2105/2871 evaluated.\n",
      "Row 2106/2871 evaluated.\n",
      "Row 2107/2871 evaluated.\n",
      "Row 2108/2871 evaluated.\n",
      "Skipping row 2109: Empty or invalid text.\n",
      "Row 2110/2871 evaluated.\n",
      "Row 2111/2871 evaluated.\n",
      "Row 2112/2871 evaluated.\n",
      "Row 2113/2871 evaluated.\n",
      "Row 2114/2871 evaluated.\n",
      "Row 2115/2871 evaluated.\n",
      "Row 2116/2871 evaluated.\n",
      "Row 2117/2871 evaluated.\n",
      "Row 2118/2871 evaluated.\n",
      "Row 2119/2871 evaluated.\n",
      "Row 2120/2871 evaluated.\n",
      "Row 2121/2871 evaluated.\n",
      "Row 2122/2871 evaluated.\n",
      "Row 2123/2871 evaluated.\n",
      "Row 2124/2871 evaluated.\n",
      "Row 2125/2871 evaluated.\n",
      "Row 2126/2871 evaluated.\n",
      "Row 2127/2871 evaluated.\n",
      "Row 2128/2871 evaluated.\n",
      "Row 2129/2871 evaluated.\n",
      "Row 2130/2871 evaluated.\n",
      "Row 2131/2871 evaluated.\n",
      "Row 2132/2871 evaluated.\n",
      "Row 2133/2871 evaluated.\n",
      "Row 2134/2871 evaluated.\n",
      "Row 2135/2871 evaluated.\n",
      "Row 2136/2871 evaluated.\n",
      "Row 2137/2871 evaluated.\n",
      "Row 2138/2871 evaluated.\n",
      "Row 2139/2871 evaluated.\n",
      "Row 2140/2871 evaluated.\n",
      "Row 2141/2871 evaluated.\n",
      "Row 2142/2871 evaluated.\n",
      "Row 2143/2871 evaluated.\n",
      "Row 2144/2871 evaluated.\n",
      "Row 2145/2871 evaluated.\n",
      "Row 2146/2871 evaluated.\n",
      "Row 2147/2871 evaluated.\n",
      "Row 2148/2871 evaluated.\n",
      "Row 2149/2871 evaluated.\n",
      "Row 2150/2871 evaluated.\n",
      "Row 2151/2871 evaluated.\n",
      "Row 2152/2871 evaluated.\n",
      "Row 2153/2871 evaluated.\n",
      "Row 2154/2871 evaluated.\n",
      "Row 2155/2871 evaluated.\n",
      "Row 2156/2871 evaluated.\n",
      "Row 2157/2871 evaluated.\n",
      "Row 2158/2871 evaluated.\n",
      "Row 2159/2871 evaluated.\n",
      "Row 2160/2871 evaluated.\n",
      "Row 2161/2871 evaluated.\n",
      "Row 2162/2871 evaluated.\n",
      "Row 2163/2871 evaluated.\n",
      "Row 2164/2871 evaluated.\n",
      "Row 2165/2871 evaluated.\n",
      "Row 2166/2871 evaluated.\n",
      "Row 2167/2871 evaluated.\n",
      "Row 2168/2871 evaluated.\n",
      "Row 2169/2871 evaluated.\n",
      "Row 2170/2871 evaluated.\n",
      "Row 2171/2871 evaluated.\n",
      "Row 2172/2871 evaluated.\n",
      "Row 2173/2871 evaluated.\n",
      "Row 2174/2871 evaluated.\n",
      "Row 2175/2871 evaluated.\n",
      "Row 2176/2871 evaluated.\n",
      "Row 2177/2871 evaluated.\n",
      "Row 2178/2871 evaluated.\n",
      "Row 2179/2871 evaluated.\n",
      "Row 2180/2871 evaluated.\n",
      "Row 2181/2871 evaluated.\n",
      "Row 2182/2871 evaluated.\n",
      "Row 2183/2871 evaluated.\n",
      "Row 2184/2871 evaluated.\n",
      "Row 2185/2871 evaluated.\n",
      "Row 2186/2871 evaluated.\n",
      "Row 2187/2871 evaluated.\n",
      "Row 2188/2871 evaluated.\n",
      "Row 2189/2871 evaluated.\n",
      "Row 2190/2871 evaluated.\n",
      "Row 2191/2871 evaluated.\n",
      "Row 2192/2871 evaluated.\n",
      "Row 2193/2871 evaluated.\n",
      "Row 2194/2871 evaluated.\n",
      "Row 2195/2871 evaluated.\n",
      "Row 2196/2871 evaluated.\n",
      "Row 2197/2871 evaluated.\n",
      "Row 2198/2871 evaluated.\n",
      "Row 2199/2871 evaluated.\n",
      "Row 2200/2871 evaluated.\n",
      "Row 2201/2871 evaluated.\n",
      "Row 2202/2871 evaluated.\n",
      "Row 2203/2871 evaluated.\n",
      "Row 2204/2871 evaluated.\n",
      "Row 2205/2871 evaluated.\n",
      "Row 2206/2871 evaluated.\n",
      "Row 2207/2871 evaluated.\n",
      "Row 2208/2871 evaluated.\n",
      "Row 2209/2871 evaluated.\n",
      "Row 2210/2871 evaluated.\n",
      "Row 2211/2871 evaluated.\n",
      "Row 2212/2871 evaluated.\n",
      "Row 2213/2871 evaluated.\n",
      "Row 2214/2871 evaluated.\n",
      "Row 2215/2871 evaluated.\n",
      "Row 2216/2871 evaluated.\n",
      "Row 2217/2871 evaluated.\n",
      "Row 2218/2871 evaluated.\n",
      "Row 2219/2871 evaluated.\n",
      "Row 2220/2871 evaluated.\n",
      "Row 2221/2871 evaluated.\n",
      "Row 2222/2871 evaluated.\n",
      "Row 2223/2871 evaluated.\n",
      "Row 2224/2871 evaluated.\n",
      "Row 2225/2871 evaluated.\n",
      "Row 2226/2871 evaluated.\n",
      "Row 2227/2871 evaluated.\n",
      "Row 2228/2871 evaluated.\n",
      "Row 2229/2871 evaluated.\n",
      "Row 2230/2871 evaluated.\n",
      "Row 2231/2871 evaluated.\n",
      "Row 2232/2871 evaluated.\n",
      "Row 2233/2871 evaluated.\n",
      "Row 2234/2871 evaluated.\n",
      "Row 2235/2871 evaluated.\n",
      "Row 2236/2871 evaluated.\n",
      "Row 2237/2871 evaluated.\n",
      "Row 2238/2871 evaluated.\n",
      "Row 2239/2871 evaluated.\n",
      "Row 2240/2871 evaluated.\n",
      "Row 2241/2871 evaluated.\n",
      "Row 2242/2871 evaluated.\n",
      "Row 2243/2871 evaluated.\n",
      "Row 2244/2871 evaluated.\n",
      "Row 2245/2871 evaluated.\n",
      "Row 2246/2871 evaluated.\n",
      "Row 2247/2871 evaluated.\n",
      "Row 2248/2871 evaluated.\n",
      "Row 2249/2871 evaluated.\n",
      "Row 2250/2871 evaluated.\n",
      "Row 2251/2871 evaluated.\n",
      "Row 2252/2871 evaluated.\n",
      "Row 2253/2871 evaluated.\n",
      "Row 2254/2871 evaluated.\n",
      "Row 2255/2871 evaluated.\n",
      "Row 2256/2871 evaluated.\n",
      "Row 2257/2871 evaluated.\n",
      "Row 2258/2871 evaluated.\n",
      "Row 2259/2871 evaluated.\n",
      "Row 2260/2871 evaluated.\n",
      "Row 2261/2871 evaluated.\n",
      "Row 2262/2871 evaluated.\n",
      "Row 2263/2871 evaluated.\n",
      "Row 2264/2871 evaluated.\n",
      "Row 2265/2871 evaluated.\n",
      "Row 2266/2871 evaluated.\n",
      "Row 2267/2871 evaluated.\n",
      "Row 2268/2871 evaluated.\n",
      "Row 2269/2871 evaluated.\n",
      "Row 2270/2871 evaluated.\n",
      "Row 2271/2871 evaluated.\n",
      "Row 2272/2871 evaluated.\n",
      "Row 2273/2871 evaluated.\n",
      "Row 2274/2871 evaluated.\n",
      "Row 2275/2871 evaluated.\n",
      "Row 2276/2871 evaluated.\n",
      "Row 2277/2871 evaluated.\n",
      "Row 2278/2871 evaluated.\n",
      "Row 2279/2871 evaluated.\n",
      "Row 2280/2871 evaluated.\n",
      "Row 2281/2871 evaluated.\n",
      "Row 2282/2871 evaluated.\n",
      "Row 2283/2871 evaluated.\n",
      "Row 2284/2871 evaluated.\n",
      "Row 2285/2871 evaluated.\n",
      "Row 2286/2871 evaluated.\n",
      "Row 2287/2871 evaluated.\n",
      "Row 2288/2871 evaluated.\n",
      "Row 2289/2871 evaluated.\n",
      "Row 2290/2871 evaluated.\n",
      "Row 2291/2871 evaluated.\n",
      "Row 2292/2871 evaluated.\n",
      "Row 2293/2871 evaluated.\n",
      "Row 2294/2871 evaluated.\n",
      "Row 2295/2871 evaluated.\n",
      "Row 2296/2871 evaluated.\n",
      "Row 2297/2871 evaluated.\n",
      "Row 2298/2871 evaluated.\n",
      "Row 2299/2871 evaluated.\n",
      "Row 2300/2871 evaluated.\n",
      "Row 2301/2871 evaluated.\n",
      "Row 2302/2871 evaluated.\n",
      "Row 2303/2871 evaluated.\n",
      "Row 2304/2871 evaluated.\n",
      "Row 2305/2871 evaluated.\n",
      "Row 2306/2871 evaluated.\n",
      "Row 2307/2871 evaluated.\n",
      "Row 2308/2871 evaluated.\n",
      "Row 2309/2871 evaluated.\n",
      "Row 2310/2871 evaluated.\n",
      "Row 2311/2871 evaluated.\n",
      "Row 2312/2871 evaluated.\n",
      "Row 2313/2871 evaluated.\n",
      "Row 2314/2871 evaluated.\n",
      "Row 2315/2871 evaluated.\n",
      "Row 2316/2871 evaluated.\n",
      "Row 2317/2871 evaluated.\n",
      "Row 2318/2871 evaluated.\n",
      "Row 2319/2871 evaluated.\n",
      "Row 2320/2871 evaluated.\n",
      "Row 2321/2871 evaluated.\n",
      "Row 2322/2871 evaluated.\n",
      "Row 2323/2871 evaluated.\n",
      "Row 2324/2871 evaluated.\n",
      "Row 2325/2871 evaluated.\n",
      "Row 2326/2871 evaluated.\n",
      "Row 2327/2871 evaluated.\n",
      "Row 2328/2871 evaluated.\n",
      "Row 2329/2871 evaluated.\n",
      "Row 2330/2871 evaluated.\n",
      "Row 2331/2871 evaluated.\n",
      "Row 2332/2871 evaluated.\n",
      "Row 2333/2871 evaluated.\n",
      "Row 2334/2871 evaluated.\n",
      "Row 2335/2871 evaluated.\n",
      "Row 2336/2871 evaluated.\n",
      "Row 2337/2871 evaluated.\n",
      "Row 2338/2871 evaluated.\n",
      "Row 2339/2871 evaluated.\n",
      "Row 2340/2871 evaluated.\n",
      "Row 2341/2871 evaluated.\n",
      "Row 2342/2871 evaluated.\n",
      "Row 2343/2871 evaluated.\n",
      "Row 2344/2871 evaluated.\n",
      "Row 2345/2871 evaluated.\n",
      "Row 2346/2871 evaluated.\n",
      "Row 2347/2871 evaluated.\n",
      "Row 2348/2871 evaluated.\n",
      "Row 2349/2871 evaluated.\n",
      "Row 2350/2871 evaluated.\n",
      "Row 2351/2871 evaluated.\n",
      "Row 2352/2871 evaluated.\n",
      "Row 2353/2871 evaluated.\n",
      "Row 2354/2871 evaluated.\n",
      "Row 2355/2871 evaluated.\n",
      "Row 2356/2871 evaluated.\n",
      "Row 2357/2871 evaluated.\n",
      "Row 2358/2871 evaluated.\n",
      "Row 2359/2871 evaluated.\n",
      "Row 2360/2871 evaluated.\n",
      "Row 2361/2871 evaluated.\n",
      "Row 2362/2871 evaluated.\n",
      "Row 2363/2871 evaluated.\n",
      "Row 2364/2871 evaluated.\n",
      "Row 2365/2871 evaluated.\n",
      "Row 2366/2871 evaluated.\n",
      "Row 2367/2871 evaluated.\n",
      "Row 2368/2871 evaluated.\n",
      "Row 2369/2871 evaluated.\n",
      "Row 2370/2871 evaluated.\n",
      "Row 2371/2871 evaluated.\n",
      "Row 2372/2871 evaluated.\n",
      "Row 2373/2871 evaluated.\n",
      "Row 2374/2871 evaluated.\n",
      "Row 2375/2871 evaluated.\n",
      "Row 2376/2871 evaluated.\n",
      "Row 2377/2871 evaluated.\n",
      "Row 2378/2871 evaluated.\n",
      "Row 2379/2871 evaluated.\n",
      "Row 2380/2871 evaluated.\n",
      "Row 2381/2871 evaluated.\n",
      "Row 2382/2871 evaluated.\n",
      "Row 2383/2871 evaluated.\n",
      "Row 2384/2871 evaluated.\n",
      "Row 2385/2871 evaluated.\n",
      "Row 2386/2871 evaluated.\n",
      "Row 2387/2871 evaluated.\n",
      "Row 2388/2871 evaluated.\n",
      "Row 2389/2871 evaluated.\n",
      "Row 2390/2871 evaluated.\n",
      "Row 2391/2871 evaluated.\n",
      "Row 2392/2871 evaluated.\n",
      "Row 2393/2871 evaluated.\n",
      "Row 2394/2871 evaluated.\n",
      "Row 2395/2871 evaluated.\n",
      "Row 2396/2871 evaluated.\n",
      "Row 2397/2871 evaluated.\n",
      "Row 2398/2871 evaluated.\n",
      "Row 2399/2871 evaluated.\n",
      "Row 2400/2871 evaluated.\n",
      "Row 2401/2871 evaluated.\n",
      "Row 2402/2871 evaluated.\n",
      "Row 2403/2871 evaluated.\n",
      "Row 2404/2871 evaluated.\n",
      "Row 2405/2871 evaluated.\n",
      "Row 2406/2871 evaluated.\n",
      "Row 2407/2871 evaluated.\n",
      "Row 2408/2871 evaluated.\n",
      "Row 2409/2871 evaluated.\n",
      "Row 2410/2871 evaluated.\n",
      "Row 2411/2871 evaluated.\n",
      "Row 2412/2871 evaluated.\n",
      "Row 2413/2871 evaluated.\n",
      "Row 2414/2871 evaluated.\n",
      "Row 2415/2871 evaluated.\n",
      "Row 2416/2871 evaluated.\n",
      "Row 2417/2871 evaluated.\n",
      "Row 2418/2871 evaluated.\n",
      "Row 2419/2871 evaluated.\n",
      "Row 2420/2871 evaluated.\n",
      "Row 2421/2871 evaluated.\n",
      "Row 2422/2871 evaluated.\n",
      "Row 2423/2871 evaluated.\n",
      "Row 2424/2871 evaluated.\n",
      "Row 2425/2871 evaluated.\n",
      "Row 2426/2871 evaluated.\n",
      "Row 2427/2871 evaluated.\n",
      "Row 2428/2871 evaluated.\n",
      "Row 2429/2871 evaluated.\n",
      "Row 2430/2871 evaluated.\n",
      "Row 2431/2871 evaluated.\n",
      "Row 2432/2871 evaluated.\n",
      "Row 2433/2871 evaluated.\n",
      "Row 2434/2871 evaluated.\n",
      "Row 2435/2871 evaluated.\n",
      "Row 2436/2871 evaluated.\n",
      "Row 2437/2871 evaluated.\n",
      "Row 2438/2871 evaluated.\n",
      "Row 2439/2871 evaluated.\n",
      "Row 2440/2871 evaluated.\n",
      "Row 2441/2871 evaluated.\n",
      "Row 2442/2871 evaluated.\n",
      "Row 2443/2871 evaluated.\n",
      "Row 2444/2871 evaluated.\n",
      "Row 2445/2871 evaluated.\n",
      "Row 2446/2871 evaluated.\n",
      "Row 2447/2871 evaluated.\n",
      "Row 2448/2871 evaluated.\n",
      "Row 2449/2871 evaluated.\n",
      "Row 2450/2871 evaluated.\n",
      "Row 2451/2871 evaluated.\n",
      "Row 2452/2871 evaluated.\n",
      "Row 2453/2871 evaluated.\n",
      "Row 2454/2871 evaluated.\n",
      "Row 2455/2871 evaluated.\n",
      "Row 2456/2871 evaluated.\n",
      "Row 2457/2871 evaluated.\n",
      "Row 2458/2871 evaluated.\n",
      "Row 2459/2871 evaluated.\n",
      "Row 2460/2871 evaluated.\n",
      "Row 2461/2871 evaluated.\n",
      "Row 2462/2871 evaluated.\n",
      "Row 2463/2871 evaluated.\n",
      "Row 2464/2871 evaluated.\n",
      "Row 2465/2871 evaluated.\n",
      "Row 2466/2871 evaluated.\n",
      "Row 2467/2871 evaluated.\n",
      "Row 2468/2871 evaluated.\n",
      "Row 2469/2871 evaluated.\n",
      "Row 2470/2871 evaluated.\n",
      "Row 2471/2871 evaluated.\n",
      "Row 2472/2871 evaluated.\n",
      "Row 2473/2871 evaluated.\n",
      "Row 2474/2871 evaluated.\n",
      "Row 2475/2871 evaluated.\n",
      "Row 2476/2871 evaluated.\n",
      "Row 2477/2871 evaluated.\n",
      "Row 2478/2871 evaluated.\n",
      "Row 2479/2871 evaluated.\n",
      "Row 2480/2871 evaluated.\n",
      "Row 2481/2871 evaluated.\n",
      "Row 2482/2871 evaluated.\n",
      "Row 2483/2871 evaluated.\n",
      "Row 2484/2871 evaluated.\n",
      "Row 2485/2871 evaluated.\n",
      "Row 2486/2871 evaluated.\n",
      "Row 2487/2871 evaluated.\n",
      "Row 2488/2871 evaluated.\n",
      "Row 2489/2871 evaluated.\n",
      "Row 2490/2871 evaluated.\n",
      "Skipping row 2491: Empty or invalid text.\n",
      "Row 2492/2871 evaluated.\n",
      "Row 2493/2871 evaluated.\n",
      "Row 2494/2871 evaluated.\n",
      "Row 2495/2871 evaluated.\n",
      "Row 2496/2871 evaluated.\n",
      "Row 2497/2871 evaluated.\n",
      "Row 2498/2871 evaluated.\n",
      "Row 2499/2871 evaluated.\n",
      "Row 2500/2871 evaluated.\n",
      "Row 2501/2871 evaluated.\n",
      "Row 2502/2871 evaluated.\n",
      "Row 2503/2871 evaluated.\n",
      "Row 2504/2871 evaluated.\n",
      "Row 2505/2871 evaluated.\n",
      "Row 2506/2871 evaluated.\n",
      "Row 2507/2871 evaluated.\n",
      "Row 2508/2871 evaluated.\n",
      "Row 2509/2871 evaluated.\n",
      "Row 2510/2871 evaluated.\n",
      "Row 2511/2871 evaluated.\n",
      "Row 2512/2871 evaluated.\n",
      "Row 2513/2871 evaluated.\n",
      "Row 2514/2871 evaluated.\n",
      "Row 2515/2871 evaluated.\n",
      "Row 2516/2871 evaluated.\n",
      "Row 2517/2871 evaluated.\n",
      "Row 2518/2871 evaluated.\n",
      "Row 2519/2871 evaluated.\n",
      "Row 2520/2871 evaluated.\n",
      "Row 2521/2871 evaluated.\n",
      "Row 2522/2871 evaluated.\n",
      "Row 2523/2871 evaluated.\n",
      "Row 2524/2871 evaluated.\n",
      "Row 2525/2871 evaluated.\n",
      "Row 2526/2871 evaluated.\n",
      "Row 2527/2871 evaluated.\n",
      "Row 2528/2871 evaluated.\n",
      "Row 2529/2871 evaluated.\n",
      "Row 2530/2871 evaluated.\n",
      "Row 2531/2871 evaluated.\n",
      "Row 2532/2871 evaluated.\n",
      "Row 2533/2871 evaluated.\n",
      "Row 2534/2871 evaluated.\n",
      "Row 2535/2871 evaluated.\n",
      "Row 2536/2871 evaluated.\n",
      "Row 2537/2871 evaluated.\n",
      "Row 2538/2871 evaluated.\n",
      "Row 2539/2871 evaluated.\n",
      "Row 2540/2871 evaluated.\n",
      "Row 2541/2871 evaluated.\n",
      "Row 2542/2871 evaluated.\n",
      "Row 2543/2871 evaluated.\n",
      "Row 2544/2871 evaluated.\n",
      "Row 2545/2871 evaluated.\n",
      "Row 2546/2871 evaluated.\n",
      "Row 2547/2871 evaluated.\n",
      "Row 2548/2871 evaluated.\n",
      "Row 2549/2871 evaluated.\n",
      "Row 2550/2871 evaluated.\n",
      "Row 2551/2871 evaluated.\n",
      "Row 2552/2871 evaluated.\n",
      "Row 2553/2871 evaluated.\n",
      "Row 2554/2871 evaluated.\n",
      "Row 2555/2871 evaluated.\n",
      "Row 2556/2871 evaluated.\n",
      "Row 2557/2871 evaluated.\n",
      "Row 2558/2871 evaluated.\n",
      "Row 2559/2871 evaluated.\n",
      "Row 2560/2871 evaluated.\n",
      "Row 2561/2871 evaluated.\n",
      "Row 2562/2871 evaluated.\n",
      "Row 2563/2871 evaluated.\n",
      "Row 2564/2871 evaluated.\n",
      "Row 2565/2871 evaluated.\n",
      "Row 2566/2871 evaluated.\n",
      "Row 2567/2871 evaluated.\n",
      "Row 2568/2871 evaluated.\n",
      "Row 2569/2871 evaluated.\n",
      "Row 2570/2871 evaluated.\n",
      "Row 2571/2871 evaluated.\n",
      "Row 2572/2871 evaluated.\n",
      "Row 2573/2871 evaluated.\n",
      "Row 2574/2871 evaluated.\n",
      "Row 2575/2871 evaluated.\n",
      "Row 2576/2871 evaluated.\n",
      "Row 2577/2871 evaluated.\n",
      "Row 2578/2871 evaluated.\n",
      "Row 2579/2871 evaluated.\n",
      "Row 2580/2871 evaluated.\n",
      "Row 2581/2871 evaluated.\n",
      "Row 2582/2871 evaluated.\n",
      "Row 2583/2871 evaluated.\n",
      "Row 2584/2871 evaluated.\n",
      "Row 2585/2871 evaluated.\n",
      "Row 2586/2871 evaluated.\n",
      "Row 2587/2871 evaluated.\n",
      "Row 2588/2871 evaluated.\n",
      "Row 2589/2871 evaluated.\n",
      "Row 2590/2871 evaluated.\n",
      "Row 2591/2871 evaluated.\n",
      "Row 2592/2871 evaluated.\n",
      "Row 2593/2871 evaluated.\n",
      "Row 2594/2871 evaluated.\n",
      "Row 2595/2871 evaluated.\n",
      "Row 2596/2871 evaluated.\n",
      "Row 2597/2871 evaluated.\n",
      "Row 2598/2871 evaluated.\n",
      "Row 2599/2871 evaluated.\n",
      "Row 2600/2871 evaluated.\n",
      "Row 2601/2871 evaluated.\n",
      "Row 2602/2871 evaluated.\n",
      "Row 2603/2871 evaluated.\n",
      "Row 2604/2871 evaluated.\n",
      "Row 2605/2871 evaluated.\n",
      "Row 2606/2871 evaluated.\n",
      "Row 2607/2871 evaluated.\n",
      "Row 2608/2871 evaluated.\n",
      "Row 2609/2871 evaluated.\n",
      "Row 2610/2871 evaluated.\n",
      "Row 2611/2871 evaluated.\n",
      "Row 2612/2871 evaluated.\n",
      "Row 2613/2871 evaluated.\n",
      "Row 2614/2871 evaluated.\n",
      "Row 2615/2871 evaluated.\n",
      "Row 2616/2871 evaluated.\n",
      "Row 2617/2871 evaluated.\n",
      "Row 2618/2871 evaluated.\n",
      "Row 2619/2871 evaluated.\n",
      "Row 2620/2871 evaluated.\n",
      "Row 2621/2871 evaluated.\n",
      "Row 2622/2871 evaluated.\n",
      "Row 2623/2871 evaluated.\n",
      "Row 2624/2871 evaluated.\n",
      "Row 2625/2871 evaluated.\n",
      "Row 2626/2871 evaluated.\n",
      "Row 2627/2871 evaluated.\n",
      "Row 2628/2871 evaluated.\n",
      "Row 2629/2871 evaluated.\n",
      "Row 2630/2871 evaluated.\n",
      "Row 2631/2871 evaluated.\n",
      "Row 2632/2871 evaluated.\n",
      "Row 2633/2871 evaluated.\n",
      "Row 2634/2871 evaluated.\n",
      "Row 2635/2871 evaluated.\n",
      "Row 2636/2871 evaluated.\n",
      "Row 2637/2871 evaluated.\n",
      "Row 2638/2871 evaluated.\n",
      "Row 2639/2871 evaluated.\n",
      "Row 2640/2871 evaluated.\n",
      "Row 2641/2871 evaluated.\n",
      "Row 2642/2871 evaluated.\n",
      "Row 2643/2871 evaluated.\n",
      "Row 2644/2871 evaluated.\n",
      "Row 2645/2871 evaluated.\n",
      "Row 2646/2871 evaluated.\n",
      "Row 2647/2871 evaluated.\n",
      "Row 2648/2871 evaluated.\n",
      "Row 2649/2871 evaluated.\n",
      "Row 2650/2871 evaluated.\n",
      "Row 2651/2871 evaluated.\n",
      "Row 2652/2871 evaluated.\n",
      "Row 2653/2871 evaluated.\n",
      "Row 2654/2871 evaluated.\n",
      "Row 2655/2871 evaluated.\n",
      "Row 2656/2871 evaluated.\n",
      "Row 2657/2871 evaluated.\n",
      "Row 2658/2871 evaluated.\n",
      "Row 2659/2871 evaluated.\n",
      "Row 2660/2871 evaluated.\n",
      "Row 2661/2871 evaluated.\n",
      "Row 2662/2871 evaluated.\n",
      "Row 2663/2871 evaluated.\n",
      "Row 2664/2871 evaluated.\n",
      "Row 2665/2871 evaluated.\n",
      "Row 2666/2871 evaluated.\n",
      "Row 2667/2871 evaluated.\n",
      "Row 2668/2871 evaluated.\n",
      "Row 2669/2871 evaluated.\n",
      "Row 2670/2871 evaluated.\n",
      "Row 2671/2871 evaluated.\n",
      "Row 2672/2871 evaluated.\n",
      "Row 2673/2871 evaluated.\n",
      "Row 2674/2871 evaluated.\n",
      "Row 2675/2871 evaluated.\n",
      "Row 2676/2871 evaluated.\n",
      "Row 2677/2871 evaluated.\n",
      "Row 2678/2871 evaluated.\n",
      "Row 2679/2871 evaluated.\n",
      "Row 2680/2871 evaluated.\n",
      "Row 2681/2871 evaluated.\n",
      "Row 2682/2871 evaluated.\n",
      "Row 2683/2871 evaluated.\n",
      "Row 2684/2871 evaluated.\n",
      "Row 2685/2871 evaluated.\n",
      "Row 2686/2871 evaluated.\n",
      "Row 2687/2871 evaluated.\n",
      "Row 2688/2871 evaluated.\n",
      "Row 2689/2871 evaluated.\n",
      "Row 2690/2871 evaluated.\n",
      "Row 2691/2871 evaluated.\n",
      "Row 2692/2871 evaluated.\n",
      "Row 2693/2871 evaluated.\n",
      "Row 2694/2871 evaluated.\n",
      "Row 2695/2871 evaluated.\n",
      "Row 2696/2871 evaluated.\n",
      "Row 2697/2871 evaluated.\n",
      "Row 2698/2871 evaluated.\n",
      "Row 2699/2871 evaluated.\n",
      "Row 2700/2871 evaluated.\n",
      "Row 2701/2871 evaluated.\n",
      "Row 2702/2871 evaluated.\n",
      "Row 2703/2871 evaluated.\n",
      "Row 2704/2871 evaluated.\n",
      "Row 2705/2871 evaluated.\n",
      "Row 2706/2871 evaluated.\n",
      "Row 2707/2871 evaluated.\n",
      "Row 2708/2871 evaluated.\n",
      "Row 2709/2871 evaluated.\n",
      "Row 2710/2871 evaluated.\n",
      "Row 2711/2871 evaluated.\n",
      "Row 2712/2871 evaluated.\n",
      "Row 2713/2871 evaluated.\n",
      "Row 2714/2871 evaluated.\n",
      "Row 2715/2871 evaluated.\n",
      "Row 2716/2871 evaluated.\n",
      "Row 2717/2871 evaluated.\n",
      "Row 2718/2871 evaluated.\n",
      "Row 2719/2871 evaluated.\n",
      "Row 2720/2871 evaluated.\n",
      "Row 2721/2871 evaluated.\n",
      "Row 2722/2871 evaluated.\n",
      "Row 2723/2871 evaluated.\n",
      "Row 2724/2871 evaluated.\n",
      "Row 2725/2871 evaluated.\n",
      "Row 2726/2871 evaluated.\n",
      "Row 2727/2871 evaluated.\n",
      "Row 2728/2871 evaluated.\n",
      "Row 2729/2871 evaluated.\n",
      "Row 2730/2871 evaluated.\n",
      "Row 2731/2871 evaluated.\n",
      "Row 2732/2871 evaluated.\n",
      "Row 2733/2871 evaluated.\n",
      "Row 2734/2871 evaluated.\n",
      "Row 2735/2871 evaluated.\n",
      "Row 2736/2871 evaluated.\n",
      "Row 2737/2871 evaluated.\n",
      "Row 2738/2871 evaluated.\n",
      "Row 2739/2871 evaluated.\n",
      "Row 2740/2871 evaluated.\n",
      "Row 2741/2871 evaluated.\n",
      "Row 2742/2871 evaluated.\n",
      "Row 2743/2871 evaluated.\n",
      "Row 2744/2871 evaluated.\n",
      "Row 2745/2871 evaluated.\n",
      "Row 2746/2871 evaluated.\n",
      "Row 2747/2871 evaluated.\n",
      "Row 2748/2871 evaluated.\n",
      "Row 2749/2871 evaluated.\n",
      "Row 2750/2871 evaluated.\n",
      "Row 2751/2871 evaluated.\n",
      "Row 2752/2871 evaluated.\n",
      "Row 2753/2871 evaluated.\n",
      "Row 2754/2871 evaluated.\n",
      "Row 2755/2871 evaluated.\n",
      "Row 2756/2871 evaluated.\n",
      "Row 2757/2871 evaluated.\n",
      "Row 2758/2871 evaluated.\n",
      "Row 2759/2871 evaluated.\n",
      "Row 2760/2871 evaluated.\n",
      "Row 2761/2871 evaluated.\n",
      "Row 2762/2871 evaluated.\n",
      "Row 2763/2871 evaluated.\n",
      "Row 2764/2871 evaluated.\n",
      "Row 2765/2871 evaluated.\n",
      "Row 2766/2871 evaluated.\n",
      "Row 2767/2871 evaluated.\n",
      "Row 2768/2871 evaluated.\n",
      "Row 2769/2871 evaluated.\n",
      "Skipping row 2770: Empty or invalid text.\n",
      "Row 2771/2871 evaluated.\n",
      "Row 2772/2871 evaluated.\n",
      "Row 2773/2871 evaluated.\n",
      "Row 2774/2871 evaluated.\n",
      "Row 2775/2871 evaluated.\n",
      "Row 2776/2871 evaluated.\n",
      "Row 2777/2871 evaluated.\n",
      "Row 2778/2871 evaluated.\n",
      "Row 2779/2871 evaluated.\n",
      "Row 2780/2871 evaluated.\n",
      "Row 2781/2871 evaluated.\n",
      "Row 2782/2871 evaluated.\n",
      "Row 2783/2871 evaluated.\n",
      "Row 2784/2871 evaluated.\n",
      "Row 2785/2871 evaluated.\n",
      "Row 2786/2871 evaluated.\n",
      "Row 2787/2871 evaluated.\n",
      "Row 2788/2871 evaluated.\n",
      "Row 2789/2871 evaluated.\n",
      "Row 2790/2871 evaluated.\n",
      "Row 2791/2871 evaluated.\n",
      "Row 2792/2871 evaluated.\n",
      "Row 2793/2871 evaluated.\n",
      "Row 2794/2871 evaluated.\n",
      "Row 2795/2871 evaluated.\n",
      "Row 2796/2871 evaluated.\n",
      "Row 2797/2871 evaluated.\n",
      "Row 2798/2871 evaluated.\n",
      "Row 2799/2871 evaluated.\n",
      "Row 2800/2871 evaluated.\n",
      "Row 2801/2871 evaluated.\n",
      "Row 2802/2871 evaluated.\n",
      "Row 2803/2871 evaluated.\n",
      "Row 2804/2871 evaluated.\n",
      "Row 2805/2871 evaluated.\n",
      "Row 2806/2871 evaluated.\n",
      "Row 2807/2871 evaluated.\n",
      "Row 2808/2871 evaluated.\n",
      "Row 2809/2871 evaluated.\n",
      "Row 2810/2871 evaluated.\n",
      "Row 2811/2871 evaluated.\n",
      "Row 2812/2871 evaluated.\n",
      "Row 2813/2871 evaluated.\n",
      "Row 2814/2871 evaluated.\n",
      "Row 2815/2871 evaluated.\n",
      "Row 2816/2871 evaluated.\n",
      "Row 2817/2871 evaluated.\n",
      "Row 2818/2871 evaluated.\n",
      "Row 2819/2871 evaluated.\n",
      "Row 2820/2871 evaluated.\n",
      "Row 2821/2871 evaluated.\n",
      "Row 2822/2871 evaluated.\n",
      "Row 2823/2871 evaluated.\n",
      "Row 2824/2871 evaluated.\n",
      "Row 2825/2871 evaluated.\n",
      "Row 2826/2871 evaluated.\n",
      "Row 2827/2871 evaluated.\n",
      "Row 2828/2871 evaluated.\n",
      "Row 2829/2871 evaluated.\n",
      "Row 2830/2871 evaluated.\n",
      "Row 2831/2871 evaluated.\n",
      "Row 2832/2871 evaluated.\n",
      "Row 2833/2871 evaluated.\n",
      "Row 2834/2871 evaluated.\n",
      "Row 2835/2871 evaluated.\n",
      "Row 2836/2871 evaluated.\n",
      "Row 2837/2871 evaluated.\n",
      "Row 2838/2871 evaluated.\n",
      "Row 2839/2871 evaluated.\n",
      "Row 2840/2871 evaluated.\n",
      "Row 2841/2871 evaluated.\n",
      "Row 2842/2871 evaluated.\n",
      "Row 2843/2871 evaluated.\n",
      "Row 2844/2871 evaluated.\n",
      "Row 2845/2871 evaluated.\n",
      "Row 2846/2871 evaluated.\n",
      "Row 2847/2871 evaluated.\n",
      "Row 2848/2871 evaluated.\n",
      "Row 2849/2871 evaluated.\n",
      "Row 2850/2871 evaluated.\n",
      "Row 2851/2871 evaluated.\n",
      "Row 2852/2871 evaluated.\n",
      "Row 2853/2871 evaluated.\n",
      "Row 2854/2871 evaluated.\n",
      "Row 2855/2871 evaluated.\n",
      "Row 2856/2871 evaluated.\n",
      "Row 2857/2871 evaluated.\n",
      "Row 2858/2871 evaluated.\n",
      "Row 2859/2871 evaluated.\n",
      "Row 2860/2871 evaluated.\n",
      "Row 2861/2871 evaluated.\n",
      "Row 2862/2871 evaluated.\n",
      "Row 2863/2871 evaluated.\n",
      "Row 2864/2871 evaluated.\n",
      "Row 2865/2871 evaluated.\n",
      "Row 2866/2871 evaluated.\n",
      "Row 2867/2871 evaluated.\n",
      "Row 2868/2871 evaluated.\n",
      "Row 2869/2871 evaluated.\n",
      "Row 2870/2871 evaluated.\n",
      "Row 2871/2871 evaluated.\n",
      " Evaluation complete. Results saved to 'summary_evaluation_results1.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu as nltk_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from rouge import Rouge\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sacrebleu import corpus_bleu as sacre_bleu\n",
    "\n",
    "# Sample and add doc_id\n",
    "# train_sample = train_df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "train_sample['doc_id'] = train_sample.index\n",
    "\n",
    "# Load summaries\n",
    "generated = pd.read_csv(\"deep_extract_stage7_final_summaries.csv\")  # refined_summary\n",
    "reference = pd.read_csv(\"new_extractive_summaries.csv\")             # extractive_summary\n",
    "\n",
    "# Merge\n",
    "merged = pd.merge(generated, reference, on='doc_id')\n",
    "merged = pd.merge(merged, train_sample[['doc_id', 'article', 'highlights']], on='doc_id')\n",
    "\n",
    "rouge = Rouge()\n",
    "language = \"english\"\n",
    "results = []\n",
    "\n",
    "print(f\"Starting evaluation for {len(merged)} rows...\")\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    try:\n",
    "        original_text = row['article']\n",
    "        sys_summary = row['refined_summary']\n",
    "        reference_text = row['extractive_summary']\n",
    "\n",
    "        if not all(isinstance(x, str) and x.strip() for x in [original_text, sys_summary, reference_text]):\n",
    "            print(f\"Skipping row {idx + 1}: Empty or invalid text.\")\n",
    "            continue\n",
    "\n",
    "        I = max(1, round(len(sent_tokenize(original_text)) * 0.4))  # at least 1 sentence\n",
    "        parser = PlaintextParser.from_string(original_text, Tokenizer(language))\n",
    "\n",
    "        def safe_scores(summary_text):\n",
    "            if not summary_text.strip():\n",
    "                return [0.0] * 7, \"\"  # default zero scores\n",
    "            r_scores = rouge.get_scores(summary_text, reference_text)[0]\n",
    "            su4 = rouge.get_scores(summary_text, reference_text, avg=True)['rouge-l']['f']\n",
    "            bleu = nltk_bleu([reference_text.split()], summary_text.split(), smoothing_function=SmoothingFunction().method1)\n",
    "            meteor = meteor_score([word_tokenize(reference_text)], word_tokenize(summary_text))\n",
    "            sacre = sacre_bleu(summary_text, [reference_text]).score / 100\n",
    "            return [r_scores['rouge-1']['f'], r_scores['rouge-2']['f'], r_scores['rouge-l']['f'], su4, bleu, meteor, sacre], summary_text\n",
    "\n",
    "        # --- System-generated ---\n",
    "        sys_scores, sys_summary_clean = safe_scores(' '.join(sys_summary.split()))\n",
    "\n",
    "        # --- TextRank ---\n",
    "        tr_summary = ' '.join(str(s) for s in TextRankSummarizer()(parser.document, I))\n",
    "        tr_scores, tr_summary = safe_scores(tr_summary)\n",
    "\n",
    "        # --- LSA ---\n",
    "        lsa_summary = ' '.join(str(s) for s in LsaSummarizer()(parser.document, I))\n",
    "        lsa_scores, lsa_summary = safe_scores(lsa_summary)\n",
    "\n",
    "        # --- Luhn ---\n",
    "        luhn_summary = ' '.join(str(s) for s in LuhnSummarizer()(parser.document, I))\n",
    "        luhn_scores, luhn_summary = safe_scores(luhn_summary)\n",
    "\n",
    "        results.append([\n",
    "            reference_text, sys_summary_clean, *sys_scores,\n",
    "            tr_summary, *tr_scores,\n",
    "            lsa_summary, *lsa_scores,\n",
    "            luhn_summary, *luhn_scores\n",
    "        ])\n",
    "        print(f\"Row {idx + 1}/{len(merged)} evaluated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {idx + 1}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Column headers\n",
    "columns = [\n",
    "    'reference_text', 'sys_summary', 'sys_rouge1', 'sys_rouge2', 'sys_rougel', 'sys_su4', 'bleu_sys', 'meteor_sys', 'sacrebleu_sys',\n",
    "    'text_rank_summary', 'text_rank_rouge1', 'text_rank_rouge2', 'text_rank_rougel', 'text_rank_su4', 'bleu_tr', 'meteor_tr', 'sacrebleu_tr',\n",
    "    'lsa_summary', 'lsa_rouge1', 'lsa_rouge2', 'lsa_rougel', 'lsa_su4', 'bleu_lsa', 'meteor_lsa', 'sacrebleu_lsa',\n",
    "    'luhn_summary', 'luhn_rouge1', 'luhn_rouge2', 'luhn_rougel', 'luhn_su4', 'bleu_luhn', 'meteor_luhn', 'sacrebleu_luhn'\n",
    "]\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "results_df.to_csv(\"summary_evaluation_results1.csv\", index=False)\n",
    "print(\" Evaluation complete. Results saved to 'summary_evaluation_results1.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b9a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f358c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the evaluation results\n",
    "df = pd.read_csv(\"summary_evaluation_results1.csv\")\n",
    "\n",
    "# Define models and corresponding metric columns\n",
    "models = {\n",
    "    \"DeepExtract\": ['sys_rouge1', 'sys_rouge2', 'sys_rougel', 'sys_su4', 'bleu_sys', 'meteor_sys', 'sacrebleu_sys'],\n",
    "    \"TextRank\":    ['text_rank_rouge1', 'text_rank_rouge2', 'text_rank_rougel', 'text_rank_su4', 'bleu_tr', 'meteor_tr', 'sacrebleu_tr'],\n",
    "    \"LSA\":         ['lsa_rouge1', 'lsa_rouge2', 'lsa_rougel', 'lsa_su4', 'bleu_lsa', 'meteor_lsa', 'sacrebleu_lsa'],\n",
    "    \"Luhn\":        ['luhn_rouge1', 'luhn_rouge2', 'luhn_rougel', 'luhn_su4', 'bleu_luhn', 'meteor_luhn', 'sacrebleu_luhn'],\n",
    "   \n",
    "}\n",
    "\n",
    "metric_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "\n",
    "# Compute average scores for each method\n",
    "plot_data = []\n",
    "for model, cols in models.items():\n",
    "    averages = df[cols].mean().tolist()\n",
    "    for metric, score in zip(metric_names, averages):\n",
    "        plot_data.append({'Model': model, 'Metric': metric, 'Score': score})\n",
    "\n",
    "# Convert to DataFrame\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=plot_df, x=\"Metric\", y=\"Score\", hue=\"Model\")\n",
    "plt.title(\"Summarization Evaluation Metrics Comparison\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"summary_comparison_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2126b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summarization ROUGE Score Table ===\n",
      "\n",
      "             ROUGE-1  ROUGE-2  ROUGE-L\n",
      "DeepExtract   0.4605   0.3430   0.4361\n",
      "TextRank      0.4031   0.3029   0.3934\n",
      "LSA           0.3831   0.2717   0.3702\n",
      "Luhn          0.4279   0.3265   0.4211\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the evaluation results\n",
    "df = pd.read_csv(\"summary_evaluation_results1.csv\")\n",
    "\n",
    "# Define only the ROUGE-related metric columns\n",
    "models = {\n",
    "    \"DeepExtract\": ['sys_rouge1', 'sys_rouge2', 'sys_rougel'],\n",
    "    \"TextRank\":    ['text_rank_rouge1', 'text_rank_rouge2', 'text_rank_rougel'],\n",
    "    \"LSA\":         ['lsa_rouge1', 'lsa_rouge2', 'lsa_rougel'],\n",
    "    \"Luhn\":        ['luhn_rouge1', 'luhn_rouge2', 'luhn_rougel'],\n",
    "}\n",
    "\n",
    "# Metric labels\n",
    "metric_labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = {}\n",
    "for model_name, metric_cols in models.items():\n",
    "    averages = df[metric_cols].mean()\n",
    "    table_data[model_name] = averages.values\n",
    "\n",
    "# Convert to DataFrame\n",
    "score_table = pd.DataFrame(table_data, index=metric_labels).T  # Transpose for readability\n",
    "\n",
    "# Display the score table\n",
    "print(\"\\n=== Summarization ROUGE Score Table ===\\n\")\n",
    "print(score_table.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e572039",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b506309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the evaluation results\n",
    "df = pd.read_csv(\"summary_evaluation_results1.csv\")\n",
    "\n",
    "# Define models and corresponding metric columns\n",
    "models = {\n",
    "    \"DeepExtract\": ['sys_rouge1', 'sys_rouge2', 'sys_rougel', 'sys_su4', 'bleu_sys', 'meteor_sys', 'sacrebleu_sys'],\n",
    "    \"TextRank\":    ['text_rank_rouge1', 'text_rank_rouge2', 'text_rank_rougel', 'text_rank_su4', 'bleu_tr', 'meteor_tr', 'sacrebleu_tr'],\n",
    "    \"LSA\":         ['lsa_rouge1', 'lsa_rouge2', 'lsa_rougel', 'lsa_su4', 'bleu_lsa', 'meteor_lsa', 'sacrebleu_lsa'],\n",
    "    \"Luhn\":        ['luhn_rouge1', 'luhn_rouge2', 'luhn_rougel', 'luhn_su4', 'bleu_luhn', 'meteor_luhn', 'sacrebleu_luhn'],\n",
    "   \n",
    "}\n",
    "\n",
    "metric_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'SU4', 'BLEU']\n",
    "#metric_names = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "\n",
    "# Compute average scores for each method\n",
    "plot_data = []\n",
    "for model, cols in models.items():\n",
    "    averages = df[cols].mean().tolist()\n",
    "    for metric, score in zip(metric_names, averages):\n",
    "        plot_data.append({'Model': model, 'Metric': metric, 'Score': score})\n",
    "\n",
    "# Convert to DataFrame\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=plot_df, x=\"Metric\", y=\"Score\", hue=\"Model\")\n",
    "plt.title(\"Summarization Evaluation Metrics Comparison\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score (0-1)\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"summary_comparison_plot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b1403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99342979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
